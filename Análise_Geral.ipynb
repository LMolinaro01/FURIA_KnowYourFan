{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "#### Importa√ß√£o Chave API (Arquivo .env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd5d8a7e76898c",
   "metadata": {},
   "source": [
    "### Coleta de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58c9e8",
   "metadata": {},
   "source": [
    "##### Coment√°rios YTB - JSON (Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea82564d0b8b86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:08:55.632710Z",
     "start_time": "2025-04-30T14:08:43.298626Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Testa se a chave da API est√° sendo carregada\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "\n",
    "# ID do v√≠deo do qual voc√™ quer obter os coment√°rios\n",
    "video_id = '8aIcU-_5W34'\n",
    "\n",
    "\n",
    "def get_video_channel_name(video_id, api_key):\n",
    "    # Conectando √† API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Obt√©m as informa√ß√µes do v√≠deo\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisi√ß√£o e pega o nome do canal\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "        return channel_name\n",
    "    return None\n",
    "\n",
    "# Fun√ß√£o para obter coment√°rios do v√≠deo\n",
    "def get_comments(video_id, api_key):\n",
    "    # Conectando √† API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Lista para armazenar os coment√°rios\n",
    "    comments = []\n",
    "    \n",
    "    channel_name = get_video_channel_name(video_id, api_key)\n",
    "    \n",
    "    # Inicializa a requisi√ß√£o para obter os coment√°rios\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100  # Max resultados por requisi√ß√£o (pode ajustar conforme necess√°rio)\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisi√ß√£o\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Itera sobre os coment√°rios e armazena os dados\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                    'video_id': video_id,                    # ‚Üê aqui voc√™ adiciona o ID do v√≠deo\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'text': comment['textDisplay'],\n",
    "                    'published_at': comment['publishedAt'],\n",
    "                    'likes': comment['likeCount'],\n",
    "                    'channel_name': channel_name\n",
    "                }\n",
    "            \n",
    "            comments.append(comment_data)\n",
    "        \n",
    "        # Verifica se existe uma pr√≥xima p√°gina de resultados\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Obter os coment√°rios\n",
    "# Fun√ß√£o para salvar os coment√°rios sem sobrescrever o arquivo existente\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    # Caminho da pasta onde voc√™ deseja salvar o arquivo\n",
    "    pasta = 'form_data'\n",
    "\n",
    "    # Certifique-se de que a pasta existe\n",
    "    if not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "    # Caminho completo do arquivo JSON\n",
    "    arquivo_json = os.path.join(pasta, filename)\n",
    "    \n",
    "    # Verifica se o arquivo j√° existe\n",
    "    if os.path.exists(arquivo_json):\n",
    "        # Carrega o conte√∫do existente\n",
    "        with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "            existing_comments = json.load(f)\n",
    "        \n",
    "        # Adiciona os novos coment√°rios ao conte√∫do existente\n",
    "        existing_comments.extend(comments)\n",
    "        \n",
    "        # Salva o conte√∫do atualizado\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_comments, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        # Caso o arquivo n√£o exista, cria um novo com os coment√°rios\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Obter os coment√°rios\n",
    "comments = get_comments(video_id, api_key)\n",
    "\n",
    "# Salvar os coment√°rios sem sobrescrever o arquivo\n",
    "save_comments(comments)\n",
    "\n",
    "print(\"Coment√°rios salvos em 'form_data/comentarios_video.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8841c4",
   "metadata": {},
   "source": [
    "#### Coment√°rios YTB - JSON - GUI (Custom Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install customtkinter google-api-python-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4942f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import customtkinter as ctk\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tkinter.messagebox as msgbox\n",
    "\n",
    "# Load .env\n",
    "env_path = Path('.idea/.env')  # Ajuste conforme necess√°rio\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "# Detecta ID do v√≠deo\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Detecta ID da playlist\n",
    "def extract_playlist_id(url):\n",
    "    match = re.search(r\"[?&]list=([a-zA-Z0-9_-]+)\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Pega nome do canal\n",
    "def get_video_channel_name(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    request = youtube.videos().list(part='snippet', id=video_id)\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        return response['items'][0]['snippet']['channelTitle']\n",
    "    return None\n",
    "\n",
    "# Busca coment√°rios\n",
    "def get_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    comments = []\n",
    "    channel_name = get_video_channel_name(video_id)\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                'video_id': video_id,\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'text': comment['textDisplay'],\n",
    "                'published_at': comment['publishedAt'],\n",
    "                'likes': comment['likeCount'],\n",
    "                'channel_name': channel_name\n",
    "            }\n",
    "            comments.append(comment_data)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "# Salva no JSON\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    pasta = 'form_data'\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "    caminho = os.path.join(pasta, filename)\n",
    "    if os.path.exists(caminho):\n",
    "        with open(caminho, 'r', encoding='utf-8') as f:\n",
    "            existentes = json.load(f)\n",
    "        existentes.extend(comments)\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existentes, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Busca v√≠deos de uma playlist\n",
    "def get_video_ids_from_playlist(playlist_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    video_ids = []\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "        request = youtube.playlistItems().list_next(request, response)\n",
    "    return video_ids\n",
    "\n",
    "# Processa link inserido\n",
    "def process_link():\n",
    "    url = entry_url.get().strip()\n",
    "    if not url:\n",
    "        status_label.configure(text=\"Cole um link v√°lido do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    playlist_id = extract_playlist_id(url)\n",
    "    video_id = extract_video_id(url)\n",
    "\n",
    "    if playlist_id:\n",
    "        try:\n",
    "            video_ids = get_video_ids_from_playlist(playlist_id)\n",
    "            total_comments = []\n",
    "            for vid in video_ids:\n",
    "                status_label.configure(text=f\"Buscando coment√°rios de {vid}...\", text_color=\"blue\")\n",
    "                comments = get_comments(vid)\n",
    "                total_comments.extend(comments)\n",
    "            save_comments(total_comments)\n",
    "            status_label.configure(text=f\"Todos os coment√°rios da playlist foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "\n",
    "    elif video_id:\n",
    "        try:\n",
    "            status_label.configure(text=\"Buscando coment√°rios do v√≠deo...\", text_color=\"blue\")\n",
    "            comments = get_comments(video_id)\n",
    "            save_comments(comments)\n",
    "            status_label.configure(text=f\"Coment√°rios do v√≠deo foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "    else:\n",
    "        status_label.configure(text=\"Link inv√°lido. Verifique se √© um link do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    # Pergunta se deseja adicionar mais\n",
    "    continuar = msgbox.askyesno(\"Continuar\", \"Deseja adicionar outro v√≠deo ou playlist?\")\n",
    "    if continuar:\n",
    "        entry_url.delete(0, 'end')\n",
    "        status_label.configure(text=\"Cole outro link para continuar.\", text_color=\"black\")\n",
    "    else:\n",
    "        app.quit()\n",
    "        app.destroy()\n",
    "\n",
    "# GUI com customtkinter\n",
    "ctk.set_appearance_mode(\"System\")\n",
    "ctk.set_default_color_theme(\"blue\")\n",
    "\n",
    "app = ctk.CTk()\n",
    "app.title(\"Coletor de Coment√°rios YouTube\")\n",
    "app.geometry(\"600x300\")\n",
    "\n",
    "label = ctk.CTkLabel(app, text=\"Cole o link do v√≠deo ou playlist do YouTube:\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "entry_url = ctk.CTkEntry(app, width=500)\n",
    "entry_url.pack(pady=10)\n",
    "\n",
    "submit_button = ctk.CTkButton(app, text=\"Buscar e Salvar Coment√°rios\", command=process_link)\n",
    "submit_button.pack(pady=20)\n",
    "\n",
    "status_label = ctk.CTkLabel(app, text=\"\")\n",
    "status_label.pack(pady=10)\n",
    "\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5021376815ef2",
   "metadata": {},
   "source": [
    "#### Coment√°rios - JSON (Twitter/X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5376ad21a39994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Configura√ß√µes e autentica√ß√£o ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Par√¢metros de pesquisa com a hashtag #DIADEFURIA inclu√≠da\n",
    "query = '(Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR Trigo OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson OR #FURIACS OR #FURIAR6 OR #FURIAFC OR #DIADEFURIA) -is:retweet lang:pt'\n",
    "max_results = 10\n",
    "\n",
    "# Fazendo a busca com os campos desejados\n",
    "tweets = client.search_recent_tweets(query=query, max_results=max_results,\n",
    "                                     tweet_fields=[\"author_id\", \"conversation_id\", \"created_at\", \"geo\", \"id\", \"lang\", \"source\", \"text\"],\n",
    "                                     user_fields=[\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"url\", \"username\"],\n",
    "                                     expansions=[\"author_id\"])\n",
    "\n",
    "# Convertendo os tweets para um formato de dicion√°rio\n",
    "tweets_data = []\n",
    "if tweets.data:\n",
    "    for tweet in tweets.data:\n",
    "        tweet_info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'author_id': tweet.author_id,\n",
    "            'conversation_id': tweet.conversation_id,\n",
    "            'geo': tweet.geo,\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source\n",
    "        }\n",
    "\n",
    "        # Obtendo informa√ß√µes do usu√°rio (quem postou o tweet)\n",
    "        if tweets.includes and 'users' in tweets.includes:\n",
    "            for user in tweets.includes['users']:\n",
    "                if user.id == tweet.author_id:\n",
    "                    tweet_info['user'] = {\n",
    "                        'created_at': str(user.created_at),\n",
    "                        'description': user.description,\n",
    "                        'entities': user.entities,\n",
    "                        'location': user.location,\n",
    "                        'name': user.name,\n",
    "                        'url': user.url,\n",
    "                        'username': user.username\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "        tweets_data.append(tweet_info)\n",
    "\n",
    "# Caminho da pasta onde voc√™ deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, 'tweetsGerais_furia.json')\n",
    "\n",
    "# Carrega o conte√∫do existente, se houver\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Adiciona os novos tweets\n",
    "dados_existentes.extend(tweets_data)\n",
    "\n",
    "# Salva de volta no JSON\n",
    "with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_existentes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tweets adicionados com sucesso a 'tweetsGerais_furia.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9cecf67adf235",
   "metadata": {},
   "source": [
    "#### Posts - JSON (Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb4487c423268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "query = \"FURIA OR Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson\"\n",
    "\n",
    "subreddits = [\"GlobalOffensive\", \"csgo\", \"VALORANT\", \"cs2\", \"cblol\", \"LolEsports\", \"ValorantCompetitive\", \"VCT\", \"R6ProLeague\"]\n",
    "limit = 50\n",
    "\n",
    "resultados = []\n",
    "\n",
    "# Loop pelos subreddits\n",
    "for subreddit in subreddits:\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr=on&limit={limit}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"data\" in data and \"children\" in data[\"data\"]:\n",
    "        for post in data[\"data\"][\"children\"]:\n",
    "            p = post[\"data\"]\n",
    "\n",
    "            # Verifica se a query aparece no t√≠tulo ou no texto do post\n",
    "            titulo = p.get(\"title\", \"\")\n",
    "            texto = p.get(\"selftext\", \"\")\n",
    "\n",
    "            # Verifica√ß√£o literal, sem usar lower()\n",
    "            if any(jogador in titulo or jogador in texto for jogador in query.split(\" OR \")):\n",
    "                resultados.append({\n",
    "                    \"titulo\": p.get(\"title\"),\n",
    "                    \"autor\": p.get(\"author\"),\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"score\": p.get(\"score\", 0),\n",
    "                    \"url\": \"https://reddit.com\" + p.get(\"permalink\"),\n",
    "                    \"data_criacao\": p.get(\"created_utc\"),\n",
    "                    \"comentario_exemplo\": p.get(\"selftext\", \"\")\n",
    "                })\n",
    "\n",
    "# Caminho da pasta onde voc√™ deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, \"posts_furia_reddit.json\")\n",
    "\n",
    "# Salva em JSON\n",
    "# Verifica se o arquivo j√° existe e carrega os dados antigos\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Junta os dados antigos com os novos\n",
    "dados_atuaisizados = dados_existentes + resultados\n",
    "\n",
    "# Salva todos os dados no JSON\n",
    "with open(arquivo_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dados_atuaisizados, f, indent=4, ensure_ascii=False)\n",
    "    print(\"Novos dados adicionados ao JSON.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5023f63bcefe89",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Enriquecimento de Perfil com Dados Sociais e Multim√≠dia</h3> </div>\n",
    "\n",
    "- **An√°lise de Coment√°rios:** Para integrar coment√°rios pr√©vios do usu√°rio no YouTube, Reddit e Twitter, incluir blocos que consumam APIs ou dados locais de an√°lise anterior (supondo que existam). Usar `google-api-python-client` para extrair coment√°rios de v√≠deos de e-sports do YouTube, `PRAW` para posts/coment√°rios no Reddit, e `tweepy` ou dados simulados para tweets.  \n",
    "- **Processamento de Linguagem Natural:** Aplicar NLP para entender o perfil do usu√°rio: usar bibliotecas como `transformers` ou `spaCy` para classificar sentimento, identificar t√≥picos ou palavras-chave frequentes nesses coment√°rios. Por exemplo, gerar um gr√°fico de palavras-chave mais mencionadas em e-sports, ou uma an√°lise de sentimento geral sobre jogos espec√≠ficos.  \n",
    "- **Integra√ß√£o de Informa√ß√µes:** Combinar esses insights com os interesses declarados pelo usu√°rio. Exibir visualmente (via `matplotlib` ou `seaborn`) uma nuvem de palavras ou gr√°fico que mostre as categorias de e-sports mais relevantes para o perfil (baseado em interesses + an√°lise de coment√°rios).  \n",
    "- **Perfis em Sites de e-Sports:** Permitir que o usu√°rio insira links para seus perfis em plataformas de e-sports (como GameBattles, HLTV, Liquipedia). Usar `requests` e `BeautifulSoup` para raspar detalhes do perfil (jogos, hist√≥rico de partidas). Em seguida, aplicar um modelo de IA (ex: `transformers` BERT) para classificar se o conte√∫do textual do perfil √© relevante √†s prefer√™ncias do usu√°rio (por exemplo, buscando termos de jogos citados pelo usu√°rio). Mostrar se h√° ‚Äúmatch‚Äù entre interesses do usu√°rio e informa√ß√µes do perfil scraped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f4811",
   "metadata": {},
   "source": [
    "#### An√°lise Geral (Youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5714698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc151832ad30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_json(R'form_data\\comentarios_video.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050497bf",
   "metadata": {},
   "source": [
    "Remo√ß√£o de Duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unico = df.drop_duplicates(subset=[\"text\", \"author\"])\n",
    "\n",
    "# Salva o DataFrame limpo de volta no JSON\n",
    "df_unico.to_json(R\"form_data\\comentarios_video.json\", orient=\"records\", indent=4, force_ascii=False)\n",
    "\n",
    "print(f\"Removidas {len(df) - len(df_unico)} duplicatas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735dadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de palavras indesejadas (insens√≠vel a mai√∫sculas)\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "\n",
    "# Normaliza as palavras para min√∫sculo\n",
    "palavras_banidas = [p.lower() for p in palavras_banidas]\n",
    "\n",
    "# Carrega o DataFrame original (ajuste o caminho conforme necess√°rio)\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# Remove as linhas cujo texto contenha qualquer das palavras\n",
    "df_filtrado = df[~df['text'].str.lower().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# Exibe o n√∫mero de linhas removidas\n",
    "removidas = df.shape[0] - df_filtrado.shape[0]\n",
    "print(f'{removidas} coment√°rios foram removidos com base nas palavras proibidas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converte published_at (ISO 8601) para datetime do pandas\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], utc=True)\n",
    "\n",
    "# 3. Extrai colunas de data e hora para facilitar agrega√ß√µes\n",
    "df['data'] = df['published_at'].dt.date\n",
    "df['hora'] = df['published_at'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Filtra coment√°rios com entre 1 e 45 likes\n",
    "df_filtrado = df[(df['likes'] > 0) & (df['likes'] <= 45)]\n",
    "\n",
    "# 3. Plota histograma com bins de tamanho 2\n",
    "plt.figure()\n",
    "plt.hist(df_filtrado['likes'], bins=range(0, 47, 2), edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Likes (1 a 45 likes, bins de 2 em 2)')\n",
    "plt.xlabel('N√∫mero de Likes')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "\n",
    "# 4. Ajusta os ticks do eixo x para mostrar de 2 em 2\n",
    "plt.xticks(range(0, 47, 2), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Exibe quantos coment√°rios ficaram fora desse intervalo\n",
    "total = df.shape[0]\n",
    "mantidos = df_filtrado.shape[0]\n",
    "removidos = total - mantidos\n",
    "print(f\"{removidos} coment√°rios foram removidos por terem 0 ou mais de 45 likes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24822976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Ordena pelo n√∫mero de likes e pega os top N (ajuste N se quiser mais/menos)\n",
    "N = 10\n",
    "top_comments = df.sort_values('likes', ascending=False).head(N)\n",
    "\n",
    "# 3. Prepara os r√≥tulos: quebra linhas para caber melhor no gr√°fico\n",
    "wrapped_texts = [\n",
    "    textwrap.fill(text, width=50)\n",
    "    for text in top_comments['text']\n",
    "]\n",
    "\n",
    "# 4. Plota gr√°fico de barras horizontais com o texto do coment√°rio\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(N), top_comments['likes'], edgecolor='black')\n",
    "plt.yticks(range(N), wrapped_texts)\n",
    "plt.xlabel('N√∫mero de Likes')\n",
    "plt.title(f'Top {N} Coment√°rios Mais Curtidos')\n",
    "plt.gca().invert_yaxis()  # Inverte o eixo para o coment√°rio mais curtido ficar no topo\n",
    "\n",
    "# 5. Anota√ß√µes com o n√∫mero exato de likes ao final de cada barra\n",
    "for i, like in enumerate(top_comments['likes']):\n",
    "    plt.text(like + 5, i, str(like), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 autores por n√∫mero de coment√°rios\n",
    "top_autores = df['author'].value_counts().head(10)\n",
    "plt.figure()\n",
    "plt.bar(top_autores.index, top_autores.values)\n",
    "plt.title('Top 10 Autores por N√∫mero de Coment√°rios')\n",
    "plt.xlabel('Autor')\n",
    "plt.ylabel('Quantidade de Coment√°rios')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "# 2. Converte published_at para datetime e extrai ano e m√™s\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], utc=True)\n",
    "df['year'] = df['published_at'].dt.year\n",
    "df['month'] = df['published_at'].dt.month\n",
    "\n",
    "# 3. Filtra para ignorar anos muito distantes (por exemplo, antes de 2000)\n",
    "df = df[df['year'] >= 2000]\n",
    "\n",
    "# 4. Para cada ano presente no DataFrame, gera um gr√°fico mensal\n",
    "for year in sorted(df['year'].unique()):\n",
    "    df_year = df[df['year'] == year]\n",
    "    # Agrupa por m√™s (1 a 12), preenchendo meses ausentes com zero\n",
    "    comentarios_por_mes = df_year.groupby('month').size().reindex(range(1, 13), fill_value=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(comentarios_por_mes.index, comentarios_por_mes.values, marker='o')\n",
    "    plt.title(f'Coment√°rios por M√™s em {year}')\n",
    "    plt.xlabel('M√™s')\n",
    "    plt.ylabel('N√∫mero de Coment√°rios')\n",
    "    \n",
    "    # Usa nomes de meses no eixo x\n",
    "    labels = [calendar.month_name[m] for m in comentarios_por_mes.index]\n",
    "    plt.xticks(ticks=comentarios_por_mes.index, labels=labels, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Caminho do JSON\n",
    "JSON_PATH = 'form_data/comentarios_video.json'\n",
    "\n",
    "# 2. Carrega o DataFrame\n",
    "df = pd.read_json(JSON_PATH)\n",
    "\n",
    "# 3. Top 10 coment√°rios mais curtidos por canal\n",
    "top_por_canal = (\n",
    "    df\n",
    "    .sort_values(['channel_name', 'likes'], ascending=[True, False])\n",
    "    .groupby('channel_name')\n",
    "    .head(25)\n",
    ")\n",
    "\n",
    "# 4. Pipeline de sentimento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 5. Aplica o modelo em lotes\n",
    "def analyze_batch(texts):\n",
    "    return sentiment_analyzer(texts, truncation=True)\n",
    "\n",
    "results = []\n",
    "batch_size = 32\n",
    "texts = top_por_canal['text'].tolist()\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    results.extend(analyze_batch(texts[i:i+batch_size]))\n",
    "\n",
    "# 6. Processa os resultados\n",
    "scores, labels = [], []\n",
    "for res in results:\n",
    "    stars = int(res['label'][0])\n",
    "    score = (stars - 1) / 4\n",
    "    if stars == 1:\n",
    "        sentiment = \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        sentiment = \"negativo\"\n",
    "    elif stars == 3:\n",
    "        sentiment = \"neutro\"\n",
    "    elif stars == 4:\n",
    "        sentiment = \"positivo\"\n",
    "    else:\n",
    "        sentiment = \"muito positivo\"\n",
    "    scores.append(score)\n",
    "    labels.append(sentiment)\n",
    "\n",
    "top_por_canal = top_por_canal.reset_index(drop=True)\n",
    "top_por_canal['sentiment_score'] = scores\n",
    "top_por_canal['sentiment'] = labels\n",
    "\n",
    "# 7. Classifica√ß√£o por faixa de m√©dia\n",
    "def classificar_media(score):\n",
    "    if score <= 0.2:\n",
    "        return \"muito negativo\"\n",
    "    elif score <= 0.3:\n",
    "        return \"negativo\"\n",
    "    elif score <= 0.43:\n",
    "        return \"neutro\"\n",
    "    elif score <= 0.5:\n",
    "        return \"levemente positivo\"\n",
    "    elif score <= 0.65:\n",
    "        return \"positivo\"\n",
    "    elif score <= 0.8:\n",
    "        return \"muito positivo\"\n",
    "    else:\n",
    "        return \"extremamente positivo\"\n",
    "\n",
    "# 8. Agrupamento por canal com contagem\n",
    "resumo_canais = (\n",
    "    top_por_canal\n",
    "    .groupby('channel_name')\n",
    "    .agg(\n",
    "        mean_sentiment_score=('sentiment_score', 'mean'),\n",
    "        total_comentarios=('sentiment_score', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 9. Aplica r√≥tulo textual e monta nome com contagem\n",
    "resumo_canais['sentiment_class'] = resumo_canais['mean_sentiment_score'].apply(classificar_media)\n",
    "resumo_canais['channel_label'] = resumo_canais.apply(\n",
    "    lambda row: f\"{row['channel_name']}\", axis=1\n",
    ")\n",
    "\n",
    "# 10. Gr√°fico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(resumo_canais['channel_label'], resumo_canais['mean_sentiment_score'], color='skyblue', edgecolor='black')\n",
    "plt.ylabel('M√©dia do Sentimento (0 = ruim, 1 = √≥timo)')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('M√©dia de Sentimento por Canal (Top 25 Coment√°rios Mais Curtidos)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 11. Anota a classifica√ß√£o acima das barras\n",
    "for bar, label in zip(bars, resumo_canais['sentiment_class']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, label, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "# Lista de palavras banidas\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "\n",
    "# 1. Filtra os 200 melhores coment√°rios por likes\n",
    "top100 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# 2. Filtra coment√°rios que contenham palavras banidas\n",
    "top100_filtrado = top100[~top100['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# 3. Cria o pipeline de an√°lise de sentimento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# 4. Aplica em lotes a an√°lise de sentimentos\n",
    "results = []\n",
    "batch_size = 32\n",
    "texts = top100_filtrado['text'].tolist()\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    results.extend(sentiment_analyzer(texts[i:i+batch_size], truncation=True))\n",
    "\n",
    "# 5. Converte labels em score normalizado\n",
    "scores = []\n",
    "for res in results:\n",
    "    stars = int(res['label'][0])\n",
    "    score = (stars - 1) / 4  # normaliza para 0‚Äì1\n",
    "    scores.append(score)\n",
    "\n",
    "top100_filtrado = top100_filtrado.reset_index(drop=True)\n",
    "top100_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# 6. Filtra 10 coment√°rios por canal com base no score de sentimento\n",
    "top_comentarios_canal = []\n",
    "\n",
    "for channel, group in top100_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(group.sort_values('sentiment_score', ascending=False).head(10))\n",
    "\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# Fun√ß√£o para simplificar coment√°rios longos\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "# 7. Fun√ß√£o para plotar gr√°fico individual para cada canal, agora exibindo o n√∫mero de likes\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    likes = df['likes']\n",
    "    \n",
    "    spacing = 1.5  # aumenta a dist√¢ncia entre as barras\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(10, len(df) * 1.5))\n",
    "    bars = plt.barh(y_positions, likes, color='green')\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('N√∫mero de Likes')\n",
    "    plt.title(f'Top 10 Coment√°rios Mais Curtidos - Canal: {canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o n√∫mero de likes ao lado de cada barra\n",
    "    for y, like in zip(y_positions, likes):\n",
    "        plt.text(like + 0.5, y, f'{like}', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 8. Cria um gr√°fico para cada canal\n",
    "for channel, group in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(group, channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove o cache Remove-Item -Recurse -Force \"$env:USERPROFILE\\.cache\\huggingface\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0944",
   "metadata": {},
   "source": [
    "##### An√°lise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ====== 1. Carrega os dados JSON ======\n",
    "with open(r'form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Elimina duplicatas baseado no texto ======\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# ====== 3. Adiciona likes aleat√≥rios para simula√ß√£o ======\n",
    "df['likes'] = [random.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Filtra os 200 melhores por likes ======\n",
    "top200 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# ====== 5. Remove palavras banidas ======\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "top200_filtrado = top200[~top200['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# ====== 6. Cria pipeline de an√°lise de sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# ====== 7. Aplica o modelo de an√°lise de sentimento ======\n",
    "batch_size = 32\n",
    "top200_filtrado = top200_filtrado.reset_index(drop=True)\n",
    "texts = top200_filtrado['text'].tolist()\n",
    "results = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results.extend(sentiment_analyzer(batch, truncation=True))\n",
    "\n",
    "# ====== 8. Normaliza os scores de sentimento ======\n",
    "scores = [(int(res['label'][0]) - 1) / 4 for res in results]\n",
    "top200_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# ====== 9. Adiciona um campo \"channel_name\" ======\n",
    "top200_filtrado['channel_name'] = top200_filtrado['author'].apply(lambda a: a['username'])\n",
    "\n",
    "# ====== 10. Pega os top 10 coment√°rios por canal ======\n",
    "top_comentarios_canal = []\n",
    "for canal, grupo in top200_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(grupo.sort_values('sentiment_score', ascending=False).head(10))\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# ====== 11. Fun√ß√µes auxiliares ======\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "\n",
    "def estrela_para_sentimento(score):\n",
    "    \"\"\"\n",
    "    Converte score normalizado (0-1) para sentimento textual em 1-5 estrelas.\n",
    "    \"\"\"\n",
    "    stars = int(round(score * 4)) + 1\n",
    "    if stars == 1:\n",
    "        return \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        return \"negativo\"\n",
    "    elif stars == 3:\n",
    "        return \"neutro\"\n",
    "    elif stars == 4:\n",
    "        return \"positivo\"\n",
    "    else:\n",
    "        return \"muito positivo\"\n",
    "\n",
    "# ====== 12. Fun√ß√£o para gerar gr√°fico com sentimento ======\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    sentiment_scores = df['sentiment_score']\n",
    "    \n",
    "    spacing = 1.5\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(12, len(df) * 1.5))\n",
    "    plt.barh(y_positions, sentiment_scores, color='skyblue')  # azul claro\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('Score de Sentimento (0 - Negativo, 1 - Positivo)')\n",
    "    plt.title(f'Coment√°rios e Sentimento - @{canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o score de sentimento e texto do sentimento ao lado de cada barra\n",
    "    for y, score in zip(y_positions, sentiment_scores):\n",
    "        sentimento = estrela_para_sentimento(score)\n",
    "        plt.text(score + 0.01, y, f'{score:.2f} ({sentimento})', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ====== 13. Gera o gr√°fico para cada canal ======\n",
    "for canal, grupo in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(grupo, canal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56a672",
   "metadata": {},
   "source": [
    "### Dashbord Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7573c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "\n",
    "# ====== 1. Configura√ß√µes iniciais ======\n",
    "DATA_DIR = 'form_data'\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'r', encoding='utf-8') as f:\n",
    "    USER_ID = json.load(f)['last_user_id']\n",
    "USER_JSON = os.path.join(DATA_DIR, f'{USER_ID}.json')\n",
    "USER_IMAGE_PATH = os.path.join(DATA_DIR, f'{USER_ID}_selfie.png')\n",
    "TWEETS_JSON = os.path.join(DATA_DIR, 'tweets_User.json')\n",
    "\n",
    "# ====== 2. Carrega dados de usu√°rio ======\n",
    "with open(USER_JSON, 'r', encoding='utf-8') as f:\n",
    "    user = json.load(f)\n",
    "\n",
    "# ====== 2.1 Classifica√ß√£o de Produtos ======\n",
    "COLLABS_KEYWORDS = ['Adidas', 'Batman', 'Champion', 'My Hero Academia', 'Zor', 'New Era']\n",
    "COLECOES_KEYWORDS = ['Classic', 'Clutch', 'Furia', 'Future is Black', 'Hard To Love x Harder To Kill', 'Magic Panthera', 'Spray It']\n",
    "produtos = user.get('produtos_furia', [])\n",
    "collabs = [p for p in produtos if p in COLLABS_KEYWORDS]\n",
    "colecoes = [p for p in produtos if p in COLECOES_KEYWORDS]\n",
    "produtos_restantes = [p for p in produtos if p not in COLLABS_KEYWORDS + COLECOES_KEYWORDS]\n",
    "\n",
    "# ====== 3. Carrega e processa tweets ======\n",
    "df = pd.read_json(TWEETS_JSON)\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "gen = random.Random(42)\n",
    "df['likes'] = [gen.randint(1, 5000) for _ in range(len(df))]\n",
    "top = df.sort_values('likes', ascending=False).head(300)\n",
    "\n",
    "# ====== 4. An√°lise de Sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    ")\n",
    "results = sentiment_analyzer(top['text'].tolist(), truncation=True)\n",
    "top['sentiment_score'] = [(int(r['label'][0]) - 1) / 4 for r in results]\n",
    "\n",
    "def score_to_star(score):\n",
    "    return int(round(score * 4)) + 1\n",
    "\n",
    "top['stars'] = top['sentiment_score'].apply(score_to_star)\n",
    "\n",
    "# ====== 5. Estat√≠sticas de Texto ======\n",
    "STOPWORDS = set([\n",
    "    'a', 'o', 'as', 'os', 'e', '√©', 'de', 'do', 'da', 'dos', 'das', 'em', 'no', 'na', 'nos', 'nas',\n",
    "    'um', 'uma', 'uns', 'umas', 'para', 'por', 'com', 'sem', 'que', 'qui', 'on', 'the', 'and', 'is', 'in', 'to', 'of', 'it', 'you', 'for', 'this'\n",
    "])\n",
    "word_counts = Counter()\n",
    "for text in top['text']:\n",
    "    for w in text.lower().split():\n",
    "        w_clean = ''.join(ch for ch in w if ch.isalpha())\n",
    "        if w_clean and w_clean not in STOPWORDS:\n",
    "            word_counts[w_clean] += 1\n",
    "\n",
    "top_words = word_counts.most_common(10)\n",
    "\n",
    "# ====== 6. Gera√ß√£o de Gr√°ficos ======\n",
    "fig_sentiment = px.bar(\n",
    "    x=top['stars'].value_counts().sort_index().index.astype(str) + '‚òÖ',\n",
    "    y=top['stars'].value_counts().sort_index().values,\n",
    "    title='Distribui√ß√£o de Sentimento por Estrelas',\n",
    "    labels={'x': 'Estrelas', 'y': 'Contagem'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig_words = px.bar(\n",
    "    x=[w for w, _ in top_words],\n",
    "    y=[cnt for _, cnt in top_words],\n",
    "    title='Top 10 Palavras Mais Usadas',\n",
    "    labels={'x': 'Palavra', 'y': 'Frequ√™ncia'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# ====== 7. Frase mais positiva e F√£ % ======\n",
    "best_idx = top['sentiment_score'].idxmax()\n",
    "best_phrase = top.loc[best_idx, 'text']\n",
    "fan_pct = top['sentiment_score'].mean() * 100\n",
    "\n",
    "# ====== 7.1 Define emoji conforme n√≠vel de f√£ ======\n",
    "if fan_pct > 75:\n",
    "    fan_emoji = 'üñ§'  # Pantera\n",
    "elif fan_pct > 60:\n",
    "    fan_emoji = 'üòä'  # feliz\n",
    "elif fan_pct > 45:\n",
    "    fan_emoji = 'üòê'  # sem emo√ß√£o\n",
    "elif fan_pct > 25:\n",
    "    fan_emoji = '‚òπÔ∏è'  # triste\n",
    "else:\n",
    "    fan_emoji = 'üò≠'  # chorando\n",
    "\n",
    "# ====== 8. Encode da Imagem ======\n",
    "encoded_image = base64.b64encode(open(USER_IMAGE_PATH, 'rb').read()).decode()\n",
    "IMAGE_SRC = f'data:image/png;base64,{encoded_image}'\n",
    "\n",
    "# ====== 9. Monta App Dash ======\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div(\n",
    "    style={\n",
    "        'backgroundColor': '#121212',\n",
    "        'color': '#e0e0e0',\n",
    "        'fontFamily': 'Inter, sans-serif',\n",
    "        'padding': '20px'\n",
    "    },\n",
    "    children=[\n",
    "        html.H1(\n",
    "            'Dashboard Geral: F√£ da FURIA',\n",
    "            style={'textAlign': 'center', 'color': '#fff', 'fontFamily': 'Georgia, serif'}\n",
    "        ),\n",
    "        html.Div(\n",
    "            style={'display': 'flex', 'gap': '20px'},\n",
    "            children=[\n",
    "                html.Div(\n",
    "                    style={\n",
    "                        'flex': '2',\n",
    "                        'backgroundColor': '#1e1e1e',\n",
    "                        'padding': '20px',\n",
    "                        'borderRadius': '10px',\n",
    "                        'boxShadow': '0 4px 15px rgba(0,0,0,0.2)'\n",
    "                    },\n",
    "                    children=[\n",
    "                        html.Img(\n",
    "                            src=IMAGE_SRC,\n",
    "                            style={'width': '120px', 'borderRadius': '8px', 'marginBottom': '15px', 'display': 'block', 'margin': '0 auto'}\n",
    "                        ),\n",
    "                        html.Blockquote(\n",
    "                            best_phrase,\n",
    "                            style={\n",
    "                                'borderLeft': '4px solid #4caf50',\n",
    "                                'padding': '10px 15px',\n",
    "                                'backgroundColor': '#2a2a2a',\n",
    "                                'fontStyle': 'italic',\n",
    "                                'fontFamily': 'Inter, sans-serif'\n",
    "                            }\n",
    "                        ),\n",
    "                       html.P([\n",
    "                            'Nome: ',\n",
    "                            html.Span(user['nome'], style={'color': '#4caf50' if user.get('name_match') else '#888'})\n",
    "                        ]),\n",
    "                        html.P([\n",
    "                            'Face match: ',\n",
    "                            html.Span(\n",
    "                                'Validado' if user.get('name_match') else 'N√£o verificado',\n",
    "                                style={'color': '#4caf50' if user.get('name_match') else '#888'}\n",
    "                            )\n",
    "                        ]),\n",
    "                        html.P([\n",
    "                            'Naturalidade: ',\n",
    "                            html.Span(\n",
    "                                user.get('naturalidade_extraida') or '(n√£o verificado)',\n",
    "                                style={'color': '#4caf50' if user.get('naturalidade_extraida') else '#888'}\n",
    "                            )\n",
    "                        ]),\n",
    "\n",
    "                        html.P(f'Porcentagem de f√£: {fan_pct:.1f}% {fan_emoji}')\n",
    "                    ]\n",
    "                ),\n",
    "                html.Div(\n",
    "                    style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '20px', 'borderRadius': '10px', 'boxShadow': '0 4px 15px rgba(0,0,0,0.2)'},\n",
    "                    children=[\n",
    "                        html.H3('Interesses e Atividades', style={'fontFamily': 'Georgia, serif'}),\n",
    "                        html.P('Jogos Favoritos: ' + ', '.join(user.get('jogos_furia', []))),\n",
    "                        html.P('Produtos Comprados: ' + ', '.join(produtos_restantes)),\n",
    "                        html.P('Collabs: ' + (', '.join(collabs) if collabs else 'Nenhum')),\n",
    "                        html.P('Cole√ß√µes: ' + (', '.join(colecoes) if colecoes else 'Nenhuma')),\n",
    "                        html.P('Eventos: ' + user.get('eventos_furia', ''))\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        html.Div(\n",
    "            style={'display': 'flex', 'gap': '20px', 'marginTop': '30px'},\n",
    "            children=[\n",
    "                html.Div(dcc.Graph(figure=fig_words), style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '10px', 'borderRadius': '8px'}),\n",
    "                html.Div(dcc.Graph(figure=fig_sentiment), style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '10px', 'borderRadius': '8px'})\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e65090fbbfcb5a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Estrutura√ß√£o do Notebook</h3> </div>\n",
    "\n",
    "- Organizar o notebook em se√ß√µes claras conforme as etapas acima: **Coleta de Dados**, **Valida√ß√£o de Identidade**, **Integra√ß√£o de Redes Sociais**, **Enriquecimento com Dados Sociais**, **Conclus√£o**.  \n",
    "- Incluir explica√ß√µes breves em cada se√ß√£o usando c√©lulas Markdown, resumindo o objetivo daquela etapa. Combinar descri√ß√µes em texto com c√©lulas de c√≥digo demonstrativas.  \n",
    "- Sugerir bibliotecas espec√≠ficas no contexto de cada etapa: por exemplo, mencionar `ipywidgets` ou `streamlit` na coleta de dados, `pytesseract`/`OpenCV` na valida√ß√£o de documentos, `tweepy`/`PRAW`/`BeautifulSoup` na integra√ß√£o social, e `transformers`/`spaCy` na an√°lise de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b8c4d4857ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfadd7aa567531f2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Dicas de Apresenta√ß√£o do Prot√≥tipo</h3> </div>\n",
    "\n",
    "- **Formata√ß√£o Atraente:** Usar cabe√ßalhos (`#`, `##`), listas e imagens (logotipos de e-sports, √≠cones de redes sociais) para tornar o notebook visualmente agrad√°vel. C√©lulas Markdown bem elaboradas ajudam na legibilidade.  \n",
    "- **Interatividade:** Incluir elementos interativos (sliders, bot√µes de upload, caixas de sele√ß√£o) via `ipywidgets` para simular um fluxo real de uso. Isso torna a demonstra√ß√£o din√¢mica mesmo no ambiente de notebook.  \n",
    "- **Visualiza√ß√£o de Dados:** Aproveitar gr√°ficos (matplotlib, seaborn ou plotly) para mostrar perfis de interesse ou resultados das an√°lises de coment√°rios. Um gr√°fico de barras ou nuvem de palavras torna o conte√∫do mais did√°tico.  \n",
    "- **Narra√ß√£o do C√≥digo:** Inserir coment√°rios explicativos e outputs de exemplo que guiem o avaliador pelo processo passo a passo. Ao final, apresentar um breve resumo dos resultados obtidos para evidenciar que todos os requisitos foram atendidos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37fafd0bfde477",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Conclus√£o</h3> </div>\n",
    "\n",
    "Este plano garante uma implementa√ß√£o completa dos requisitos do desafio, integrando coleta de informa√ß√µes pessoais e de interesse em e-sports, valida√ß√£o de identidade baseada em IA, simula√ß√£o de integra√ß√£o social e enriquecimento de perfil com dados externos. A organiza√ß√£o em se√ß√µes claras, o uso de bibliotecas especializadas (ex: **Streamlit/ipywidgets** para interfaces, **OpenCV/face_recognition** para valida√ß√£o, **transformers/spaCy** para IA, **pandas** para dados) e as sugest√µes de apresenta√ß√£o asseguram uma entrega alinhada e de f√°cil acompanhamento, mesmo no formato de notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
