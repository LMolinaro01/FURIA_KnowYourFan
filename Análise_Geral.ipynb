{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan (Análise Geral) </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e774306",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "#### Importação Chave API (Arquivo .env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd5d8a7e76898c",
   "metadata": {},
   "source": [
    "### Coleta de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58c9e8",
   "metadata": {},
   "source": [
    "##### Comentários YTB - JSON (Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea82564d0b8b86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:08:55.632710Z",
     "start_time": "2025-04-30T14:08:43.298626Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Testa se a chave da API está sendo carregada\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "\n",
    "# ID do vídeo do qual você quer obter os comentários\n",
    "video_id = '8aIcU-_5W34'\n",
    "\n",
    "\n",
    "def get_video_channel_name(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Obtém as informações do vídeo\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição e pega o nome do canal\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "        return channel_name\n",
    "    return None\n",
    "\n",
    "# Função para obter comentários do vídeo\n",
    "def get_comments(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Lista para armazenar os comentários\n",
    "    comments = []\n",
    "    \n",
    "    channel_name = get_video_channel_name(video_id, api_key)\n",
    "    \n",
    "    # Inicializa a requisição para obter os comentários\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100  # Max resultados por requisição (pode ajustar conforme necessário)\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Itera sobre os comentários e armazena os dados\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                    'video_id': video_id,                    # ← aqui você adiciona o ID do vídeo\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'text': comment['textDisplay'],\n",
    "                    'published_at': comment['publishedAt'],\n",
    "                    'likes': comment['likeCount'],\n",
    "                    'channel_name': channel_name\n",
    "                }\n",
    "            \n",
    "            comments.append(comment_data)\n",
    "        \n",
    "        # Verifica se existe uma próxima página de resultados\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Obter os comentários\n",
    "# Função para salvar os comentários sem sobrescrever o arquivo existente\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    # Caminho da pasta onde você deseja salvar o arquivo\n",
    "    pasta = 'form_data'\n",
    "\n",
    "    # Certifique-se de que a pasta existe\n",
    "    if not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "    # Caminho completo do arquivo JSON\n",
    "    arquivo_json = os.path.join(pasta, filename)\n",
    "    \n",
    "    # Verifica se o arquivo já existe\n",
    "    if os.path.exists(arquivo_json):\n",
    "        # Carrega o conteúdo existente\n",
    "        with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "            existing_comments = json.load(f)\n",
    "        \n",
    "        # Adiciona os novos comentários ao conteúdo existente\n",
    "        existing_comments.extend(comments)\n",
    "        \n",
    "        # Salva o conteúdo atualizado\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_comments, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        # Caso o arquivo não exista, cria um novo com os comentários\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Obter os comentários\n",
    "comments = get_comments(video_id, api_key)\n",
    "\n",
    "# Salvar os comentários sem sobrescrever o arquivo\n",
    "save_comments(comments)\n",
    "\n",
    "print(\"Comentários salvos em 'form_data/comentarios_video.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8841c4",
   "metadata": {},
   "source": [
    "#### Comentários YTB - JSON - GUI (Custom Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install customtkinter google-api-python-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4942f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import customtkinter as ctk\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tkinter.messagebox as msgbox\n",
    "\n",
    "# Load .env\n",
    "env_path = Path('.idea/.env')  # Ajuste conforme necessário\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "# Detecta ID do vídeo\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Detecta ID da playlist\n",
    "def extract_playlist_id(url):\n",
    "    match = re.search(r\"[?&]list=([a-zA-Z0-9_-]+)\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Pega nome do canal\n",
    "def get_video_channel_name(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    request = youtube.videos().list(part='snippet', id=video_id)\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        return response['items'][0]['snippet']['channelTitle']\n",
    "    return None\n",
    "\n",
    "# Busca comentários\n",
    "def get_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    comments = []\n",
    "    channel_name = get_video_channel_name(video_id)\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                'video_id': video_id,\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'text': comment['textDisplay'],\n",
    "                'published_at': comment['publishedAt'],\n",
    "                'likes': comment['likeCount'],\n",
    "                'channel_name': channel_name\n",
    "            }\n",
    "            comments.append(comment_data)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "# Salva no JSON\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    pasta = 'form_data'\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "    caminho = os.path.join(pasta, filename)\n",
    "    if os.path.exists(caminho):\n",
    "        with open(caminho, 'r', encoding='utf-8') as f:\n",
    "            existentes = json.load(f)\n",
    "        existentes.extend(comments)\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existentes, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Busca vídeos de uma playlist\n",
    "def get_video_ids_from_playlist(playlist_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    video_ids = []\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "        request = youtube.playlistItems().list_next(request, response)\n",
    "    return video_ids\n",
    "\n",
    "# Processa link inserido\n",
    "def process_link():\n",
    "    url = entry_url.get().strip()\n",
    "    if not url:\n",
    "        status_label.configure(text=\"Cole um link válido do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    playlist_id = extract_playlist_id(url)\n",
    "    video_id = extract_video_id(url)\n",
    "\n",
    "    if playlist_id:\n",
    "        try:\n",
    "            video_ids = get_video_ids_from_playlist(playlist_id)\n",
    "            total_comments = []\n",
    "            for vid in video_ids:\n",
    "                status_label.configure(text=f\"Buscando comentários de {vid}...\", text_color=\"blue\")\n",
    "                comments = get_comments(vid)\n",
    "                total_comments.extend(comments)\n",
    "            save_comments(total_comments)\n",
    "            status_label.configure(text=f\"Todos os comentários da playlist foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "\n",
    "    elif video_id:\n",
    "        try:\n",
    "            status_label.configure(text=\"Buscando comentários do vídeo...\", text_color=\"blue\")\n",
    "            comments = get_comments(video_id)\n",
    "            save_comments(comments)\n",
    "            status_label.configure(text=f\"Comentários do vídeo foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "    else:\n",
    "        status_label.configure(text=\"Link inválido. Verifique se é um link do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    # Pergunta se deseja adicionar mais\n",
    "    continuar = msgbox.askyesno(\"Continuar\", \"Deseja adicionar outro vídeo ou playlist?\")\n",
    "    if continuar:\n",
    "        entry_url.delete(0, 'end')\n",
    "        status_label.configure(text=\"Cole outro link para continuar.\", text_color=\"black\")\n",
    "    else:\n",
    "        app.quit()\n",
    "        app.destroy()\n",
    "\n",
    "# GUI com customtkinter\n",
    "ctk.set_appearance_mode(\"System\")\n",
    "ctk.set_default_color_theme(\"blue\")\n",
    "\n",
    "app = ctk.CTk()\n",
    "app.title(\"Coletor de Comentários YouTube\")\n",
    "app.geometry(\"600x300\")\n",
    "\n",
    "label = ctk.CTkLabel(app, text=\"Cole o link do vídeo ou playlist do YouTube:\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "entry_url = ctk.CTkEntry(app, width=500)\n",
    "entry_url.pack(pady=10)\n",
    "\n",
    "submit_button = ctk.CTkButton(app, text=\"Buscar e Salvar Comentários\", command=process_link)\n",
    "submit_button.pack(pady=20)\n",
    "\n",
    "status_label = ctk.CTkLabel(app, text=\"\")\n",
    "status_label.pack(pady=10)\n",
    "\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5021376815ef2",
   "metadata": {},
   "source": [
    "#### Comentários - JSON (Twitter/X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5376ad21a39994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ————————— Configurações e autenticação —————————\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Parâmetros de pesquisa com a hashtag #DIADEFURIA incluída\n",
    "query = '(Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR Trigo OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson OR #FURIACS OR #FURIAR6 OR #FURIAFC OR #DIADEFURIA) -is:retweet lang:pt'\n",
    "max_results = 10\n",
    "\n",
    "# Fazendo a busca com os campos desejados\n",
    "tweets = client.search_recent_tweets(query=query, max_results=max_results,\n",
    "                                     tweet_fields=[\"author_id\", \"conversation_id\", \"created_at\", \"geo\", \"id\", \"lang\", \"source\", \"text\"],\n",
    "                                     user_fields=[\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"url\", \"username\"],\n",
    "                                     expansions=[\"author_id\"])\n",
    "\n",
    "# Convertendo os tweets para um formato de dicionário\n",
    "tweets_data = []\n",
    "if tweets.data:\n",
    "    for tweet in tweets.data:\n",
    "        tweet_info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'author_id': tweet.author_id,\n",
    "            'conversation_id': tweet.conversation_id,\n",
    "            'geo': tweet.geo,\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source\n",
    "        }\n",
    "\n",
    "        # Obtendo informações do usuário (quem postou o tweet)\n",
    "        if tweets.includes and 'users' in tweets.includes:\n",
    "            for user in tweets.includes['users']:\n",
    "                if user.id == tweet.author_id:\n",
    "                    tweet_info['user'] = {\n",
    "                        'created_at': str(user.created_at),\n",
    "                        'description': user.description,\n",
    "                        'entities': user.entities,\n",
    "                        'location': user.location,\n",
    "                        'name': user.name,\n",
    "                        'url': user.url,\n",
    "                        'username': user.username\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "        tweets_data.append(tweet_info)\n",
    "\n",
    "# Caminho da pasta onde você deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, 'tweetsGerais_furia.json')\n",
    "\n",
    "# Carrega o conteúdo existente, se houver\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Adiciona os novos tweets\n",
    "dados_existentes.extend(tweets_data)\n",
    "\n",
    "# Salva de volta no JSON\n",
    "with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_existentes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tweets adicionados com sucesso a 'tweetsGerais_furia.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9cecf67adf235",
   "metadata": {},
   "source": [
    "#### Posts - JSON (Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb4487c423268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "query = \"FURIA OR Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson\"\n",
    "\n",
    "subreddits = [\"GlobalOffensive\", \"csgo\", \"VALORANT\", \"cs2\", \"cblol\", \"LolEsports\", \"ValorantCompetitive\", \"VCT\", \"R6ProLeague\"]\n",
    "limit = 50\n",
    "\n",
    "resultados = []\n",
    "\n",
    "# Loop pelos subreddits\n",
    "for subreddit in subreddits:\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr=on&limit={limit}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"data\" in data and \"children\" in data[\"data\"]:\n",
    "        for post in data[\"data\"][\"children\"]:\n",
    "            p = post[\"data\"]\n",
    "\n",
    "            # Verifica se a query aparece no título ou no texto do post\n",
    "            titulo = p.get(\"title\", \"\")\n",
    "            texto = p.get(\"selftext\", \"\")\n",
    "\n",
    "            # Verificação literal, sem usar lower()\n",
    "            if any(jogador in titulo or jogador in texto for jogador in query.split(\" OR \")):\n",
    "                resultados.append({\n",
    "                    \"titulo\": p.get(\"title\"),\n",
    "                    \"autor\": p.get(\"author\"),\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"score\": p.get(\"score\", 0),\n",
    "                    \"url\": \"https://reddit.com\" + p.get(\"permalink\"),\n",
    "                    \"data_criacao\": p.get(\"created_utc\"),\n",
    "                    \"comentario_exemplo\": p.get(\"selftext\", \"\")\n",
    "                })\n",
    "\n",
    "# Caminho da pasta onde você deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, \"posts_furia_reddit.json\")\n",
    "\n",
    "# Salva em JSON\n",
    "# Verifica se o arquivo já existe e carrega os dados antigos\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Junta os dados antigos com os novos\n",
    "dados_atuaisizados = dados_existentes + resultados\n",
    "\n",
    "# Salva todos os dados no JSON\n",
    "with open(arquivo_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dados_atuaisizados, f, indent=4, ensure_ascii=False)\n",
    "    print(\"Novos dados adicionados ao JSON.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5023f63bcefe89",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Enriquecimento de Perfil com Dados Sociais e Multimídia</h3> </div>\n",
    "\n",
    "- **Análise de Comentários:** Para integrar comentários prévios do usuário no YouTube, Reddit e Twitter, incluir blocos que consumam APIs ou dados locais de análise anterior (supondo que existam). Usar `google-api-python-client` para extrair comentários de vídeos de e-sports do YouTube, `PRAW` para posts/comentários no Reddit, e `tweepy` ou dados simulados para tweets.  \n",
    "- **Processamento de Linguagem Natural:** Aplicar NLP para entender o perfil do usuário: usar bibliotecas como `transformers` ou `spaCy` para classificar sentimento, identificar tópicos ou palavras-chave frequentes nesses comentários. Por exemplo, gerar um gráfico de palavras-chave mais mencionadas em e-sports, ou uma análise de sentimento geral sobre jogos específicos.  \n",
    "- **Integração de Informações:** Combinar esses insights com os interesses declarados pelo usuário. Exibir visualmente (via `matplotlib` ou `seaborn`) uma nuvem de palavras ou gráfico que mostre as categorias de e-sports mais relevantes para o perfil (baseado em interesses + análise de comentários).  \n",
    "- **Perfis em Sites de e-Sports:** Permitir que o usuário insira links para seus perfis em plataformas de e-sports (como GameBattles, HLTV, Liquipedia). Usar `requests` e `BeautifulSoup` para raspar detalhes do perfil (jogos, histórico de partidas). Em seguida, aplicar um modelo de IA (ex: `transformers` BERT) para classificar se o conteúdo textual do perfil é relevante às preferências do usuário (por exemplo, buscando termos de jogos citados pelo usuário). Mostrar se há “match” entre interesses do usuário e informações do perfil scraped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eddccd",
   "metadata": {},
   "source": [
    "### Análise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f4811",
   "metadata": {},
   "source": [
    "#### Análise Geral (Youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5714698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc151832ad30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_json(R'form_data\\comentarios_video.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050497bf",
   "metadata": {},
   "source": [
    "Remoção de Duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unico = df.drop_duplicates(subset=[\"text\", \"author\"])\n",
    "\n",
    "# Salva o DataFrame limpo de volta no JSON\n",
    "df_unico.to_json(R\"form_data\\comentarios_video.json\", orient=\"records\", indent=4, force_ascii=False)\n",
    "\n",
    "print(f\"Removidas {len(df) - len(df_unico)} duplicatas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converte published_at (ISO 8601) para datetime do pandas\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], utc=True)\n",
    "\n",
    "# 3. Extrai colunas de data e hora para facilitar agregações\n",
    "df['data'] = df['published_at'].dt.date\n",
    "df['hora'] = df['published_at'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Filtra comentários com entre 1 e 45 likes\n",
    "df_filtrado = df[(df['likes'] > 0) & (df['likes'] <= 45)]\n",
    "\n",
    "# 3. Plota histograma com bins de tamanho 2\n",
    "plt.figure()\n",
    "plt.hist(df_filtrado['likes'], bins=range(0, 47, 2), edgecolor='black')\n",
    "plt.title('Distribuição de Likes (1 a 45 likes, bins de 2 em 2)')\n",
    "plt.xlabel('Número de Likes')\n",
    "plt.ylabel('Frequência')\n",
    "\n",
    "# 4. Ajusta os ticks do eixo x para mostrar de 2 em 2\n",
    "plt.xticks(range(0, 47, 2), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Exibe quantos comentários ficaram fora desse intervalo\n",
    "total = df.shape[0]\n",
    "mantidos = df_filtrado.shape[0]\n",
    "removidos = total - mantidos\n",
    "print(f\"{removidos} comentários foram removidos por terem 0 ou mais de 45 likes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24822976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Ordena pelo número de likes e pega os top N (ajuste N se quiser mais/menos)\n",
    "N = 10\n",
    "top_comments = df.sort_values('likes', ascending=False).head(N)\n",
    "\n",
    "# 3. Prepara os rótulos: quebra linhas para caber melhor no gráfico\n",
    "wrapped_texts = [\n",
    "    textwrap.fill(text, width=50)\n",
    "    for text in top_comments['text']\n",
    "]\n",
    "\n",
    "# 4. Plota gráfico de barras horizontais com o texto do comentário\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(N), top_comments['likes'], edgecolor='black')\n",
    "plt.yticks(range(N), wrapped_texts)\n",
    "plt.xlabel('Número de Likes')\n",
    "plt.title(f'Top {N} Comentários Mais Curtidos')\n",
    "plt.gca().invert_yaxis()  # Inverte o eixo para o comentário mais curtido ficar no topo\n",
    "\n",
    "# 5. Anotações com o número exato de likes ao final de cada barra\n",
    "for i, like in enumerate(top_comments['likes']):\n",
    "    plt.text(like + 5, i, str(like), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 autores por número de comentários\n",
    "top_autores = df['author'].value_counts().head(10)\n",
    "plt.figure()\n",
    "plt.bar(top_autores.index, top_autores.values)\n",
    "plt.title('Top 10 Autores por Número de Comentários')\n",
    "plt.xlabel('Autor')\n",
    "plt.ylabel('Quantidade de Comentários')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "# 2. Converte published_at para datetime e extrai ano e mês\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], utc=True)\n",
    "df['year'] = df['published_at'].dt.year\n",
    "df['month'] = df['published_at'].dt.month\n",
    "\n",
    "# 3. Filtra para ignorar anos muito distantes (por exemplo, antes de 2000)\n",
    "df = df[df['year'] >= 2000]\n",
    "\n",
    "# 4. Para cada ano presente no DataFrame, gera um gráfico mensal\n",
    "for year in sorted(df['year'].unique()):\n",
    "    df_year = df[df['year'] == year]\n",
    "    # Agrupa por mês (1 a 12), preenchendo meses ausentes com zero\n",
    "    comentarios_por_mes = df_year.groupby('month').size().reindex(range(1, 13), fill_value=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(comentarios_por_mes.index, comentarios_por_mes.values, marker='o')\n",
    "    plt.title(f'Comentários por Mês em {year}')\n",
    "    plt.xlabel('Mês')\n",
    "    plt.ylabel('Número de Comentários')\n",
    "    \n",
    "    # Usa nomes de meses no eixo x\n",
    "    labels = [calendar.month_name[m] for m in comentarios_por_mes.index]\n",
    "    plt.xticks(ticks=comentarios_por_mes.index, labels=labels, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Caminho do JSON\n",
    "JSON_PATH = 'form_data/comentarios_video.json'\n",
    "\n",
    "# 2. Carrega o DataFrame\n",
    "df = pd.read_json(JSON_PATH)\n",
    "\n",
    "# 3. Top 10 comentários mais curtidos por canal\n",
    "top_por_canal = (\n",
    "    df\n",
    "    .sort_values(['channel_name', 'likes'], ascending=[True, False])\n",
    "    .groupby('channel_name')\n",
    "    .head(25)\n",
    ")\n",
    "\n",
    "# 4. Pipeline de sentimento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 5. Aplica o modelo em lotes\n",
    "def analyze_batch(texts):\n",
    "    return sentiment_analyzer(texts, truncation=True)\n",
    "\n",
    "results = []\n",
    "batch_size = 32\n",
    "texts = top_por_canal['text'].tolist()\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    results.extend(analyze_batch(texts[i:i+batch_size]))\n",
    "\n",
    "# 6. Processa os resultados\n",
    "scores, labels = [], []\n",
    "for res in results:\n",
    "    stars = int(res['label'][0])\n",
    "    score = (stars - 1) / 4\n",
    "    if stars == 1:\n",
    "        sentiment = \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        sentiment = \"negativo\"\n",
    "    elif stars == 3:\n",
    "        sentiment = \"neutro\"\n",
    "    elif stars == 4:\n",
    "        sentiment = \"positivo\"\n",
    "    else:\n",
    "        sentiment = \"muito positivo\"\n",
    "    scores.append(score)\n",
    "    labels.append(sentiment)\n",
    "\n",
    "top_por_canal = top_por_canal.reset_index(drop=True)\n",
    "top_por_canal['sentiment_score'] = scores\n",
    "top_por_canal['sentiment'] = labels\n",
    "\n",
    "# 7. Classificação por faixa de média\n",
    "def classificar_media(score):\n",
    "    if score <= 0.2:\n",
    "        return \"muito negativo\"\n",
    "    elif score <= 0.3:\n",
    "        return \"negativo\"\n",
    "    elif score <= 0.43:\n",
    "        return \"neutro\"\n",
    "    elif score <= 0.5:\n",
    "        return \"levemente positivo\"\n",
    "    elif score <= 0.65:\n",
    "        return \"positivo\"\n",
    "    elif score <= 0.8:\n",
    "        return \"muito positivo\"\n",
    "    else:\n",
    "        return \"extremamente positivo\"\n",
    "\n",
    "# 8. Agrupamento por canal com contagem\n",
    "resumo_canais = (\n",
    "    top_por_canal\n",
    "    .groupby('channel_name')\n",
    "    .agg(\n",
    "        mean_sentiment_score=('sentiment_score', 'mean'),\n",
    "        total_comentarios=('sentiment_score', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 9. Aplica rótulo textual e monta nome com contagem\n",
    "resumo_canais['sentiment_class'] = resumo_canais['mean_sentiment_score'].apply(classificar_media)\n",
    "resumo_canais['channel_label'] = resumo_canais.apply(\n",
    "    lambda row: f\"{row['channel_name']}\", axis=1\n",
    ")\n",
    "\n",
    "# 10. Gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(resumo_canais['channel_label'], resumo_canais['mean_sentiment_score'], color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Média do Sentimento (0 = ruim, 1 = ótimo)')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Média de Sentimento por Canal (Top 25 Comentários Mais Curtidos)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 11. Anota a classificação acima das barras\n",
    "for bar, label in zip(bars, resumo_canais['sentiment_class']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, label, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "# Lista de palavras banidas\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "\n",
    "# 1. Filtra os 200 melhores comentários por likes\n",
    "top100 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# 2. Filtra comentários que contenham palavras banidas\n",
    "top100_filtrado = top100[~top100['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# 3. Cria o pipeline de análise de sentimento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# 4. Aplica em lotes a análise de sentimentos\n",
    "results = []\n",
    "batch_size = 32\n",
    "texts = top100_filtrado['text'].tolist()\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    results.extend(sentiment_analyzer(texts[i:i+batch_size], truncation=True))\n",
    "\n",
    "# 5. Converte labels em score normalizado\n",
    "scores = []\n",
    "for res in results:\n",
    "    stars = int(res['label'][0])\n",
    "    score = (stars - 1) / 4  # normaliza para 0–1\n",
    "    scores.append(score)\n",
    "\n",
    "top100_filtrado = top100_filtrado.reset_index(drop=True)\n",
    "top100_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# 6. Filtra 10 comentários por canal com base no score de sentimento\n",
    "top_comentarios_canal = []\n",
    "\n",
    "for channel, group in top100_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(group.sort_values('sentiment_score', ascending=False).head(10))\n",
    "\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# Função para simplificar comentários longos\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "# 7. Função para plotar gráfico individual para cada canal, agora exibindo o número de likes\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    likes = df['likes']\n",
    "    \n",
    "    spacing = 1.5  # aumenta a distância entre as barras\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(10, len(df) * 1.5))\n",
    "    bars = plt.barh(y_positions, likes, color='green')\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('Número de Likes')\n",
    "    plt.title(f'Top 10 Comentários Mais Curtidos - Canal: {canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o número de likes ao lado de cada barra\n",
    "    for y, like in zip(y_positions, likes):\n",
    "        plt.text(like + 0.5, y, f'{like}', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 8. Cria um gráfico para cada canal\n",
    "for channel, group in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(group, channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0944",
   "metadata": {},
   "source": [
    "##### Análise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ====== 1. Carrega os dados JSON ======\n",
    "with open(r'form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Elimina duplicatas baseado no texto ======\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# ====== 3. Adiciona likes aleatórios para simulação ======\n",
    "df['likes'] = [random.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Filtra os 200 melhores por likes ======\n",
    "top200 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# ====== 5. Remove palavras banidas ======\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "top200_filtrado = top200[~top200['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# ====== 6. Cria pipeline de análise de sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# ====== 7. Aplica o modelo de análise de sentimento ======\n",
    "batch_size = 32\n",
    "top200_filtrado = top200_filtrado.reset_index(drop=True)\n",
    "texts = top200_filtrado['text'].tolist()\n",
    "results = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results.extend(sentiment_analyzer(batch, truncation=True))\n",
    "\n",
    "# ====== 8. Normaliza os scores de sentimento ======\n",
    "scores = [(int(res['label'][0]) - 1) / 4 for res in results]\n",
    "top200_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# ====== 9. Adiciona um campo \"channel_name\" ======\n",
    "top200_filtrado['channel_name'] = top200_filtrado['author'].apply(lambda a: a['username'])\n",
    "\n",
    "# ====== 10. Pega os top 10 comentários por canal ======\n",
    "top_comentarios_canal = []\n",
    "for canal, grupo in top200_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(grupo.sort_values('sentiment_score', ascending=False).head(10))\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# ====== 11. Funções auxiliares ======\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "\n",
    "def estrela_para_sentimento(score):\n",
    "    \"\"\"\n",
    "    Converte score normalizado (0-1) para sentimento textual em 1-5 estrelas.\n",
    "    \"\"\"\n",
    "    stars = int(round(score * 4)) + 1\n",
    "    if stars == 1:\n",
    "        return \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        return \"negativo\"\n",
    "    elif stars == 3:\n",
    "        return \"neutro\"\n",
    "    elif stars == 4:\n",
    "        return \"positivo\"\n",
    "    else:\n",
    "        return \"muito positivo\"\n",
    "\n",
    "# ====== 12. Função para gerar gráfico com sentimento ======\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    sentiment_scores = df['sentiment_score']\n",
    "    \n",
    "    spacing = 1.5\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(12, len(df) * 1.5))\n",
    "    plt.barh(y_positions, sentiment_scores, color='skyblue')  # azul claro\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('Score de Sentimento (0 - Negativo, 1 - Positivo)')\n",
    "    plt.title(f'Comentários e Sentimento - @{canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o score de sentimento e texto do sentimento ao lado de cada barra\n",
    "    for y, score in zip(y_positions, sentiment_scores):\n",
    "        sentimento = estrela_para_sentimento(score)\n",
    "        plt.text(score + 0.01, y, f'{score:.2f} ({sentimento})', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ====== 13. Gera o gráfico para cada canal ======\n",
    "for canal, grupo in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(grupo, canal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cce8d1",
   "metadata": {},
   "source": [
    "### Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56a672",
   "metadata": {},
   "source": [
    "#### Dashbord Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7573c17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdash\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dcc, html\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     29\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     logging,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     52\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\transformers\\utils\\__init__.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     29\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     replace_return_docstrings,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     43\u001b[39m BASIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), ...)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\__init__.py:2240\u001b[39m\n\u001b[32m   2236\u001b[39m sys.modules.setdefault(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.classes\u001b[39m\u001b[33m\"\u001b[39m, classes)\n\u001b[32m   2238\u001b[39m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[32m   2239\u001b[39m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2242\u001b[39m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\quantization\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuse_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuse_modules\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\quantization\\fake_quantize.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33;03mhere.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     _is_fake_quant_script_module,\n\u001b[32m     12\u001b[39m     _is_per_channel,\n\u001b[32m     13\u001b[39m     _is_per_tensor,\n\u001b[32m     14\u001b[39m     _is_symmetric_quant,\n\u001b[32m     15\u001b[39m     default_fake_quant,\n\u001b[32m     16\u001b[39m     default_fixed_qparams_range_0to1_fake_quant,\n\u001b[32m     17\u001b[39m     default_fixed_qparams_range_neg1to1_fake_quant,\n\u001b[32m     18\u001b[39m     default_fused_act_fake_quant,\n\u001b[32m     19\u001b[39m     default_fused_per_channel_wt_fake_quant,\n\u001b[32m     20\u001b[39m     default_fused_wt_fake_quant,\n\u001b[32m     21\u001b[39m     default_histogram_fake_quant,\n\u001b[32m     22\u001b[39m     default_per_channel_weight_fake_quant,\n\u001b[32m     23\u001b[39m     default_weight_fake_quant,\n\u001b[32m     24\u001b[39m     disable_fake_quant,\n\u001b[32m     25\u001b[39m     disable_observer,\n\u001b[32m     26\u001b[39m     enable_fake_quant,\n\u001b[32m     27\u001b[39m     enable_observer,\n\u001b[32m     28\u001b[39m     FakeQuantize,\n\u001b[32m     29\u001b[39m     FakeQuantizeBase,\n\u001b[32m     30\u001b[39m     FixedQParamsFakeQuantize,\n\u001b[32m     31\u001b[39m     FusedMovingAvgObsFakeQuantize,\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\ao\\quantization\\__init__.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mobserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpt2e\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_numeric_debugger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     13\u001b[39m     compare_results,\n\u001b[32m     14\u001b[39m     CUSTOM_KEY,\n\u001b[32m     15\u001b[39m     extract_results_from_loggers,\n\u001b[32m     16\u001b[39m     generate_numeric_debug_handle,\n\u001b[32m     17\u001b[39m     NUMERIC_DEBUG_HANDLE_KEY,\n\u001b[32m     18\u001b[39m     prepare_for_propagation_comparison,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpt2e\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     _allow_exported_model_train_eval \u001b[38;5;28;01mas\u001b[39;00m allow_exported_model_train_eval,\n\u001b[32m     22\u001b[39m     _move_exported_model_to_eval \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_eval,\n\u001b[32m     23\u001b[39m     _move_exported_model_to_train \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_train,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\_numeric_debugger.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mns\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_sqnr\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpt2e\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bfs_trace_with_node_process\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule, Node\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\graph_utils.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, Union\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msource_matcher_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     check_subgraphs_connected,\n\u001b[32m     13\u001b[39m     get_source_partitions,\n\u001b[32m     14\u001b[39m     SourcePartition,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\export\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpass_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PassResult\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpass_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PassManager\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLike\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\fx\\passes\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     graph_drawer,\n\u001b[32m      3\u001b[39m     graph_manipulation,\n\u001b[32m      4\u001b[39m     net_min_base,\n\u001b[32m      5\u001b[39m     operator_support,\n\u001b[32m      6\u001b[39m     param_fetch,\n\u001b[32m      7\u001b[39m     reinplace,\n\u001b[32m      8\u001b[39m     runtime_assert,\n\u001b[32m      9\u001b[39m     shape_prop,\n\u001b[32m     10\u001b[39m     split_module,\n\u001b[32m     11\u001b[39m     split_utils,\n\u001b[32m     12\u001b[39m     splitter_base,\n\u001b[32m     13\u001b[39m     tools_common,\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\fx\\passes\\graph_drawer.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _format_arg, _get_qualified_name\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperator_schemas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_function\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mshape_prop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorMetadata\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydot\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\fx\\passes\\shape_prop.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dispatch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect_fake_mode\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_sparse_any\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_aggregate, Node\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     DynamicOutputShapeException,\n\u001b[32m      4\u001b[39m     FakeTensor,\n\u001b[32m      5\u001b[39m     FakeTensorMode,\n\u001b[32m      6\u001b[39m     UnsupportedFakeTensorException,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossRefFakeMode\n\u001b[32m     11\u001b[39m __all__ = [\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFakeTensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFakeTensorMode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCrossRefFakeMode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leomo\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_functorch_wrapped_tensor, is_legacy_batchedtensor\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_class_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeScriptObject\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtrace_structured\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suggest_memory_format\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     assert_eq,\n\u001b[32m     30\u001b[39m     assert_metadata_eq,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     MetaConverter,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1525\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1499\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1598\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Código Referente a Análise Individual (não modificada)\n",
    "\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "\n",
    "# ====== 1. Configurações iniciais ======\n",
    "DATA_DIR = 'form_data'\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'r', encoding='utf-8') as f:\n",
    "    USER_ID = json.load(f)['last_user_id']\n",
    "USER_JSON = os.path.join(DATA_DIR, f'{USER_ID}.json')\n",
    "USER_IMAGE_PATH = os.path.join(DATA_DIR, f'{USER_ID}_selfie.png')\n",
    "TWEETS_JSON = os.path.join(DATA_DIR, 'tweets_User.json')\n",
    "\n",
    "# ====== 2. Carrega dados de usuário ======\n",
    "with open(USER_JSON, 'r', encoding='utf-8') as f:\n",
    "    user = json.load(f)\n",
    "\n",
    "# ====== 2.1 Classificação de Produtos ======\n",
    "COLLABS_KEYWORDS = ['Adidas', 'Batman', 'Champion', 'My Hero Academia', 'Zor', 'New Era']\n",
    "COLECOES_KEYWORDS = ['Classic', 'Clutch', 'Furia', 'Future is Black', 'Hard To Love x Harder To Kill', 'Magic Panthera', 'Spray It']\n",
    "produtos = user.get('produtos_furia', [])\n",
    "collabs = [p for p in produtos if p in COLLABS_KEYWORDS]\n",
    "colecoes = [p for p in produtos if p in COLECOES_KEYWORDS]\n",
    "produtos_restantes = [p for p in produtos if p not in COLLABS_KEYWORDS + COLECOES_KEYWORDS]\n",
    "\n",
    "# ====== 3. Carrega e processa tweets ======\n",
    "df = pd.read_json(TWEETS_JSON)\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "gen = random.Random(42)\n",
    "df['likes'] = [gen.randint(1, 5000) for _ in range(len(df))]\n",
    "top = df.sort_values('likes', ascending=False).head(300)\n",
    "\n",
    "# ====== 4. Análise de Sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    ")\n",
    "results = sentiment_analyzer(top['text'].tolist(), truncation=True)\n",
    "top['sentiment_score'] = [(int(r['label'][0]) - 1) / 4 for r in results]\n",
    "\n",
    "def score_to_star(score):\n",
    "    return int(round(score * 4)) + 1\n",
    "\n",
    "top['stars'] = top['sentiment_score'].apply(score_to_star)\n",
    "\n",
    "# ====== 5. Estatísticas de Texto ======\n",
    "STOPWORDS = set([\n",
    "    'a', 'o', 'as', 'os', 'e', 'é', 'de', 'do', 'da', 'dos', 'das', 'em', 'no', 'na', 'nos', 'nas',\n",
    "    'um', 'uma', 'uns', 'umas', 'para', 'por', 'com', 'sem', 'que', 'qui', 'on', 'the', 'and', 'is', 'in', 'to', 'of', 'it', 'you', 'for', 'this'\n",
    "])\n",
    "word_counts = Counter()\n",
    "for text in top['text']:\n",
    "    for w in text.lower().split():\n",
    "        w_clean = ''.join(ch for ch in w if ch.isalpha())\n",
    "        if w_clean and w_clean not in STOPWORDS:\n",
    "            word_counts[w_clean] += 1\n",
    "\n",
    "top_words = word_counts.most_common(10)\n",
    "\n",
    "# ====== 6. Geração de Gráficos ======\n",
    "fig_sentiment = px.bar(\n",
    "    x=top['stars'].value_counts().sort_index().index.astype(str) + '★',\n",
    "    y=top['stars'].value_counts().sort_index().values,\n",
    "    title='Distribuição de Sentimento por Estrelas',\n",
    "    labels={'x': 'Estrelas', 'y': 'Contagem'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig_words = px.bar(\n",
    "    x=[w for w, _ in top_words],\n",
    "    y=[cnt for _, cnt in top_words],\n",
    "    title='Top 10 Palavras Mais Usadas',\n",
    "    labels={'x': 'Palavra', 'y': 'Frequência'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# ====== 7. Frase mais positiva e Fã % ======\n",
    "best_idx = top['sentiment_score'].idxmax()\n",
    "best_phrase = top.loc[best_idx, 'text']\n",
    "fan_pct = top['sentiment_score'].mean() * 100\n",
    "\n",
    "# ====== 7.1 Define emoji conforme nível de fã ======\n",
    "if fan_pct > 75:\n",
    "    fan_emoji = '🖤'  # Pantera\n",
    "elif fan_pct > 60:\n",
    "    fan_emoji = '😊'  # feliz\n",
    "elif fan_pct > 45:\n",
    "    fan_emoji = '😐'  # sem emoção\n",
    "elif fan_pct > 25:\n",
    "    fan_emoji = '☹️'  # triste\n",
    "else:\n",
    "    fan_emoji = '😭'  # chorando\n",
    "\n",
    "# ====== 8. Encode da Imagem ======\n",
    "encoded_image = base64.b64encode(open(USER_IMAGE_PATH, 'rb').read()).decode()\n",
    "IMAGE_SRC = f'data:image/png;base64,{encoded_image}'\n",
    "\n",
    "# ====== 9. Monta App Dash ======\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div(\n",
    "    style={\n",
    "        'backgroundColor': '#121212',\n",
    "        'color': '#e0e0e0',\n",
    "        'fontFamily': 'Inter, sans-serif',\n",
    "        'padding': '20px'\n",
    "    },\n",
    "    children=[\n",
    "        html.H1(\n",
    "            'Dashboard Geral: Fã da FURIA',\n",
    "            style={'textAlign': 'center', 'color': '#fff', 'fontFamily': 'Georgia, serif'}\n",
    "        ),\n",
    "        html.Div(\n",
    "            style={'display': 'flex', 'gap': '20px'},\n",
    "            children=[\n",
    "                html.Div(\n",
    "                    style={\n",
    "                        'flex': '2',\n",
    "                        'backgroundColor': '#1e1e1e',\n",
    "                        'padding': '20px',\n",
    "                        'borderRadius': '10px',\n",
    "                        'boxShadow': '0 4px 15px rgba(0,0,0,0.2)'\n",
    "                    },\n",
    "                    children=[\n",
    "                        html.Img(\n",
    "                            src=IMAGE_SRC,\n",
    "                            style={'width': '120px', 'borderRadius': '8px', 'marginBottom': '15px', 'display': 'block', 'margin': '0 auto'}\n",
    "                        ),\n",
    "                        html.Blockquote(\n",
    "                            best_phrase,\n",
    "                            style={\n",
    "                                'borderLeft': '4px solid #4caf50',\n",
    "                                'padding': '10px 15px',\n",
    "                                'backgroundColor': '#2a2a2a',\n",
    "                                'fontStyle': 'italic',\n",
    "                                'fontFamily': 'Inter, sans-serif'\n",
    "                            }\n",
    "                        ),\n",
    "                       html.P([\n",
    "                            'Nome: ',\n",
    "                            html.Span(user['nome'], style={'color': '#4caf50' if user.get('name_match') else '#888'})\n",
    "                        ]),\n",
    "                        html.P([\n",
    "                            'Face match: ',\n",
    "                            html.Span(\n",
    "                                'Validado' if user.get('name_match') else 'Não verificado',\n",
    "                                style={'color': '#4caf50' if user.get('name_match') else '#888'}\n",
    "                            )\n",
    "                        ]),\n",
    "                        html.P([\n",
    "                            'Naturalidade: ',\n",
    "                            html.Span(\n",
    "                                user.get('naturalidade_extraida') or '(não verificado)',\n",
    "                                style={'color': '#4caf50' if user.get('naturalidade_extraida') else '#888'}\n",
    "                            )\n",
    "                        ]),\n",
    "\n",
    "                        html.P(f'Porcentagem de fã: {fan_pct:.1f}% {fan_emoji}')\n",
    "                    ]\n",
    "                ),\n",
    "                html.Div(\n",
    "                    style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '20px', 'borderRadius': '10px', 'boxShadow': '0 4px 15px rgba(0,0,0,0.2)'},\n",
    "                    children=[\n",
    "                        html.H3('Interesses e Atividades', style={'fontFamily': 'Georgia, serif'}),\n",
    "                        html.P('Jogos Favoritos: ' + ', '.join(user.get('jogos_furia', []))),\n",
    "                        html.P('Produtos Comprados: ' + ', '.join(produtos_restantes)),\n",
    "                        html.P('Collabs: ' + (', '.join(collabs) if collabs else 'Nenhum')),\n",
    "                        html.P('Coleções: ' + (', '.join(colecoes) if colecoes else 'Nenhuma')),\n",
    "                        html.P('Eventos: ' + user.get('eventos_furia', ''))\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        html.Div(\n",
    "            style={'display': 'flex', 'gap': '20px', 'marginTop': '30px'},\n",
    "            children=[\n",
    "                html.Div(dcc.Graph(figure=fig_words), style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '10px', 'borderRadius': '8px'}),\n",
    "                html.Div(dcc.Graph(figure=fig_sentiment), style={'flex': '1', 'backgroundColor': '#1e1e1e', 'padding': '10px', 'borderRadius': '8px'})\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e65090fbbfcb5a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Estruturação do Notebook</h3> </div>\n",
    "\n",
    "- Organizar o notebook em seções claras conforme as etapas acima: **Coleta de Dados**, **Validação de Identidade**, **Integração de Redes Sociais**, **Enriquecimento com Dados Sociais**, **Conclusão**.  \n",
    "- Incluir explicações breves em cada seção usando células Markdown, resumindo o objetivo daquela etapa. Combinar descrições em texto com células de código demonstrativas.  \n",
    "- Sugerir bibliotecas específicas no contexto de cada etapa: por exemplo, mencionar `ipywidgets` ou `streamlit` na coleta de dados, `pytesseract`/`OpenCV` na validação de documentos, `tweepy`/`PRAW`/`BeautifulSoup` na integração social, e `transformers`/`spaCy` na análise de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b8c4d4857ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfadd7aa567531f2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Dicas de Apresentação do Protótipo</h3> </div>\n",
    "\n",
    "- **Formatação Atraente:** Usar cabeçalhos (`#`, `##`), listas e imagens (logotipos de e-sports, ícones de redes sociais) para tornar o notebook visualmente agradável. Células Markdown bem elaboradas ajudam na legibilidade.  \n",
    "- **Interatividade:** Incluir elementos interativos (sliders, botões de upload, caixas de seleção) via `ipywidgets` para simular um fluxo real de uso. Isso torna a demonstração dinâmica mesmo no ambiente de notebook.  \n",
    "- **Visualização de Dados:** Aproveitar gráficos (matplotlib, seaborn ou plotly) para mostrar perfis de interesse ou resultados das análises de comentários. Um gráfico de barras ou nuvem de palavras torna o conteúdo mais didático.  \n",
    "- **Narração do Código:** Inserir comentários explicativos e outputs de exemplo que guiem o avaliador pelo processo passo a passo. Ao final, apresentar um breve resumo dos resultados obtidos para evidenciar que todos os requisitos foram atendidos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37fafd0bfde477",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Conclusão</h3> </div>\n",
    "\n",
    "Este plano garante uma implementação completa dos requisitos do desafio, integrando coleta de informações pessoais e de interesse em e-sports, validação de identidade baseada em IA, simulação de integração social e enriquecimento de perfil com dados externos. A organização em seções claras, o uso de bibliotecas especializadas (ex: **Streamlit/ipywidgets** para interfaces, **OpenCV/face_recognition** para validação, **transformers/spaCy** para IA, **pandas** para dados) e as sugestões de apresentação asseguram uma entrega alinhada e de fácil acompanhamento, mesmo no formato de notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
