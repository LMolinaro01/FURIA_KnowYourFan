{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan (Análise Geral) </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e774306",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "#### Importação Chave API (Arquivo .env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd5d8a7e76898c",
   "metadata": {},
   "source": [
    "### Coleta de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58c9e8",
   "metadata": {},
   "source": [
    "##### Comentários YTB - JSON (Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea82564d0b8b86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:08:55.632710Z",
     "start_time": "2025-04-30T14:08:43.298626Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Testa se a chave da API está sendo carregada\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "\n",
    "# ID do vídeo do qual você quer obter os comentários\n",
    "video_id = '8aIcU-_5W34'\n",
    "\n",
    "\n",
    "def get_video_channel_name(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Obtém as informações do vídeo\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição e pega o nome do canal\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "        return channel_name\n",
    "    return None\n",
    "\n",
    "# Função para obter comentários do vídeo\n",
    "def get_comments(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Lista para armazenar os comentários\n",
    "    comments = []\n",
    "    \n",
    "    channel_name = get_video_channel_name(video_id, api_key)\n",
    "    \n",
    "    # Inicializa a requisição para obter os comentários\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100  # Max resultados por requisição (pode ajustar conforme necessário)\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Itera sobre os comentários e armazena os dados\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                    'video_id': video_id,                    # ← aqui você adiciona o ID do vídeo\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'text': comment['textDisplay'],\n",
    "                    'published_at': comment['publishedAt'],\n",
    "                    'likes': comment['likeCount'],\n",
    "                    'channel_name': channel_name\n",
    "                }\n",
    "            \n",
    "            comments.append(comment_data)\n",
    "        \n",
    "        # Verifica se existe uma próxima página de resultados\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Obter os comentários\n",
    "# Função para salvar os comentários sem sobrescrever o arquivo existente\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    # Caminho da pasta onde você deseja salvar o arquivo\n",
    "    pasta = 'form_data'\n",
    "\n",
    "    # Certifique-se de que a pasta existe\n",
    "    if not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "    # Caminho completo do arquivo JSON\n",
    "    arquivo_json = os.path.join(pasta, filename)\n",
    "    \n",
    "    # Verifica se o arquivo já existe\n",
    "    if os.path.exists(arquivo_json):\n",
    "        # Carrega o conteúdo existente\n",
    "        with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "            existing_comments = json.load(f)\n",
    "        \n",
    "        # Adiciona os novos comentários ao conteúdo existente\n",
    "        existing_comments.extend(comments)\n",
    "        \n",
    "        # Salva o conteúdo atualizado\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_comments, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        # Caso o arquivo não exista, cria um novo com os comentários\n",
    "        with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Obter os comentários\n",
    "comments = get_comments(video_id, api_key)\n",
    "\n",
    "# Salvar os comentários sem sobrescrever o arquivo\n",
    "save_comments(comments)\n",
    "\n",
    "print(\"Comentários salvos em 'form_data/comentarios_video.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8841c4",
   "metadata": {},
   "source": [
    "#### Comentários YTB - JSON - GUI (Custom Tkinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install customtkinter google-api-python-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4942f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import customtkinter as ctk\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tkinter.messagebox as msgbox\n",
    "\n",
    "# Load .env\n",
    "env_path = Path('.idea/.env')  # Ajuste conforme necessário\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "# Detecta ID do vídeo\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Detecta ID da playlist\n",
    "def extract_playlist_id(url):\n",
    "    match = re.search(r\"[?&]list=([a-zA-Z0-9_-]+)\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Pega nome do canal\n",
    "def get_video_channel_name(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    request = youtube.videos().list(part='snippet', id=video_id)\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        return response['items'][0]['snippet']['channelTitle']\n",
    "    return None\n",
    "\n",
    "# Busca comentários\n",
    "def get_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    comments = []\n",
    "    channel_name = get_video_channel_name(video_id)\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                'video_id': video_id,\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'text': comment['textDisplay'],\n",
    "                'published_at': comment['publishedAt'],\n",
    "                'likes': comment['likeCount'],\n",
    "                'channel_name': channel_name\n",
    "            }\n",
    "            comments.append(comment_data)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "# Salva no JSON\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    pasta = 'form_data'\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "    caminho = os.path.join(pasta, filename)\n",
    "    if os.path.exists(caminho):\n",
    "        with open(caminho, 'r', encoding='utf-8') as f:\n",
    "            existentes = json.load(f)\n",
    "        existentes.extend(comments)\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existentes, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Busca vídeos de uma playlist\n",
    "def get_video_ids_from_playlist(playlist_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    video_ids = []\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "        request = youtube.playlistItems().list_next(request, response)\n",
    "    return video_ids\n",
    "\n",
    "# Processa link inserido\n",
    "def process_link():\n",
    "    url = entry_url.get().strip()\n",
    "    if not url:\n",
    "        status_label.configure(text=\"Cole um link válido do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    playlist_id = extract_playlist_id(url)\n",
    "    video_id = extract_video_id(url)\n",
    "\n",
    "    if playlist_id:\n",
    "        try:\n",
    "            video_ids = get_video_ids_from_playlist(playlist_id)\n",
    "            total_comments = []\n",
    "            for vid in video_ids:\n",
    "                status_label.configure(text=f\"Buscando comentários de {vid}...\", text_color=\"blue\")\n",
    "                comments = get_comments(vid)\n",
    "                total_comments.extend(comments)\n",
    "            save_comments(total_comments)\n",
    "            status_label.configure(text=f\"Todos os comentários da playlist foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "\n",
    "    elif video_id:\n",
    "        try:\n",
    "            status_label.configure(text=\"Buscando comentários do vídeo...\", text_color=\"blue\")\n",
    "            comments = get_comments(video_id)\n",
    "            save_comments(comments)\n",
    "            status_label.configure(text=f\"Comentários do vídeo foram salvos!\", text_color=\"green\")\n",
    "        except Exception as e:\n",
    "            status_label.configure(text=f\"Erro: {e}\", text_color=\"red\")\n",
    "            return\n",
    "    else:\n",
    "        status_label.configure(text=\"Link inválido. Verifique se é um link do YouTube.\", text_color=\"red\")\n",
    "        return\n",
    "\n",
    "    # Pergunta se deseja adicionar mais\n",
    "    continuar = msgbox.askyesno(\"Continuar\", \"Deseja adicionar outro vídeo ou playlist?\")\n",
    "    if continuar:\n",
    "        entry_url.delete(0, 'end')\n",
    "        status_label.configure(text=\"Cole outro link para continuar.\", text_color=\"black\")\n",
    "    else:\n",
    "        app.quit()\n",
    "        app.destroy()\n",
    "\n",
    "# GUI com customtkinter\n",
    "ctk.set_appearance_mode(\"System\")\n",
    "ctk.set_default_color_theme(\"blue\")\n",
    "\n",
    "app = ctk.CTk()\n",
    "app.title(\"Coletor de Comentários YouTube\")\n",
    "app.geometry(\"600x300\")\n",
    "\n",
    "label = ctk.CTkLabel(app, text=\"Cole o link do vídeo ou playlist do YouTube:\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "entry_url = ctk.CTkEntry(app, width=500)\n",
    "entry_url.pack(pady=10)\n",
    "\n",
    "submit_button = ctk.CTkButton(app, text=\"Buscar e Salvar Comentários\", command=process_link)\n",
    "submit_button.pack(pady=20)\n",
    "\n",
    "status_label = ctk.CTkLabel(app, text=\"\")\n",
    "status_label.pack(pady=10)\n",
    "\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5021376815ef2",
   "metadata": {},
   "source": [
    "#### Comentários - JSON (Twitter/X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5376ad21a39994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ————————— Configurações e autenticação —————————\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Parâmetros de pesquisa com a hashtag #DIADEFURIA incluída\n",
    "query = '(Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR Trigo OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson OR #FURIACS OR #FURIAR6 OR #FURIAFC OR #DIADEFURIA) -is:retweet lang:pt'\n",
    "max_results = 10\n",
    "\n",
    "# Fazendo a busca com os campos desejados\n",
    "tweets = client.search_recent_tweets(query=query, max_results=max_results,\n",
    "                                     tweet_fields=[\"author_id\", \"conversation_id\", \"created_at\", \"geo\", \"id\", \"lang\", \"source\", \"text\"],\n",
    "                                     user_fields=[\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"url\", \"username\"],\n",
    "                                     expansions=[\"author_id\"])\n",
    "\n",
    "# Convertendo os tweets para um formato de dicionário\n",
    "tweets_data = []\n",
    "if tweets.data:\n",
    "    for tweet in tweets.data:\n",
    "        tweet_info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'author_id': tweet.author_id,\n",
    "            'conversation_id': tweet.conversation_id,\n",
    "            'geo': tweet.geo,\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source\n",
    "        }\n",
    "\n",
    "        # Obtendo informações do usuário (quem postou o tweet)\n",
    "        if tweets.includes and 'users' in tweets.includes:\n",
    "            for user in tweets.includes['users']:\n",
    "                if user.id == tweet.author_id:\n",
    "                    tweet_info['user'] = {\n",
    "                        'created_at': str(user.created_at),\n",
    "                        'description': user.description,\n",
    "                        'entities': user.entities,\n",
    "                        'location': user.location,\n",
    "                        'name': user.name,\n",
    "                        'url': user.url,\n",
    "                        'username': user.username\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "        tweets_data.append(tweet_info)\n",
    "\n",
    "# Caminho da pasta onde você deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, 'tweetsGerais_furia.json')\n",
    "\n",
    "# Carrega o conteúdo existente, se houver\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Adiciona os novos tweets\n",
    "dados_existentes.extend(tweets_data)\n",
    "\n",
    "# Salva de volta no JSON\n",
    "with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_existentes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tweets adicionados com sucesso a 'tweets_furia.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9cecf67adf235",
   "metadata": {},
   "source": [
    "#### Posts - JSON (Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb4487c423268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "query = \"FURIA OR Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson\"\n",
    "\n",
    "subreddits = [\"GlobalOffensive\", \"csgo\", \"VALORANT\", \"cs2\", \"cblol\", \"LolEsports\", \"ValorantCompetitive\", \"VCT\", \"R6ProLeague\"]\n",
    "limit = 50\n",
    "\n",
    "resultados = []\n",
    "\n",
    "# Loop pelos subreddits\n",
    "for subreddit in subreddits:\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr=on&limit={limit}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"data\" in data and \"children\" in data[\"data\"]:\n",
    "        for post in data[\"data\"][\"children\"]:\n",
    "            p = post[\"data\"]\n",
    "\n",
    "            # Verifica se a query aparece no título ou no texto do post\n",
    "            titulo = p.get(\"title\", \"\")\n",
    "            texto = p.get(\"selftext\", \"\")\n",
    "\n",
    "            # Verificação literal, sem usar lower()\n",
    "            if any(jogador in titulo or jogador in texto for jogador in query.split(\" OR \")):\n",
    "                resultados.append({\n",
    "                    \"titulo\": p.get(\"title\"),\n",
    "                    \"autor\": p.get(\"author\"),\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"score\": p.get(\"score\", 0),\n",
    "                    \"url\": \"https://reddit.com\" + p.get(\"permalink\"),\n",
    "                    \"data_criacao\": p.get(\"created_utc\"),\n",
    "                    \"comentario_exemplo\": p.get(\"selftext\", \"\")\n",
    "                })\n",
    "\n",
    "# Caminho da pasta onde você deseja salvar o arquivo\n",
    "pasta = 'form_data'\n",
    "\n",
    "# Certifique-se de que a pasta existe\n",
    "if not os.path.exists(pasta):\n",
    "    os.makedirs(pasta)\n",
    "\n",
    "# Caminho completo do arquivo JSON\n",
    "arquivo_json = os.path.join(pasta, \"posts_furia_reddit.json\")\n",
    "\n",
    "# Salva em JSON\n",
    "# Verifica se o arquivo já existe e carrega os dados antigos\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Junta os dados antigos com os novos\n",
    "dados_atuaisizados = dados_existentes + resultados\n",
    "\n",
    "# Salva todos os dados no JSON\n",
    "with open(arquivo_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dados_atuaisizados, f, indent=4, ensure_ascii=False)\n",
    "    print(\"Novos dados adicionados ao JSON.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eddccd",
   "metadata": {},
   "source": [
    "### Análise de Dados e Visualização Detalhada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f4811",
   "metadata": {},
   "source": [
    "#### Análise Geral (Youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5714698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Separa comentários válidos (1 a 45 likes) e calcula estatísticas\n",
    "df_filtrado = df[(df['likes'] > 0) & (df['likes'] <= 45)]\n",
    "# Extrai série de likes após filtragem\n",
    "n_likes = df_filtrado['likes']\n",
    "# Estatísticas descritivas\n",
    "mu = n_likes.mean()\n",
    "sigma = n_likes.std(ddof=0)\n",
    "median = n_likes.median()\n",
    "skewness = n_likes.skew()\n",
    "\n",
    "# 3. Configura histograma e sobrepõe curva Normal ajustada\n",
    "bin_edges = np.arange(0, 47, 2)\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "counts, bins = np.histogram(n_likes, bins=bin_edges)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "# histograma\n",
    "plt.hist(n_likes, bins=bin_edges, edgecolor='black', alpha=0.7, label='Frequência')\n",
    "# curva Normal ajustada (densidade * escala)\n",
    "x = np.linspace(0, 45, 500)\n",
    "pdf = norm.pdf(x, mu, sigma) * len(n_likes) * bin_width\n",
    "plt.plot(x, pdf, linestyle='--', label='Normal ajustada')\n",
    "\n",
    "# 4. Anotações de estatísticas\n",
    "stats_text = (\n",
    "    f'Média (μ) = {mu:.2f}\\n'\n",
    "    f'Desv. Padrão (σ) = {sigma:.2f}\\n'\n",
    "    f'Mediana = {median:.2f}\\n'\n",
    "    f'Assimetria = {skewness:.2f}\\n'\n",
    "    f'N = {len(n_likes)}'\n",
    ")\n",
    "plt.gca().text(\n",
    "    0.95, 0.95, stats_text,\n",
    "    transform=plt.gca().transAxes,\n",
    "    verticalalignment='top', horizontalalignment='right',\n",
    "    fontsize='10', bbox=dict(boxstyle='round', alpha=0.2)\n",
    ")\n",
    "\n",
    "# 5. Ajustes visuais\n",
    "ticks = bin_edges\n",
    "labels = [str(int(b)) for b in bin_edges]\n",
    "plt.xticks(ticks=ticks, labels=labels, rotation=45, ha='right')\n",
    "plt.title('Distribuição de Likes (1 a 45 likes, bins de 2)')\n",
    "plt.xlabel('Número de Likes')\n",
    "plt.ylabel('Frequência')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Exibe resumo de removidos e estatísticas\n",
    "removed = df.shape[0] - df_filtrado.shape[0]\n",
    "print(f\"{removed} comentários removidos (0 ou >45 likes)\")\n",
    "print(f\"Estatísticas dos comentários mantidos:\\n{stats_text}\")\n",
    "\n",
    "# 7. Estrutura dos resultados para distribuição de likes\n",
    "resultados_distribuicao = {\n",
    "    \"total_comentarios\": int(df.shape[0]),\n",
    "    \"comentarios_mantidos\": int(len(n_likes)),\n",
    "    \"comentarios_removidos\": int(removed),\n",
    "    \"estatisticas\": {\n",
    "        \"media\": round(mu, 2),\n",
    "        \"desvio_padrao\": round(sigma, 2),\n",
    "        \"mediana\": round(median, 2),\n",
    "        \"assimetria\": round(skewness, 2)\n",
    "    }\n",
    "}\n",
    "\n",
    "# 8. Diretório e arquivo de saída\n",
    "pasta_resultados = 'analysis_results'\n",
    "arquivo_resultados = os.path.join(pasta_resultados, 'analise_ytb.json')\n",
    "\n",
    "# 9. Cria a pasta se não existir\n",
    "os.makedirs(pasta_resultados, exist_ok=True)\n",
    "\n",
    "# 10. Cria o arquivo se não existir\n",
    "if not os.path.exists(arquivo_resultados):\n",
    "    with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 11. Carrega o conteúdo atual do JSON\n",
    "with open(arquivo_resultados, 'r', encoding='utf-8') as f:\n",
    "    todos_resultados = json.load(f)\n",
    "\n",
    "# 12. Atualiza os dados da análise de distribuição de likes\n",
    "todos_resultados[\"distribuicao_likes\"] = resultados_distribuicao\n",
    "\n",
    "# 13. Salva o JSON final com a nova análise\n",
    "with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "    json.dump(todos_resultados, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24822976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# 1. Carrega o JSON em DataFrame\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Ordena pelo número de likes e pega os top N (ajuste N se quiser mais/menos)\n",
    "N = 10\n",
    "top_comments = df.sort_values('likes', ascending=False).head(N)\n",
    "\n",
    "# 3. Prepara os rótulos: quebra linhas para caber melhor no gráfico\n",
    "wrapped_texts = [\n",
    "    textwrap.fill(text, width=50)\n",
    "    for text in top_comments['text']\n",
    "]\n",
    "\n",
    "# 4. Plota gráfico de barras horizontais com o texto do comentário\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(N), top_comments['likes'], edgecolor='black')\n",
    "plt.yticks(range(N), wrapped_texts)\n",
    "plt.xlabel('Número de Likes')\n",
    "plt.title(f'Top {N} Comentários Mais Curtidos')\n",
    "plt.gca().invert_yaxis()  # Inverte o eixo para o comentário mais curtido ficar no topo\n",
    "\n",
    "# 5. Anotações com o número exato de likes ao final de cada barra\n",
    "for i, like in enumerate(top_comments['likes']):\n",
    "    plt.text(like + 5, i, str(like), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Estrutura de resultados para salvar no JSON\n",
    "top_comentarios_data = [\n",
    "    {\n",
    "        \"texto\": row['text'],\n",
    "        \"likes\": int(row['likes'])\n",
    "    }\n",
    "    for _, row in top_comments.iterrows()\n",
    "]\n",
    "\n",
    "# 7. Diretório e arquivo JSON\n",
    "pasta_resultados = 'analysis_results'\n",
    "arquivo_resultados = os.path.join(pasta_resultados, 'analise_ytb.json')\n",
    "\n",
    "# 8. Garante que a pasta existe\n",
    "os.makedirs(pasta_resultados, exist_ok=True)\n",
    "\n",
    "# 9. Cria o arquivo se não existir\n",
    "if not os.path.exists(arquivo_resultados):\n",
    "    with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 10. Carrega JSON existente\n",
    "with open(arquivo_resultados, 'r', encoding='utf-8') as f:\n",
    "    todos_resultados = json.load(f)\n",
    "\n",
    "# 11. Atualiza os dados\n",
    "todos_resultados[\"top_10_comentarios\"] = top_comentarios_data\n",
    "\n",
    "# 12. Salva novamente\n",
    "with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "    json.dump(todos_resultados, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 autores por número de comentários\n",
    "top_autores = df['author'].value_counts().head(10)\n",
    "plt.figure()\n",
    "plt.bar(top_autores.index, top_autores.values)\n",
    "plt.title('Top 10 Autores por Número de Comentários')\n",
    "plt.xlabel('Autor')\n",
    "plt.ylabel('Quantidade de Comentários')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Carrega os dados (caso ainda não tenha sido feito)\n",
    "df = pd.read_json('form_data/comentarios_video.json')\n",
    "\n",
    "# 2. Converte datas e filtra\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], utc=True)\n",
    "df['year'] = df['published_at'].dt.year\n",
    "df['month'] = df['published_at'].dt.month\n",
    "df = df[df['year'] >= 2000]\n",
    "\n",
    "# 3. Prepara dicionário de resultados por ano\n",
    "comentarios_por_ano = {}\n",
    "\n",
    "for year in sorted(df['year'].unique()):\n",
    "    df_year = df[df['year'] == year]\n",
    "    comentarios_por_mes = (\n",
    "        df_year.groupby('month')\n",
    "               .size()\n",
    "               .reindex(range(1, 13), fill_value=0)\n",
    "    )\n",
    "    valores = comentarios_por_mes.values\n",
    "    mu = valores.mean()\n",
    "    sigma = valores.std(ddof=0)\n",
    "\n",
    "    # Gráfico por ano\n",
    "    plt.figure()\n",
    "    plt.plot(comentarios_por_mes.index, valores, marker='o')\n",
    "    plt.axhline(mu, linestyle='--', label=f'Média = {mu:.2f}')\n",
    "    plt.fill_between(comentarios_por_mes.index, mu - sigma, mu + sigma, alpha=0.2)\n",
    "    plt.title(f'Comentários por Mês em {year}\\nMédia={mu:.2f}, σ={sigma:.2f}')\n",
    "    plt.xlabel('Mês')\n",
    "    plt.ylabel('Número de Comentários')\n",
    "    labels = [calendar.month_name[m] for m in comentarios_por_mes.index]\n",
    "    plt.xticks(ticks=comentarios_por_mes.index, labels=labels, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Salva dados do ano\n",
    "    comentarios_por_ano[str(year)] = {\n",
    "        \"comentarios_por_mes\": {str(m): int(q) for m, q in zip(comentarios_por_mes.index, valores)},\n",
    "        \"media\": round(mu, 2),\n",
    "        \"desvio_padrao\": round(sigma, 2)\n",
    "    }\n",
    "\n",
    "# 4. Gráfico combinado de todos os anos\n",
    "table = (\n",
    "    df.groupby(['year', 'month'])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    "      .reindex(columns=range(1, 13), fill_value=0)\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "for year, row in table.iterrows():\n",
    "    plt.plot(row.index, row.values, marker='o', label=str(year))\n",
    "\n",
    "plt.title('Comentários por Mês — Comparação entre Anos')\n",
    "plt.xlabel('Mês')\n",
    "plt.ylabel('Número de Comentários')\n",
    "plt.xticks(ticks=range(1, 13), labels=[calendar.month_name[m] for m in range(1, 13)], rotation=45, ha='right')\n",
    "plt.legend(ncol=2, fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Salva em JSON (estrutura centralizada)\n",
    "pasta_resultados = 'analysis_results'\n",
    "arquivo_resultados = os.path.join(pasta_resultados, 'analise_ytb.json')\n",
    "\n",
    "os.makedirs(pasta_resultados, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(arquivo_resultados):\n",
    "    with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(arquivo_resultados, 'r', encoding='utf-8') as f:\n",
    "    todos_resultados = json.load(f)\n",
    "\n",
    "# 6. Atualiza a seção\n",
    "todos_resultados[\"comentarios_por_mes\"] = comentarios_por_ano\n",
    "\n",
    "with open(arquivo_resultados, 'w', encoding='utf-8') as f:\n",
    "    json.dump(todos_resultados, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 1. Caminho do JSON\n",
    "JSON_PATH = 'form_data/comentarios_video.json'\n",
    "\n",
    "# 2. Carrega o DataFrame\n",
    "df = pd.read_json(JSON_PATH)\n",
    "\n",
    "# 3. Top 25 comentários mais curtidos por canal\n",
    "top_por_canal = (\n",
    "    df.sort_values(['channel_name', 'likes'], ascending=[True, False])\n",
    "      .groupby('channel_name')\n",
    "      .head(25)\n",
    ")\n",
    "\n",
    "# 4. Pipeline de sentimento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 5. Aplica o modelo em lotes\n",
    "def analyze_batch(texts):\n",
    "    return sentiment_analyzer(texts, truncation=True)\n",
    "\n",
    "results = []\n",
    "batch_size = 32\n",
    "texts = top_por_canal['text'].tolist()\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    results.extend(analyze_batch(texts[i:i+batch_size]))\n",
    "\n",
    "# 6. Processa os resultados\n",
    "scores, labels = [], []\n",
    "for res in results:\n",
    "    stars = int(res['label'][0])\n",
    "    score = (stars - 1) / 4\n",
    "    if stars == 1:\n",
    "        sentiment = \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        sentiment = \"negativo\"\n",
    "    elif stars == 3:\n",
    "        sentiment = \"neutro\"\n",
    "    elif stars == 4:\n",
    "        sentiment = \"positivo\"\n",
    "    else:\n",
    "        sentiment = \"muito positivo\"\n",
    "    scores.append(score)\n",
    "    labels.append(sentiment)\n",
    "\n",
    "top_por_canal = top_por_canal.reset_index(drop=True)\n",
    "top_por_canal['sentiment_score'] = scores\n",
    "top_por_canal['sentiment'] = labels\n",
    "\n",
    "# 7. Classificação textual\n",
    "def classificar_media(score):\n",
    "    if score <= 0.2:\n",
    "        return \"muito negativo\"\n",
    "    elif score <= 0.3:\n",
    "        return \"negativo\"\n",
    "    elif score <= 0.43:\n",
    "        return \"neutro\"\n",
    "    elif score <= 0.5:\n",
    "        return \"levemente positivo\"\n",
    "    elif score <= 0.65:\n",
    "        return \"positivo\"\n",
    "    elif score <= 0.8:\n",
    "        return \"muito positivo\"\n",
    "    else:\n",
    "        return \"extremamente positivo\"\n",
    "\n",
    "# 8. Agrupa por canal\n",
    "resumo_canais = (\n",
    "    top_por_canal.groupby('channel_name')\n",
    "                 .agg(\n",
    "                     mean_sentiment_score=('sentiment_score', 'mean'),\n",
    "                     total_comentarios=('sentiment_score', 'count')\n",
    "                 )\n",
    "                 .reset_index()\n",
    ")\n",
    "\n",
    "# 9. Classifica\n",
    "resumo_canais['sentiment_class'] = resumo_canais['mean_sentiment_score'].apply(classificar_media)\n",
    "resumo_canais['channel_label'] = resumo_canais['channel_name']\n",
    "\n",
    "# 10. Gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(resumo_canais['channel_label'], resumo_canais['mean_sentiment_score'], color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Média do Sentimento (0 = ruim, 1 = ótimo)')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Média de Sentimento por Canal (Top 25 Comentários Mais Curtidos)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "for bar, label in zip(bars, resumo_canais['sentiment_class']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, label, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11. Salva no JSON\n",
    "saida = {}\n",
    "for _, row in resumo_canais.iterrows():\n",
    "    saida[row['channel_name']] = {\n",
    "        \"media_sentimento\": round(row['mean_sentiment_score'], 3),\n",
    "        \"total_comentarios\": int(row['total_comentarios']),\n",
    "        \"classificacao\": row['sentiment_class']\n",
    "    }\n",
    "\n",
    "# Caminho de saída\n",
    "caminho_pasta = 'analysis_results'\n",
    "caminho_arquivo = os.path.join(caminho_pasta, 'analise_ytb.json')\n",
    "\n",
    "os.makedirs(caminho_pasta, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "    dados_atuais = json.load(f)\n",
    "\n",
    "dados_atuais[\"sentimento_por_canal\"] = saida\n",
    "\n",
    "with open(caminho_arquivo, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_atuais, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275d68f",
   "metadata": {},
   "source": [
    "#### Análise Geral Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from scipy.stats import skew\n",
    "\n",
    "# ====== 0. Parâmetro: caminho para o JSON de entrada ======\n",
    "JSON_PATH = 'form_data/posts_furia_reddit.json'\n",
    "\n",
    "# ====== 1. Carregamento ======\n",
    "def load_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {path}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return pd.DataFrame(json.load(f))\n",
    "\n",
    "# ====== 2. Limpeza ======\n",
    "def clean_data(df):\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "    df = df.dropna(subset=['titulo', 'comentario_exemplo']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ====== 3. Featurização ======\n",
    "def engineer_features(df):\n",
    "    df['created_utc'] = pd.to_datetime(df['data_criacao'], unit='s', utc=True)\n",
    "    df['created_brt'] = df['created_utc'].dt.tz_convert('America/Sao_Paulo')\n",
    "    df['titulo_len_chars'] = df['titulo'].str.len()\n",
    "    df['titulo_len_words'] = df['titulo'].str.split().apply(len)\n",
    "    df['weekday'] = df['created_brt'].dt.day_name(locale='pt_BR')\n",
    "    df['hour']    = df['created_brt'].dt.hour\n",
    "    return df\n",
    "\n",
    "# ====== 4. Análise de Sentimento ======\n",
    "def analyze_sentiment(df):\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    pt_stop = stopwords.words('portuguese')\n",
    "    sentiment_analyzer = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    )\n",
    "    df['sentiment_score'] = df['comentario_exemplo'].apply(\n",
    "        lambda t: int(sentiment_analyzer(t[:512], truncation=True)[0]['label'][0])\n",
    "    )\n",
    "    return df, pt_stop\n",
    "\n",
    "# ====== 5. Extração de jogadores via NER ======\n",
    "def extract_player_mentions(df):\n",
    "    ner = pipeline(\n",
    "        \"ner\",\n",
    "        model=\"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
    "        tokenizer=\"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
    "        grouped_entities=True\n",
    "    )\n",
    "    texts = pd.concat([df['titulo'], df['comentario_exemplo']]).unique()\n",
    "    mentions = []\n",
    "    for txt in texts:\n",
    "        entities = ner(txt[:512])\n",
    "        mentions += [e['word'] for e in entities if e['entity_group'] == 'PER']\n",
    "    return Counter(mentions)\n",
    "\n",
    "# ====== 6. Plots Estatísticos ======\n",
    "def plot_score_distribution(df):\n",
    "    scores = df['score']\n",
    "    μ, σ = scores.mean(), scores.std(ddof=0)\n",
    "    plt.figure()\n",
    "    plt.hist(scores, bins=20, density=True, alpha=0.6, edgecolor='black')\n",
    "    x = np.linspace(scores.min(), scores.max(), 200)\n",
    "    pdf = (1/(σ*np.sqrt(2*np.pi))) * np.exp(-0.5*((x-μ)/σ)**2)\n",
    "    plt.plot(x, pdf, 'r-', lw=2)\n",
    "    plt.title('Score dos Posts (com Ajuste Normal)')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Densidade')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_series(df):\n",
    "    serie = df.set_index('created_brt').resample('D').size()\n",
    "    plt.figure()\n",
    "    serie.plot()\n",
    "    plt.title('Número Diário de Posts')\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_subreddits(df):\n",
    "    top = df['subreddit'].value_counts().head(10)\n",
    "    plt.figure()\n",
    "    plt.barh(top.index[::-1], top.values[::-1], color='steelblue')\n",
    "    plt.title('Top 10 Subreddits')\n",
    "    plt.xlabel('Número de Posts')\n",
    "    plt.show()\n",
    "\n",
    "def plot_title_length_distribution(df):\n",
    "    lengths = df['titulo_len_words']\n",
    "    μ, σ = lengths.mean(), lengths.std(ddof=0)\n",
    "    plt.figure()\n",
    "    plt.hist(lengths, bins=range(0, int(lengths.max())+2), density=True, alpha=0.6, edgecolor='black')\n",
    "    x = np.linspace(lengths.min(), lengths.max(), 200)\n",
    "    pdf = (1/(σ*np.sqrt(2*np.pi))) * np.exp(-0.5*((x-μ)/σ)**2)\n",
    "    plt.plot(x, pdf, 'r-', lw=2)\n",
    "    plt.title('Tamanho dos Títulos (com Ajuste Normal)')\n",
    "    plt.xlabel('Palavras no Título')\n",
    "    plt.ylabel('Densidade')\n",
    "    plt.show()\n",
    "\n",
    "def plot_sentiment_distribution(df):\n",
    "    counts = df['sentiment_score'].value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    counts.plot(kind='bar', edgecolor='black')\n",
    "    plt.title('Sentimento do Comentário Exemplo')\n",
    "    plt.xlabel('Estrelas')\n",
    "    plt.ylabel('Número de Posts')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ngrams(df, stop_words, top_n=20):\n",
    "    vec = CountVectorizer(ngram_range=(1,2), stop_words=stop_words)\n",
    "    X = vec.fit_transform(df['titulo'])\n",
    "    sums = X.sum(axis=0)\n",
    "    termos_contagens = [\n",
    "        (termo, int(sums[0, idx]))\n",
    "        for termo, idx in vec.vocabulary_.items()\n",
    "    ]\n",
    "    top_terms = sorted(termos_contagens, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    labels, freqs = zip(*top_terms)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(labels[::-1], freqs[::-1])\n",
    "    plt.title(f'Top {top_n} uni-/bi-grams nos títulos')\n",
    "    plt.xlabel('Frequência')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_player_mentions(counter, top_n=10):\n",
    "    most = counter.most_common(top_n)\n",
    "    players, freqs = zip(*most)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.barh(players[::-1], freqs[::-1])\n",
    "    plt.title(f'Top {top_n} Jogadores Mais Citados')\n",
    "    plt.xlabel('Menções')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ====== 7. Main ======\n",
    "def main():\n",
    "    # Carregamento, limpeza e featurização\n",
    "    df = load_data(JSON_PATH)\n",
    "    df = clean_data(df)\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    # Sentimento e stopwords\n",
    "    df, pt_stop = analyze_sentiment(df)\n",
    "\n",
    "    # Plots\n",
    "    plot_score_distribution(df)\n",
    "    plot_time_series(df)\n",
    "    plot_top_subreddits(df)\n",
    "    plot_title_length_distribution(df)\n",
    "    plot_sentiment_distribution(df)\n",
    "    plot_ngrams(df, pt_stop, top_n=20)\n",
    "\n",
    "    # NER de jogadores\n",
    "    counter_players = extract_player_mentions(df)\n",
    "    plot_player_mentions(counter_players, top_n=10)\n",
    "\n",
    "    # Estatísticas resumidas\n",
    "    sentiment_stats = {\n",
    "        \"media\":         round(df['sentiment_score'].mean(), 2),\n",
    "        \"desvio_padrao\": round(df['sentiment_score'].std(ddof=0), 2),\n",
    "        \"mediana\":       round(df['sentiment_score'].median(), 2),\n",
    "        \"assimetria\":    round(skew(df['sentiment_score']), 2)\n",
    "    }\n",
    "    top_subreddits = df['subreddit'].value_counts().head(10).to_dict()\n",
    "\n",
    "    # top_ngrams corrigido\n",
    "    vec = CountVectorizer(ngram_range=(1,2), stop_words=pt_stop)\n",
    "    X = vec.fit_transform(df['titulo'])\n",
    "    sums = X.sum(axis=0)\n",
    "    termos_contagens = [\n",
    "        (termo, int(sums[0, idx]))\n",
    "        for termo, idx in vec.vocabulary_.items()\n",
    "    ]\n",
    "    top_ngrams = dict(sorted(termos_contagens, key=lambda x: x[1], reverse=True)[:20])\n",
    "\n",
    "    top_players = dict(counter_players.most_common(10))\n",
    "\n",
    "    resultados_reddit = {\n",
    "        \"total_posts\":            int(df.shape[0]),\n",
    "        \"posts_limpos\":           int(df.drop_duplicates('url').shape[0]),\n",
    "        \"estatisticas_sentimento\": sentiment_stats,\n",
    "        \"top_subreddits\":         top_subreddits,\n",
    "        \"top_ngrams\":             top_ngrams,\n",
    "        \"top_player_mentions\":    top_players\n",
    "    }\n",
    "\n",
    "    # Salvando JSON\n",
    "    pasta = 'analysis_results'\n",
    "    arquivo = os.path.join(pasta, 'analise_reddit.json')\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "    if not os.path.exists(arquivo):\n",
    "        with open(arquivo, 'w', encoding='utf-8') as f:\n",
    "            json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "    with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "        todos = json.load(f)\n",
    "    todos[\"analise_reddit\"] = resultados_reddit\n",
    "    with open(arquivo, 'w', encoding='utf-8') as f:\n",
    "        json.dump(todos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"[✓] Resultados da análise do Reddit salvos em {arquivo}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32af0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "# --- Parâmetros ---\n",
    "INPUT_JSON   = 'form_data/posts_furia_reddit.json'\n",
    "RESULTS_DIR  = 'analysis_results'\n",
    "RESULTS_FILE = os.path.join(RESULTS_DIR, 'analise_reddit.json')\n",
    "\n",
    "# 1. Carrega posts e extrai ano/mês\n",
    "posts = pd.read_json(INPUT_JSON)\n",
    "posts['data_criacao'] = pd.to_datetime(posts['data_criacao'], unit='s', utc=True)\n",
    "posts['year']  = posts['data_criacao'].dt.year\n",
    "posts['month'] = posts['data_criacao'].dt.month\n",
    "\n",
    "# 2. Filtra anos >=2000\n",
    "posts = posts[posts['year'] >= 2000]\n",
    "\n",
    "# 3. Calcula estatísticas mensais\n",
    "monthly_stats = {}\n",
    "for year, grp in posts.groupby('year'):\n",
    "    counts = grp.groupby('month').size().reindex(range(1,13), fill_value=0)\n",
    "    vals   = counts.values\n",
    "    mu     = float(vals.mean())\n",
    "    sigma  = float(vals.std(ddof=0))\n",
    "    monthly_stats[str(year)] = {\n",
    "        \"counts\": {str(m): int(counts.loc[m]) for m in counts.index},\n",
    "        \"media\": round(mu, 2),\n",
    "        \"desvio_padrao\": round(sigma, 2)\n",
    "    }\n",
    "\n",
    "# 4. Estatística combinada\n",
    "pivot = posts.groupby(['year','month']).size().unstack(fill_value=0)\n",
    "combined = {\n",
    "    str(year): [int(v) for v in row.values]\n",
    "    for year, row in pivot.iterrows()\n",
    "}\n",
    "\n",
    "# 5. Salva no JSON do Reddit\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "if not os.path.exists(RESULTS_FILE):\n",
    "    with open(RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(RESULTS_FILE, 'r', encoding='utf-8') as f:\n",
    "    all_results = json.load(f)\n",
    "\n",
    "all_results.setdefault('analise_reddit', {})\n",
    "all_results['analise_reddit']['monthly_time_series'] = {\n",
    "    \"per_year\": monthly_stats,\n",
    "    \"combined\": combined\n",
    "}\n",
    "\n",
    "with open(RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 6. Plotagem\n",
    "\n",
    "# 6.1 Gráficos por ano\n",
    "for year, stats in monthly_stats.items():\n",
    "    meses = list(range(1,13))\n",
    "    vals  = list(stats[\"counts\"].values())\n",
    "    μ, σ  = stats[\"media\"], stats[\"desvio_padrao\"]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(meses, vals, marker='o')\n",
    "    plt.axhline(μ, linestyle='--', label=f\"Média = {μ:.2f}\")\n",
    "    plt.fill_between(meses, [μ-σ]*12, [μ+σ]*12, alpha=0.2)\n",
    "    plt.title(f\"Posts por Mês em {year}\\nMédia={μ:.2f}, σ={σ:.2f}\")\n",
    "    plt.xlabel(\"Mês\")\n",
    "    plt.ylabel(\"Número de Posts\")\n",
    "    plt.xticks(ticks=meses,\n",
    "               labels=[calendar.month_name[m] for m in meses],\n",
    "               rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6.2 Gráfico combinado\n",
    "plt.figure()\n",
    "for year, vals in combined.items():\n",
    "    plt.plot(meses, vals, marker='o', label=year)\n",
    "plt.title(\"Posts por Mês — Comparação entre Anos\")\n",
    "plt.xlabel(\"Mês\")\n",
    "plt.ylabel(\"Número de Posts\")\n",
    "plt.xticks(ticks=meses,\n",
    "           labels=[calendar.month_name[m] for m in meses],\n",
    "           rotation=45, ha='right')\n",
    "plt.legend(ncol=2, fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583d5c0",
   "metadata": {},
   "source": [
    "##### Horário de Maior Ativiade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "# ====== 1. Caminho para o JSON Reddit ======\n",
    "DATA_DIR = 'form_data'\n",
    "REDDIT_JSON = os.path.join(DATA_DIR, 'posts_furia_reddit.json')\n",
    "\n",
    "# ====== 2. Carrega dados ======\n",
    "with open(REDDIT_JSON, 'r', encoding='utf-8') as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# ====== 3. Converte timestamp e extrai hora UTC ======\n",
    "# supondo campo 'data_criacao' em epoch seconds\n",
    "if df['data_criacao'].dtype != 'datetime64[ns, UTC]':\n",
    "    df['data_criacao'] = pd.to_datetime(df['data_criacao'], unit='s', utc=True)\n",
    "df['hour_utc'] = df['data_criacao'].dt.hour\n",
    "\n",
    "# ====== 4. Conta posts por hora ======\n",
    "hourly_counts = df['hour_utc'].value_counts().sort_index()\n",
    "\n",
    "# ====== 5. Identifica hora de maior atividade ======\n",
    "most_active_hour = hourly_counts.idxmax()\n",
    "most_active_count = hourly_counts.max()\n",
    "\n",
    "# ====== 6. Plot de distribuição horária ======\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(hourly_counts.index, hourly_counts.values, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(most_active_hour, color='red', linestyle='--',\n",
    "            label=f'Mais ativo: {most_active_hour:02d}:00 UTC ({most_active_count} posts)')\n",
    "\n",
    "plt.title('Distribuição de Posts por Hora do Dia (UTC)')\n",
    "plt.xlabel('Hora (UTC)')\n",
    "plt.ylabel('Número de Posts')\n",
    "plt.xticks(ticks=range(0,24), labels=[f\"{h:02d}:00\" for h in range(24)], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03b8da",
   "metadata": {},
   "source": [
    "#### Análise Geral Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095087dd",
   "metadata": {},
   "source": [
    "##### Análise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94877e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correção para o NameError: df0 não definido\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ====== 1. Carrega os dados JSON ======\n",
    "with open(r'form_data/tweets_furia.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Criar df0 antes de qualquer filtragem para manter os totais originais\n",
    "df0 = pd.DataFrame(data)\n",
    "\n",
    "# Trabalhar em uma cópia para filtros\n",
    "df = df0.copy()\n",
    "\n",
    "# ====== 2. Remove duplicatas pelo texto e filtra pt ======\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "df = df[df['lang'] == 'pt'].copy()\n",
    "\n",
    "# ====== 3. Simula likes ======\n",
    "df['likes'] = [random.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Seleciona os 200 mais curtidos ======\n",
    "top200 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# ====== 5. Remove palavras banidas ======\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', \n",
    "                    'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "mask = ~top200['text'].str.upper().str.contains('|'.join(palavras_banidas))\n",
    "top200_filtrado = top200[mask].reset_index(drop=True)\n",
    "\n",
    "# ====== 6. Pipeline de sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# ====== 7. Aplica o modelo ======\n",
    "batch_size = 32\n",
    "texts = top200_filtrado['text'].tolist()\n",
    "results = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results.extend(sentiment_analyzer(batch, truncation=True))\n",
    "\n",
    "# ====== 8. Normaliza scores ======\n",
    "scores = [(int(r['label'][0]) - 1) / 4 for r in results]\n",
    "top200_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# ====== 9. Pega os TOP 25 mais positivos ======\n",
    "top25 = top200_filtrado.sort_values('sentiment_score', ascending=False).head(25).copy()\n",
    "\n",
    "# ====== 10. Funções auxiliares ======\n",
    "def simplificar_comentario(texto, limite=200):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    ultima_palavra = palavras[-1] if palavras else ''\n",
    "    return f\"{texto[:limite].rstrip()}... {ultima_palavra}\"\n",
    "\n",
    "def estrela_para_sentimento(score):\n",
    "    stars = int(round(score * 4)) + 1\n",
    "    return {\n",
    "        1: \"muito negativo\",\n",
    "        2: \"negativo\",\n",
    "        3: \"neutro\",\n",
    "        4: \"positivo\",\n",
    "        5: \"muito positivo\"\n",
    "    }[stars]\n",
    "\n",
    "# Geração do dicionário de resultados, usando df0 para totais originais\n",
    "res = {\n",
    "    \"total_tweets\":      int(df0.shape[0]),\n",
    "    \"tweets_unicos\":     int(df.shape[0]),\n",
    "    \"tweets_pt\":         int(df.lang.eq('pt').sum()),\n",
    "    \"top200_considered\": int(top200.shape[0]),\n",
    "    \"top25_final\":       int(top25.shape[0]),\n",
    "    \"sentiment_stats\": {\n",
    "        \"mean\": round(float(top25.sentiment_score.mean()),2),\n",
    "        \"std\":  round(float(top25.sentiment_score.std(ddof=0)),2)\n",
    "    },\n",
    "    \"top25\": [\n",
    "        {\"text\": textwrap.shorten(t, width=100, placeholder=\"...\"),\n",
    "         \"score\": round(s,2),\n",
    "         \"label\": estrela_para_sentimento(s)}\n",
    "        for t,s in zip(top25.text, top25.sentiment_score)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ====== 11. Salva JSON e plota ======\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "with open('analysis_results/analise_twitter.json','w',encoding='utf-8') as f:\n",
    "    json.dump({\"analise_twitter\": res}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Plot dos Top 25\n",
    "def plot_top25(df):\n",
    "    comentarios = [\n",
    "        textwrap.fill(simplificar_comentario(t), width=50)\n",
    "        for t in df['text']\n",
    "    ]\n",
    "    scores = df['sentiment_score'].tolist()\n",
    "\n",
    "    spacing = 1.2\n",
    "    y_pos = [i * spacing for i in range(len(comentarios))][::-1]\n",
    "\n",
    "    plt.figure(figsize=(14, len(df) * spacing * 0.8 + 2))\n",
    "    plt.barh(y_pos, scores)\n",
    "    plt.yticks(y_pos, comentarios)\n",
    "    plt.xlabel('Score de Sentimento (0 = muito negativo, 1 = muito positivo)')\n",
    "    plt.title('Top 25 Comentários Mais Positivos')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    for y, s in zip(y_pos, scores):\n",
    "        plt.text(s + 0.01, y, f'{s:.2f} ({estrela_para_sentimento(s)})',\n",
    "                 va='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top25(top25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176ba1",
   "metadata": {},
   "source": [
    "##### Frequência de Tweets (Intervalo de 3h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca11dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parâmetros\n",
    "IN_JSON    = 'form_data/tweets_furia.json'\n",
    "OUT_DIR    = 'analysis_results'\n",
    "OUT_JSON   = os.path.join(OUT_DIR, 'analise_twitter.json')\n",
    "\n",
    "# 1. Carrega dados\n",
    "with open(IN_JSON, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Extrai hora UTC e agrupa em blocos de 3h\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "df['hora_utc']   = df['created_at'].dt.hour\n",
    "df['bloco_3h']   = df['hora_utc'].apply(lambda h: f\"{(h//3)*3:02d}h–{(h//3)*3+2:02d}h\")\n",
    "\n",
    "# 3. Conta por bloco e calcula estatísticas\n",
    "freq       = df['bloco_3h'].value_counts().sort_index()\n",
    "vals       = freq.values\n",
    "stats_time = {\n",
    "    \"counts\":       freq.to_dict(),\n",
    "    \"media\":        round(float(vals.mean()), 2),\n",
    "    \"desvio_padrao\":round(float(vals.std(ddof=0)), 2),\n",
    "    \"mediana\":      round(float(np.median(vals)), 2),\n",
    "    \"assimetria\":   round(float(pd.Series(vals).skew()), 2),\n",
    "    \"n_blocos\":     int(len(vals))\n",
    "}\n",
    "\n",
    "# 4. Atualiza JSON existente\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "if not os.path.exists(OUT_JSON):\n",
    "    with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(OUT_JSON, 'r', encoding='utf-8') as f:\n",
    "    all_res = json.load(f)\n",
    "\n",
    "all_res.setdefault('analise_twitter', {})\n",
    "all_res['analise_twitter']['time_blocks_3h'] = stats_time\n",
    "\n",
    "with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_res, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 5. Plotagem\n",
    "x = list(freq.index)\n",
    "y = list(vals)\n",
    "positions = np.arange(len(x))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(positions, y, edgecolor='black', alpha=0.7, label='Tweets')\n",
    "plt.axhline(stats_time['media'], color='red', linestyle='--', label=f\"Média = {stats_time['media']}\")\n",
    "plt.fill_between(positions,\n",
    "                 stats_time['media'] - stats_time['desvio_padrao'],\n",
    "                 stats_time['media'] + stats_time['desvio_padrao'],\n",
    "                 color='red', alpha=0.2, label=f\"±1σ = {stats_time['desvio_padrao']}\")\n",
    "\n",
    "plt.xticks(positions, x, rotation=45, ha='right')\n",
    "plt.xlabel('Intervalo de 3 Horas (UTC)')\n",
    "plt.ylabel('Quantidade de Tweets')\n",
    "plt.title('Tweets por Bloco de 3 Horas (UTC) com Estatísticas')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# estatísticas no canto\n",
    "txt = (\n",
    "    f\"Média = {stats_time['media']}\\n\"\n",
    "    f\"σ = {stats_time['desvio_padrao']}\\n\"\n",
    "    f\"Mediana = {stats_time['mediana']}\\n\"\n",
    "    f\"Assimetria = {stats_time['assimetria']}\\n\"\n",
    "    f\"N = {stats_time['n_blocos']}\"\n",
    ")\n",
    "plt.gca().text(0.95, 0.95, txt, transform=plt.gca().transAxes,\n",
    "               va='top', ha='right', fontsize=10,\n",
    "               bbox=dict(boxstyle='round', alpha=0.2))\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933522f8",
   "metadata": {},
   "source": [
    "##### Localização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parâmetros\n",
    "IN_JSON     = 'form_data/tweets_furia.json'\n",
    "OUT_DIR     = 'analysis_results'\n",
    "OUT_JSON    = os.path.join(OUT_DIR, 'analise_twitter.json')\n",
    "SAMPLE_SIZE = 500\n",
    "\n",
    "# 1. Carrega e amostra\n",
    "with open(IN_JSON, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 2. Extrai e filtra localização\n",
    "df['localizacao'] = df['user'].apply(lambda u: u.get('location') if isinstance(u, dict) else None)\n",
    "locs = df['localizacao'].dropna().str.strip()\n",
    "locs = locs[locs != '']\n",
    "\n",
    "# 3. Top 10 locais\n",
    "top_locs = locs.value_counts().head(10)\n",
    "loc_stats = {loc: int(cnt) for loc, cnt in top_locs.items()}\n",
    "\n",
    "# 4. Salva no JSON existente\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "if not os.path.exists(OUT_JSON):\n",
    "    with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(OUT_JSON, 'r', encoding='utf-8') as f:\n",
    "    all_res = json.load(f)\n",
    "\n",
    "all_res.setdefault('analise_twitter', {})\n",
    "all_res['analise_twitter']['location_distribution'] = {\n",
    "    \"sample_size\": len(df),\n",
    "    \"top_10\": loc_stats\n",
    "}\n",
    "\n",
    "with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_res, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 5. Plota\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(list(loc_stats.keys())[::-1], list(loc_stats.values())[::-1], edgecolor='black')\n",
    "plt.xlabel('Quantidade de Tweets')\n",
    "plt.title(f'Top 10 Localizações (amostra de {len(df)})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d67ad7",
   "metadata": {},
   "source": [
    "##### Idade das Contas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parâmetros\n",
    "IN_JSON   = 'form_data/tweets_furia.json'\n",
    "OUT_DIR   = 'analysis_results'\n",
    "OUT_JSON  = os.path.join(OUT_DIR, 'analise_twitter.json')\n",
    "HOJE      = pd.Timestamp('2025-05-01', tz='UTC')\n",
    "\n",
    "# 1. Carrega dados e calcula idade das contas\n",
    "with open(IN_JSON, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "df['created_at'] = pd.to_datetime(\n",
    "    df['user'].apply(lambda u: u.get('created_at') if isinstance(u, dict) else None),\n",
    "    utc=True, errors='coerce'\n",
    ")\n",
    "df = df.dropna(subset=['created_at'])\n",
    "df['age'] = (HOJE - df['created_at']).dt.days / 365.25\n",
    "\n",
    "# 2. Remove duplicatas de autor e monta histograma\n",
    "unique = df.drop_duplicates('author_id')\n",
    "ages   = unique['age']\n",
    "bins   = range(0, int(ages.max()) + 2)\n",
    "counts, edges = np.histogram(ages, bins=bins)\n",
    "\n",
    "# 3. Estatísticas\n",
    "stats_age = {\n",
    "    \"counts\": {str(int(edges[i])): int(counts[i]) for i in range(len(counts))},\n",
    "    \"media\": round(float(ages.mean()), 2),\n",
    "    \"desvio_padrao\": round(float(ages.std(ddof=0)), 2),\n",
    "    \"mediana\": round(float(ages.median()), 2),\n",
    "    \"n_bins\": len(counts)\n",
    "}\n",
    "\n",
    "# 4. Salva no JSON existente\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "data_out = json.load(open(OUT_JSON)) if os.path.exists(OUT_JSON) else {}\n",
    "data_out.setdefault('analise_twitter', {})['account_age_distribution'] = stats_age\n",
    "with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_out, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 5. Plotagem\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(ages, bins=bins, density=True, alpha=0.6, edgecolor='black', label='Empírica')\n",
    "x = np.linspace(edges[0], edges[-1], 200)\n",
    "μ, σ = ages.mean(), ages.std(ddof=0)\n",
    "pdf = 1/(σ*np.sqrt(2*np.pi)) * np.exp(-0.5*((x-μ)/σ)**2)\n",
    "plt.plot(x, pdf, 'r-', linewidth=2, label=f'Normal fit\\nμ={μ:.2f}, σ={σ:.2f}')\n",
    "plt.title('Distribuição de Idade das Contas (anos)')\n",
    "plt.xlabel('Idade (anos)')\n",
    "plt.ylabel('Densidade')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cce8d1",
   "metadata": {},
   "source": [
    "### Visualização Geral/Abrangente "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56a672",
   "metadata": {},
   "source": [
    "#### Dashbord Geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ada1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# === 1. Carrega os dados ===\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'analysis_results')\n",
    "\n",
    "REDDIT_JSON = os.path.join(DATA_DIR, 'analise_reddit.json')\n",
    "TWITTER_JSON = os.path.join(DATA_DIR, 'analise_twitter.json')\n",
    "YTB_JSON = os.path.join(DATA_DIR, 'analise_ytb.json')\n",
    "\n",
    "with open(REDDIT_JSON, 'r', encoding='utf-8') as f:\n",
    "    reddit = json.load(f)['analise_reddit']\n",
    "with open(TWITTER_JSON, 'r', encoding='utf-8') as f:\n",
    "    twitter = json.load(f)['analise_twitter']\n",
    "with open(YTB_JSON, 'r', encoding='utf-8') as f:\n",
    "    ytb = json.load(f)\n",
    "\n",
    "# === 2. Processamento ===\n",
    "yt_top = ytb['top_10_comentarios']\n",
    "yt_monthly = ytb['comentarios_por_mes']\n",
    "yt_values = [c['likes'] for c in yt_top]\n",
    "yt_mu, yt_std = np.mean(yt_values), np.std(yt_values)\n",
    "\n",
    "reddit_anos = reddit['monthly_time_series']['combined']\n",
    "reddit_values = []\n",
    "for ano, meses in reddit_anos.items():\n",
    "    reddit_values.extend(meses)\n",
    "reddit_mu, reddit_std = np.mean(reddit_values), np.std(reddit_values)\n",
    "subreddits = reddit['top_subreddits']\n",
    "ngrams = {k: v for k, v in reddit['top_ngrams'].items() if k.isalpha()}\n",
    "\n",
    "tweet_hours_dict = twitter['time_blocks_3h']['counts']\n",
    "labels, values = zip(*tweet_hours_dict.items())\n",
    "tw_values = list(values)\n",
    "tw_mu, tw_std = np.mean(tw_values), np.std(tw_values)\n",
    "\n",
    "locs = twitter['location_distribution']['top_10']\n",
    "tw_age = twitter['account_age_distribution']['counts']\n",
    "\n",
    "# === 3. Gráficos ===\n",
    "px_template = 'plotly_dark'\n",
    "cores = px.colors.qualitative.Set3 * 2  # Garantir cores suficientes\n",
    "\n",
    "fig_yt_heat = go.Figure()\n",
    "for idx, (ano, mes_data) in enumerate(yt_monthly.items()):\n",
    "    meses = list(mes_data['comentarios_por_mes'].keys())\n",
    "    valores = list(mes_data['comentarios_por_mes'].values())\n",
    "    fig_yt_heat.add_trace(go.Scatter(\n",
    "        x=meses, y=valores, mode='lines+markers', name=str(ano),\n",
    "        line=dict(color=cores[idx % len(cores)])\n",
    "    ))\n",
    "fig_yt_heat.update_layout(\n",
    "    title='Evolução Mensal de Comentários (YouTube)',\n",
    "    xaxis_title='Mês', yaxis_title='Total de Comentários',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# YouTube - Likes\n",
    "yt_hist = np.histogram(yt_values, bins=12)\n",
    "x_mid = (yt_hist[1][:-1] + yt_hist[1][1:]) / 2\n",
    "x_smooth = np.linspace(min(yt_values), max(yt_values), 200)\n",
    "y_smooth = norm.pdf(x_smooth, yt_mu, yt_std) * len(yt_values) * np.mean(np.diff(yt_hist[1]))\n",
    "fig_yt_dist = go.Figure()\n",
    "fig_yt_dist.add_trace(go.Bar(\n",
    "    x=x_mid, y=yt_hist[0], name='Likes', marker_color=cores[0], opacity=0.7,\n",
    "    width=np.diff(yt_hist[1]) * 0.9\n",
    "))\n",
    "fig_yt_dist.add_trace(go.Scatter(\n",
    "    x=x_smooth, y=y_smooth,\n",
    "    name='Distribuição Normal', mode='lines', line=dict(color=cores[1], width=2)\n",
    "))\n",
    "fig_yt_dist.update_layout(\n",
    "    title='Distribuição de Likes (YouTube)',\n",
    "    xaxis_title='Likes', yaxis_title='Frequência Estimada',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# Reddit\n",
    "reddit_hist = np.histogram(reddit_values, bins=12)\n",
    "x_mid = (reddit_hist[1][:-1] + reddit_hist[1][1:]) / 2\n",
    "x_smooth = np.linspace(min(reddit_values), max(reddit_values), 200)\n",
    "y_smooth = norm.pdf(x_smooth, reddit_mu, reddit_std) * len(reddit_values) * np.mean(np.diff(reddit_hist[1]))\n",
    "fig_reddit_dist = go.Figure()\n",
    "fig_reddit_dist.add_trace(go.Bar(\n",
    "    x=x_mid, y=reddit_hist[0], name='Posts/Mês', marker_color=cores[2], opacity=0.7,\n",
    "    width=np.diff(reddit_hist[1]) * 0.9\n",
    "))\n",
    "fig_reddit_dist.add_trace(go.Scatter(\n",
    "    x=x_smooth, y=y_smooth,\n",
    "    name='Distribuição Normal', mode='lines', line=dict(color=cores[3], width=2)\n",
    "))\n",
    "fig_reddit_dist.update_layout(\n",
    "    title='Distribuição de Posts Mensais (Reddit)',\n",
    "    xaxis_title='Posts por mês', yaxis_title='Frequência Estimada',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# Twitter - Horas dos tweets (blocos de 3h)\n",
    "fig_tw_hour = go.Figure()\n",
    "fig_tw_hour.add_trace(go.Bar(\n",
    "    x=labels, y=values,\n",
    "    name='Tweets por bloco 3h', marker_color=cores[4], opacity=0.7\n",
    "))\n",
    "x_block = np.linspace(0, len(values)-1, 200)\n",
    "y_block = norm.pdf(x_block, np.mean(range(len(values))), tw_std) * len(values)\n",
    "fig_tw_hour.add_trace(go.Scatter(\n",
    "    x=labels, y=np.interp(range(len(values)), x_block, y_block),\n",
    "    name='Distribuição Normal', mode='lines', line=dict(color=cores[5], width=2)\n",
    "))\n",
    "fig_tw_hour.update_layout(\n",
    "    title='Distribuição por Bloco de 3 Horas (Twitter)',\n",
    "    xaxis_title='Bloco 3h', yaxis_title='Quantidade de Tweets',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# Barras adicionais\n",
    "fig_yt_top = px.bar(\n",
    "    x=[c['texto'][:40] + '...' for c in yt_top],\n",
    "    y=[c['likes'] for c in yt_top],\n",
    "    labels={'x':'Comentário','y':'Likes'}, title='Top Comentários por Likes (YouTube)',\n",
    "    template=px_template, color_discrete_sequence=[cores[6]]\n",
    ")\n",
    "\n",
    "fig_subs = px.bar(x=list(subreddits.keys()), y=list(subreddits.values()),\n",
    "                  labels={'x':'Subreddit','y':'Postagens'}, title='Top Subreddits',\n",
    "                  template=px_template, color_discrete_sequence=[cores[7]])\n",
    "\n",
    "top_ngrams = Counter(ngrams).most_common(10)\n",
    "fig_ngrams = px.bar(\n",
    "    x=[n for n,_ in top_ngrams], y=[c for _,c in top_ngrams],\n",
    "    labels={'x':'Termo','y':'Frequência'}, title='Top 10 N-Grams (Reddit)',\n",
    "    template=px_template, color_discrete_sequence=[cores[8]])\n",
    "\n",
    "fig_tw_locs = px.bar(x=list(locs.keys()), y=list(locs.values()),\n",
    "                     labels={'x':'Localização','y':'Contagem'}, title='Top Localizações (Twitter)',\n",
    "                     template=px_template, color_discrete_sequence=[cores[9]])\n",
    "\n",
    "fig_tw_age = px.bar(x=list(tw_age.keys()), y=list(tw_age.values()),\n",
    "                    labels={'x':'Idade da Conta','y':'Contas'}, title='Distribuição da Idade das Contas (Twitter)',\n",
    "                    template=px_template, color_discrete_sequence=[cores[10]])\n",
    "\n",
    "# === 4. App Dash ===\n",
    "app = dash.Dash(__name__)\n",
    "app.title = 'Dashboard Redes Sociais'\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div(html.H1('Dashboard Análise Geral', style={'textAlign': 'center', 'color': 'white'}),\n",
    "             style={'padding': '20px', 'background': 'linear-gradient(to right, #000000, #434343)'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(figure=fig_reddit_dist),\n",
    "        dcc.Graph(figure=fig_subs),\n",
    "        dcc.Graph(figure=fig_ngrams),\n",
    "        dcc.Graph(figure=fig_tw_hour),\n",
    "        dcc.Graph(figure=fig_tw_locs),\n",
    "        dcc.Graph(figure=fig_tw_age),\n",
    "        dcc.Graph(figure=fig_yt_heat),\n",
    "        dcc.Graph(figure=fig_yt_dist),\n",
    "        dcc.Graph(figure=fig_yt_top)\n",
    "    ], style={'padding': '20px'})\n",
    "], style={'backgroundColor': '#121212', 'color': 'white', 'fontFamily': 'Arial'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=8051)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29ea77",
   "metadata": {},
   "source": [
    "##### Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# === 1. Carrega os dados ===\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'analysis_results')\n",
    "YTB_JSON = os.path.join(DATA_DIR, 'analise_ytb.json')\n",
    "\n",
    "with open(YTB_JSON, 'r', encoding='utf-8') as f:\n",
    "    ytb = json.load(f)\n",
    "\n",
    "# === 2. Processamento ===\n",
    "yt_top = ytb['top_10_comentarios']\n",
    "yt_monthly = ytb['comentarios_por_mes']\n",
    "yt_values = [c['likes'] for c in yt_top]\n",
    "yt_mu, yt_std = np.mean(yt_values), np.std(yt_values)\n",
    "\n",
    "# === 3. Gráficos ===\n",
    "px_template = 'plotly_dark'\n",
    "\n",
    "# Gráfico 1: Evolução mensal de comentários\n",
    "fig_yt_heat = go.Figure()\n",
    "color_palette = px.colors.qualitative.Dark24\n",
    "for idx, (ano, mes_data) in enumerate(yt_monthly.items()):\n",
    "    meses = list(mes_data['comentarios_por_mes'].keys())\n",
    "    valores = list(mes_data['comentarios_por_mes'].values())\n",
    "    fig_yt_heat.add_trace(go.Scatter(\n",
    "        x=meses, y=valores, mode='lines+markers', name=str(ano),\n",
    "        line=dict(color=color_palette[idx % len(color_palette)])\n",
    "    ))\n",
    "fig_yt_heat.update_layout(\n",
    "    title='Evolução Mensal de Comentários (YouTube)',\n",
    "    xaxis_title='Mês', yaxis_title='Total de Comentários',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# Gráfico 2: Distribuição de Likes com curva\n",
    "yt_hist = np.histogram(yt_values, bins=12)\n",
    "x_mid = (yt_hist[1][:-1] + yt_hist[1][1:]) / 2\n",
    "x_smooth = np.linspace(min(yt_values), max(yt_values), 200)\n",
    "y_smooth = norm.pdf(x_smooth, yt_mu, yt_std) * len(yt_values) * np.mean(np.diff(yt_hist[1]))\n",
    "fig_yt_dist = go.Figure()\n",
    "fig_yt_dist.add_trace(go.Bar(\n",
    "    x=x_mid, y=yt_hist[0],\n",
    "    name='Likes', marker_color='white', opacity=0.7,\n",
    "    width=np.diff(yt_hist[1]) * 0.9\n",
    "))\n",
    "fig_yt_dist.add_trace(go.Scatter(\n",
    "    x=x_smooth, y=y_smooth,\n",
    "    name='Distribuição Normal', mode='lines', line=dict(color='red', width=2)\n",
    "))\n",
    "fig_yt_dist.update_layout(\n",
    "    title='Distribuição de Likes (YouTube)',\n",
    "    xaxis_title='Likes', yaxis_title='Frequência Estimada',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# Gráfico 3: Top Comentários por Likes\n",
    "fig_yt_top = go.Figure(go.Bar(\n",
    "    x=[c['texto'][:40] + '...' for c in yt_top],\n",
    "    y=[c['likes'] for c in yt_top],\n",
    "    marker_color='white'\n",
    "))\n",
    "fig_yt_top.update_layout(\n",
    "    title='Top Comentários por Likes',\n",
    "    xaxis_title='Comentário', yaxis_title='Likes',\n",
    "    template=px_template, plot_bgcolor='#121212', paper_bgcolor='#121212'\n",
    ")\n",
    "\n",
    "# === 4. App Dash ===\n",
    "app = dash.Dash(__name__)\n",
    "app.title = 'Dashboard YouTube'\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div(html.H1('Análise de Comentários do YouTube', style={'textAlign': 'center', 'color': 'white'}),\n",
    "             style={'padding': '20px', 'background': 'linear-gradient(to right, #000000, #434343)'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(figure=fig_yt_heat),\n",
    "        dcc.Graph(figure=fig_yt_dist),\n",
    "        dcc.Graph(figure=fig_yt_top)\n",
    "    ], style={'padding': '20px'})\n",
    "], style={'backgroundColor': '#121212', 'color': 'white', 'fontFamily': 'Arial'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=8051)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
