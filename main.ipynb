{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80bf6c08fb5459",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3> Todos os pip, ferramentas e bibliotecas utilizadas </h3> </div>\n",
    "\n",
    "- pip install python-dotenv ipywidgets cryptography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3> Chave API Oculta (Arquivo .env) </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba830f86466a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T13:31:15.566690Z",
     "start_time": "2025-04-30T13:31:15.391312Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from oauthlib.oauth2 import BearerToken\n",
    "\n",
    "# Carregar variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "chave_api = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aac8180eeb5b5c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Coleta de Dados Básicos e Interesses Relacionados à FURIA</h3> </div>\n",
    "  \n",
    "- **Formulário Inicial:** Criar um formulário em Jupyter Notebook (usando `ipywidgets` ou `Streamlit`) para capturar dados pessoais fundamentais: nome, CPF, endereço, data de nascimento, email etc. Validar formato de CPF (biblioteca `python-bcpf` ou validação via regex) e outros campos para evitar erros de digitação.  \n",
    "\n",
    "- **Interesses e Atividades em e-sports:** Incluir no notebook seções ou perguntas interativas sobre interesses em e-sports, times preferidos (FURIA), jogos mais acompanhados, frequência de eventos assistidos, ingressos ou periféricos adquiridos no último ano. Utilizar `pandas` para estruturar as respostas em um DataFrame. Poder-se-á simular coleta de dados de APIs públicas de eventos (por exemplo, dados de torneios CS:GO) ou pedir ao usuário que importe seu histórico (como um CSV de compras) para preencher esse perfil de maneira realista.  \n",
    "\n",
    "- **Compras Relacionadas:** Se for aplicável, permitir o upload de extratos simplificados ou listas de compras (como merch de teams e-sports). Usar `pandas` ou `openpyxl` para ler esses arquivos e filtrar itens de interesse (palavras-chave relacionadas a jogos, marcas de e-sports, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dca973f7b28f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import webbrowser\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import hashlib\n",
    "import threading\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "PORT = 8080\n",
    "DATA_DIR = \"form_data\"\n",
    "KEY_FILE = \"rg_encryption.key\"\n",
    "\n",
    "# Carrega ou gera a chave de criptografia\n",
    "if not os.path.exists(KEY_FILE):\n",
    "    key = Fernet.generate_key()\n",
    "    with open(KEY_FILE, 'wb') as kf:\n",
    "        kf.write(key)\n",
    "else:\n",
    "    with open(KEY_FILE, 'rb') as kf:\n",
    "        key = kf.read()\n",
    "fernet = Fernet(key)\n",
    "\n",
    "class MyHandler(http.server.SimpleHTTPRequestHandler):\n",
    "    def do_POST(self):\n",
    "        # Só processa a rota '/submit'\n",
    "        if self.path != '/submit':\n",
    "            return super().do_GET()\n",
    "\n",
    "        # Lê o corpo da requisição como JSON\n",
    "        content_length = int(self.headers.get('Content-Length', 0))\n",
    "        raw_body = self.rfile.read(content_length).decode('utf-8')\n",
    "        try:\n",
    "            dados = json.loads(raw_body)\n",
    "        except json.JSONDecodeError:\n",
    "            self.send_error(400, \"Bad Request: JSON inválido\")\n",
    "            return\n",
    "\n",
    "        # Hash do CPF\n",
    "        cpf_original = dados.get('cpf')\n",
    "        if cpf_original:\n",
    "            dados['cpf'] = hashlib.sha256(cpf_original.encode('utf-8')).hexdigest()\n",
    "        else:\n",
    "            print(\"Aviso: 'cpf' não enviado.\")\n",
    "\n",
    "        # Extrai imagens em Base64 do JSON\n",
    "        rg_base64     = dados.pop('rgImagem_base64', None)\n",
    "        selfie_base64 = dados.pop('selfieImagem_base64', None)\n",
    "\n",
    "        # Adiciona timestamp de submissão\n",
    "        dados['submitted_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "        # Gera ID único e prepara diretório\n",
    "        user_id = str(uuid.uuid4())\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "        # Criptografa e salva RG como arquivo .enc\n",
    "        if rg_base64:\n",
    "            try:\n",
    "                rg_bytes = base64.b64decode(rg_base64)\n",
    "                encrypted = fernet.encrypt(rg_bytes)\n",
    "                enc_filename = f\"{user_id}_rg.enc\"\n",
    "                enc_path = os.path.join(DATA_DIR, enc_filename)\n",
    "                with open(enc_path, 'wb') as ef:\n",
    "                    ef.write(encrypted)\n",
    "                dados['rgImagem_encrypted'] = enc_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criptografar/salvar RG: {e}\")\n",
    "\n",
    "        # Salva selfie como PNG (ou também criptografe se desejar)\n",
    "        if selfie_base64:\n",
    "            try:\n",
    "                selfie_bytes = base64.b64decode(selfie_base64)\n",
    "                selfie_filename = f\"{user_id}_selfie.png\"\n",
    "                selfie_path = os.path.join(DATA_DIR, selfie_filename)\n",
    "                with open(selfie_path, 'wb') as imgf:\n",
    "                    imgf.write(selfie_bytes)\n",
    "                dados['selfieImagem_file'] = selfie_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao salvar Selfie: {e}\")\n",
    "\n",
    "        # Grava JSON de metadados de forma atômica\n",
    "        file_path = os.path.join(DATA_DIR, f\"{user_id}.json\")\n",
    "        temp_path = file_path + \".tmp\"\n",
    "        try:\n",
    "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dados, f, indent=4, ensure_ascii=False)\n",
    "            os.replace(temp_path, file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar JSON: {e}\")\n",
    "            self.send_error(500, \"Internal Server Error: falha ao salvar dados\")\n",
    "            return\n",
    "\n",
    "        print(f\"Dados salvos (metadados + RG criptografado) em: {DATA_DIR}\")\n",
    "\n",
    "        # Envia resposta de sucesso\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'message': 'Dados eviados e criptografados com sucesso!',\n",
    "            'user_id': user_id\n",
    "        }\n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-Type', 'application/json')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(response).encode('utf-8'))\n",
    "\n",
    "        # Desliga o servidor após 4 segundos\n",
    "        threading.Timer(4.0, self.server.shutdown).start()\n",
    "\n",
    "\n",
    "def run_server():\n",
    "    with socketserver.TCPServer((\"\", PORT), MyHandler) as httpd:\n",
    "        print(f\"Servidor rodando em http://localhost:{PORT}\")\n",
    "        webbrowser.open(f\"http://localhost:{PORT}/Form/form.html\")\n",
    "        httpd.serve_forever()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6996e942e24f04",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Validação de Identidade com Abordagem de IA</h3> </div>\n",
    "\n",
    "- **Upload de Documentos:** Incluir um widget de upload de arquivos de imagem (RG, CNH ou passaporte) e, opcionalmente, uma selfie do usuário.  \n",
    "- **OCR e Extração de Dados:** Utilizar uma biblioteca de OCR como `pytesseract` ou `easyocr` para extrair texto do documento. Comparar nome e CPF extraídos com os dados básicos fornecidos para consistência.  \n",
    "- **Reconhecimento Facial:** Para fortalecer a validação, aplicar uma técnica de reconhecimento facial simples. Usar bibliotecas como `face_recognition` ou `OpenCV` com modelos pré-treinados para detectar rostos na selfie e na foto do documento, e então verificar se pertencem à mesma pessoa (por exemplo, comparando descritores faciais).  \n",
    "- **Feedback de Validação:** Exibir no notebook resultados da validação (válido/inválido) com base na correspondência de texto e face. Fornecer mensagens orientativas caso haja inconsistências (ex: “CPF não confere com o documento enviado”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed12b2bdcdbcf6",
   "metadata": {},
   "source": [
    "### Instale as bibliotecas necessárias\n",
    "\n",
    "- pytesseract (requer Tesseract OCR engine instalado separadamente)\n",
    " \n",
    "- Pillow (dependência do pytesseract)\n",
    " \n",
    "- face_recognition (requer dlib, pode ser complexo instalar em alguns sistemas)\n",
    "\n",
    "- ipywidgets (para o widget de upload)\n",
    "\n",
    "<br>\n",
    "\n",
    "```pip install pytesseract Pillow face_recognition ipywidgets ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e06c15ea2efa4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>OCR, Extração e Comparação de Dados Textuais</h3> </div>\n",
    "\n",
    "\n",
    "Definimos uma função para realizar o OCR na imagem do documento completo, extrair o texto, tentar encontrar Nome e CPF e compará-los com dados fornecidos (vamos simular esses dados fornecidos para o exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tesseract pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6bb22",
   "metadata": {},
   "source": [
    "### Validação Nome e Naturalidade (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82efa4ee6dc53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# ————————— Configurações Tesseract —————————\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "# OCR geral, dígitos e whitelist MRZ\n",
    "TESS_FULL   = \"--oem 3 --psm 3\"\n",
    "TESS_DIGITS = \"--oem 3 --psm 3 -c tessedit_char_whitelist=0123456789\"\n",
    "TESS_MRZ    = \"--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ<\"\n",
    "\n",
    "DATA_DIR  = \"form_data\"\n",
    "KEY_FILE  = \"rg_encryption.key\"\n",
    "\n",
    "# ————————— Carrega chave e dados do JSON mais recente —————————\n",
    "fernet        = Fernet(open(KEY_FILE, \"rb\").read())\n",
    "json_files    = sorted(\n",
    "    [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")],\n",
    "    key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)),\n",
    "    reverse=True\n",
    ")\n",
    "user_id       = os.path.splitext(json_files[0])[0]\n",
    "form_data     = json.load(open(os.path.join(DATA_DIR, json_files[0]), encoding=\"utf-8\"))\n",
    "provided_name = form_data.get(\"nome\", \"\")\n",
    "cpf_hash      = form_data.get(\"cpf\", \"\")\n",
    "\n",
    "# ————————— Decrypt + carrega imagem em alta resolução (×2) —————————\n",
    "encrypted = form_data[\"rgImagem_encrypted\"]\n",
    "raw       = fernet.decrypt(open(os.path.join(DATA_DIR, encrypted), \"rb\").read())\n",
    "pil       = Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "pil       = pil.resize((pil.width * 2, pil.height * 2), Image.LANCZOS)\n",
    "img       = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ————————— Pré-processamento —————————\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "cl = clahe.apply(gray)\n",
    "_, prep = cv2.threshold(cl, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# ————————— OCR completo (para debug) —————————\n",
    "full_ocr_text = pytesseract.image_to_string(prep, config=TESS_FULL, lang=\"por+eng\")\n",
    "\n",
    "# ————————— Função de validação de CPF —————————\n",
    "def valida_cpf(c: str) -> bool:\n",
    "    if len(c) != 11 or c == c[0]*11:\n",
    "        return False\n",
    "    s1 = sum(int(c[i])*(10-i) for i in range(9))\n",
    "    d1 = ((s1*10) % 11) % 10\n",
    "    s2 = sum(int(c[i])*(11-i) for i in range(10))\n",
    "    d2 = ((s2*10) % 11) % 10\n",
    "    return c[-2:] == f\"{d1}{d2}\"\n",
    "\n",
    "# ————————— Extrai Nome via MRZ —————————\n",
    "name = None\n",
    "name_ocr_raw = None\n",
    "mrz_lines = [L.strip() for L in full_ocr_text.splitlines() if \"<<\" in L]\n",
    "if mrz_lines:\n",
    "    mrz = mrz_lines[-1]\n",
    "    parts = mrz.split(\"<<\")\n",
    "    if len(parts) >= 2:\n",
    "        surname    = parts[0].title()\n",
    "        given_list = [p for p in parts[1].split(\"<\") if p]\n",
    "        given_list = list(reversed(given_list))\n",
    "        name       = \" \".join(p.title() for p in given_list + [surname])\n",
    "        name_ocr_raw = mrz\n",
    "\n",
    "def normalize(n: str) -> str:\n",
    "    t = re.sub(r'\\b(?:de|da|dos|das|do)\\b','', n, flags=re.IGNORECASE)\n",
    "    return re.sub(r'\\s+','', t).lower()\n",
    "\n",
    "name_match = bool(name and normalize(name) == normalize(provided_name))\n",
    "if name_match:\n",
    "    name = provided_name  # ajusta para o formato original\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# ————————— Configurações Tesseract —————————\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "TESS_CONF        = \"--oem 3 --psm 6\"\n",
    "TESS_CONF_DIGITS = \"--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789.-\"\n",
    "TESS_CONF_MRZ    = \"--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ<\"\n",
    "\n",
    "DATA_DIR  = \"form_data\"\n",
    "KEY_FILE  = \"rg_encryption.key\"\n",
    "\n",
    "# ————————— Carrega chave e dados do JSON mais recente —————————\n",
    "fernet        = Fernet(open(KEY_FILE, \"rb\").read())\n",
    "json_files    = sorted(\n",
    "    [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")],\n",
    "    key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)),\n",
    "    reverse=True\n",
    ")\n",
    "user_id       = os.path.splitext(json_files[0])[0]\n",
    "form_data     = json.load(open(os.path.join(DATA_DIR, json_files[0]), encoding=\"utf-8\"))\n",
    "provided_name = form_data.get(\"nome\", \"\")\n",
    "cpf_hash      = form_data.get(\"cpf\", \"\")\n",
    "\n",
    "# ————————— Decrypt + carrega imagem em alta resolução (×2) —————————\n",
    "encrypted = form_data[\"rgImagem_encrypted\"]\n",
    "raw       = fernet.decrypt(open(os.path.join(DATA_DIR, encrypted), \"rb\").read())\n",
    "pil       = Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "pil       = pil.resize((pil.width * 2, pil.height * 2), Image.LANCZOS)\n",
    "img       = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ————————— Pré-processamento —————————\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "cl = clahe.apply(gray)\n",
    "_, prep = cv2.threshold(cl, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# ————————— OCR completo (para debug) —————————\n",
    "full_ocr_text = pytesseract.image_to_string(prep, config=TESS_CONF)\n",
    "\n",
    "# ————————— Extrai Nome via MRZ —————————\n",
    "name = None\n",
    "name_ocr_raw = None\n",
    "mrz_lines = [L.strip() for L in full_ocr_text.splitlines() if \"<<\" in L]\n",
    "if mrz_lines:\n",
    "    mrz = mrz_lines[-1]\n",
    "    parts = mrz.split(\"<<\")\n",
    "    if len(parts) >= 2:\n",
    "        surname    = parts[0].title()\n",
    "        given_list = [p for p in parts[1].split(\"<\") if p]\n",
    "        given_list = list(reversed(given_list))\n",
    "        name       = \" \".join(p.title() for p in given_list + [surname])\n",
    "        name_ocr_raw = mrz\n",
    "\n",
    "def normalize(n: str) -> str:\n",
    "    t = re.sub(r'\\b(?:de|da|dos|das|do)\\b','', n, flags=re.IGNORECASE)\n",
    "    return re.sub(r'\\s+','', t).lower()\n",
    "\n",
    "name_match = bool(name and normalize(name) == normalize(provided_name))\n",
    "if name_match:\n",
    "    name = provided_name  # ajusta para o formato original\n",
    "\n",
    "# ————————— Extração da Naturalidade —————————\n",
    "naturalidade = None\n",
    "naturalidade_match = False\n",
    "\n",
    "# 1) Tenta padrões genéricos: “Naturalidade: Cidade – UF”\n",
    "m = re.search(\n",
    "    r'(?:Naturalidade|Natural de|Natural)\\s*[:\\-]?\\s*([\\wÀ-ú\\s]+)[-/]?\\s*([A-Za-z]{2})',\n",
    "    full_ocr_text,\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "if m:\n",
    "    cidade = m.group(1).strip().title()\n",
    "    estado = m.group(2).strip().upper()\n",
    "    naturalidade = f\"{cidade} - {estado}\"\n",
    "else:\n",
    "    # 2) Fallback: “ESTADO DE <Cidade> <UF>”\n",
    "    f = re.search(\n",
    "        r'ESTADO DE\\s+([\\wÀ-ú]+)\\s+([A-Za-z]{2})',\n",
    "        full_ocr_text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    if f:\n",
    "        cidade = f.group(1).strip().title()\n",
    "        estado = f.group(2).strip().upper()\n",
    "        naturalidade = f\"{cidade} - {estado}\"\n",
    "\n",
    "# ————————— Ajuste da comparação para usar \"endereco\" —————————\n",
    "expected_city = form_data.get(\"endereco\", \"\").strip().lower()  # aqui: \"utopia\"\n",
    "if naturalidade:\n",
    "    city_part = naturalidade.split(\" - \")[0].strip().lower()\n",
    "    naturalidade_match = (city_part == expected_city)\n",
    "\n",
    "# ————————— Resultado Final —————————\n",
    "result = {\n",
    "    \"full_ocr_text\":      full_ocr_text,\n",
    "    \"name\":               name,\n",
    "    \"name_ocr_raw\":       name_ocr_raw,\n",
    "    \"name_match\":         name_match,\n",
    "    \"naturalidade\":       naturalidade,\n",
    "    \"naturalidade_match\": naturalidade_match\n",
    "}\n",
    "\n",
    "# ————————— Atualiza o form_data e salva de volta no JSON —————————\n",
    "# (1) injeta os novos campos\n",
    "form_data[\"naturalidade_extraida\"] = naturalidade\n",
    "form_data[\"naturalidade_match\"]   = naturalidade_match\n",
    "form_data[\"name_match\"]           = name_match\n",
    "\n",
    "# (2) reescreve o arquivo JSON\n",
    "json_path = os.path.join(DATA_DIR, json_files[0])\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(form_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# (3) opcional: imprime o resultado atualizado\n",
    "print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cb67ef57bd894",
   "metadata": {},
   "source": [
    "### Reconhecimento e Comparação Facial\n",
    "\n",
    "Agora, definimos uma função para lidar com a comparação facial entre a foto do documento e a selfie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351981ce",
   "metadata": {},
   "source": [
    "Ao baixar o tesseract, lembrar de baixar a lingua portuguesa e adicionar na pasta -> C:\\Program Files\\Tesseract-OCR\\tessdata\n",
    "\n",
    "\"por.traineddata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"form_data\")\n",
    "KEY_FILE = Path(\"rg_encryption.key\")\n",
    "CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "FACE_SIZE = (150, 150)\n",
    "DISTANCE_THRESHOLD = 60.0  # adjustable threshold for face match\n",
    "\n",
    "def load_fernet_key(key_path: Path) -> Fernet:\n",
    "    \"\"\"\n",
    "    Load Fernet encryption key from file.\n",
    "    \"\"\"\n",
    "    with key_path.open(\"rb\") as f:\n",
    "        key = f.read()\n",
    "    return Fernet(key)\n",
    "\n",
    "\n",
    "def load_latest_form(data_dir: Path) -> tuple[Dict[str, Any], Path]:\n",
    "    \"\"\"\n",
    "    Load the most recent JSON form from data_dir, returning the data and its file path.\n",
    "    \"\"\"\n",
    "    json_files = sorted(data_dir.glob(\"*.json\"), key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {data_dir}\")\n",
    "    latest = json_files[0]\n",
    "    with latest.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # retornamos também o caminho do arquivo JSON para permitir regravação\n",
    "    return data, latest\n",
    "\n",
    "\n",
    "def decrypt_image(encrypted_filename: str, fernet: Fernet, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Decrypt an image file and return a PIL Image.\n",
    "    \"\"\"\n",
    "    encrypted_path = data_dir / encrypted_filename\n",
    "    if not encrypted_path.exists():\n",
    "        raise FileNotFoundError(f\"Encrypted file not found: {encrypted_path}\")\n",
    "    data = fernet.decrypt(encrypted_path.read_bytes())\n",
    "    return Image.open(io.BytesIO(data))\n",
    "\n",
    "\n",
    "def load_plain_image(filename: str, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load a non-encrypted image from disk as PIL Image.\n",
    "    \"\"\"\n",
    "    img_path = data_dir / filename\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "    return Image.open(img_path)\n",
    "\n",
    "\n",
    "def pil_to_bgr(img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to OpenCV BGR numpy array.\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def detect_face(image_bgr: np.ndarray, cascade: cv2.CascadeClassifier) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect the first face in a BGR image, crop and resize it.\n",
    "    Returns the grayscale face ROI or None.\n",
    "    \"\"\"\n",
    "    faces = cascade.detectMultiScale(image_bgr, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = faces[0]\n",
    "    face = image_bgr[y:y+h, x:x+w]\n",
    "    face_resized = cv2.resize(face, FACE_SIZE)\n",
    "    return cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def compare_faces(gray1: np.ndarray, gray2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean absolute difference between two grayscale face images.\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    return float(np.mean(diff))\n",
    "\n",
    "\n",
    "def perform_face_comparison_opencv(\n",
    "    img1_pil: Image.Image, img2_pil: Image.Image, threshold: float = DISTANCE_THRESHOLD\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare two PIL images containing faces using OpenCV Haar cascades.\n",
    "    Returns a dict with match result, distance, and status.\n",
    "    \"\"\"\n",
    "    results: Dict[str, Any] = {\n",
    "        'face_match': False,\n",
    "        'distance': None,\n",
    "        'success': False,\n",
    "        'message': ''\n",
    "    }\n",
    "    try:\n",
    "        img1_bgr = pil_to_bgr(img1_pil)\n",
    "        img2_bgr = pil_to_bgr(img2_pil)\n",
    "\n",
    "        cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "        if cascade.empty():\n",
    "            raise RuntimeError(f\"Failed to load cascade classifier at {CASCADE_PATH}\")\n",
    "\n",
    "        face1 = detect_face(img1_bgr, cascade)\n",
    "        face2 = detect_face(img2_bgr, cascade)\n",
    "\n",
    "        if face1 is None or face2 is None:\n",
    "            results['message'] = \"No face detected in one or both images.\"\n",
    "            return results\n",
    "\n",
    "        distance = compare_faces(face1, face2)\n",
    "        results['distance'] = float(distance)\n",
    "        results['face_match'] = bool(distance < threshold)\n",
    "        results['success'] = True\n",
    "        results['message'] = \"Comparison successful with OpenCV.\"\n",
    "    except Exception as e:\n",
    "        results['message'] = f\"Error during face comparison: {e}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    fernet = load_fernet_key(KEY_FILE)\n",
    "    form_data, form_path = load_latest_form(DATA_DIR)\n",
    "\n",
    "    # Decrypt RG image\n",
    "    encrypted_rg = form_data.get(\"rgImagem_encrypted\")\n",
    "    img_doc = decrypt_image(encrypted_rg, fernet, DATA_DIR)\n",
    "\n",
    "    # Load selfie (not encrypted)\n",
    "    selfie_filename = form_data.get(\"selfieImagem_file\")\n",
    "    img_selfie = load_plain_image(selfie_filename, DATA_DIR)\n",
    "\n",
    "    # Run comparison\n",
    "    result = perform_face_comparison_opencv(img_doc, img_selfie)\n",
    "\n",
    "    # Add result index to form data\n",
    "    form_data['rosto_correto_indice'] = 1 if result.get('face_match') else 0\n",
    "    form_data['face_match'] = result.get('face_match')\n",
    "    form_data['face_distance'] = result.get('distance')\n",
    "\n",
    "    # Save updated JSON back to file\n",
    "    with form_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(form_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Print JSON-safe result\n",
    "    print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e96c7714096a8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Integração com Redes Sociais (Simulada)</h3> </div>\n",
    "\n",
    "- **Linkagem de Contas:** Na ausência de integrações reais, simular o processo solicitando ao usuário os nomes de usuário ou perfis em redes sociais (Twitter, Facebook, Instagram). Criar células de código comentadas que demonstram como usar APIs (ex: `tweepy` para Twitter, `facebook-sdk` para Facebook) para conectar as contas.  \n",
    "- **Extração de Atividades Relacionadas a e-sports:** Para cada rede social simulada, mostrar como coletar dados relevantes: por exemplo, obter os últimos *tweets* do usuário e filtrar menções a e-sports ou FURIA; listar as páginas seguidas no Facebook com temas de gaming; ou verificar hashtags usadas em posts do Instagram. Usar `requests` e `BeautifulSoup` ou clientes de API para simulação. Armazenar essas informações em DataFrames para análise.  \n",
    "- **Monitoramento de Interações:** Demonstrar código que analisa curtidas, comentários ou retweets em publicações de e-sports (fingindo as permissões de API). Por exemplo, usar `tweepy` para buscar tweets que mencionem FURIA ou eventos de e-sports que o usuário retuitou ou comentou.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa7006b7b774eb",
   "metadata": {},
   "source": [
    "### Twitter (usuário individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dc903631095407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:00:59.340974Z",
     "start_time": "2025-04-30T14:00:58.696618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvos 15 tweets em form_data\\tweets_FURIA.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ————————— Configurações e autenticação —————————\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "# ————————— Carrega JSON com dados do usuário —————————\n",
    "DATA_DIR = 'form_data'\n",
    "json_files = sorted([\n",
    "    f for f in os.listdir(DATA_DIR) if f.endswith('.json')\n",
    "], key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)), reverse=True)\n",
    "json_path = os.path.join(DATA_DIR, json_files[0])\n",
    "form_data = json.load(open(json_path, encoding='utf-8'))\n",
    "\n",
    "# Extrai o username (sem o @) que está salvo no JSON, ex: \"@teste\" ou \"teste\"\n",
    "raw_handle = form_data.get('twitter', '').strip()\n",
    "twitter_handle = raw_handle.lstrip('@')\n",
    "if not twitter_handle:\n",
    "    raise ValueError('Nenhum handle de Twitter encontrado no JSON')\n",
    "\n",
    "# ————————— Termos específicos a buscar —————————\n",
    "TERMS = [\n",
    "    'Fallen', 'KSCERATO', 'yuurih', 'molodoy', 'skullz', 'chelo',\n",
    "    'fNb', 'Goot', 'Envy', 'Trigo', 'RedBert', 'Fntzy', 'R4re',\n",
    "    'Handyy', 'KDS', 'yanxnz', 'Lostt', 'nzr', 'Khalil', 'havoc',\n",
    "    'xand', 'mwzera', 'Xeratricky', 'Pandxrz', 'HisWattson',\n",
    "    '#FURIACS', '#FURIAR6', '#FURIAFC', '#DIADEFURIA'\n",
    "]\n",
    "\n",
    "# Monta query para buscar tweets do usuário contendo qualquer termo\n",
    "query_terms = ' OR '.join(f'\"{t}\"' for t in TERMS)\n",
    "query = f'from:{twitter_handle} ({query_terms}) -is:retweet lang:pt'\n",
    "\n",
    "# ————————— Busca tweets —————————\n",
    "max_results = 15  # até 100\n",
    "response = client.search_recent_tweets(\n",
    "    query=query,\n",
    "    max_results=max_results,\n",
    "    tweet_fields=['id', 'text', 'created_at', 'lang', 'source'],\n",
    "    expansions=['author_id'],\n",
    "    user_fields=['username', 'name', 'description', 'location']\n",
    ")\n",
    "\n",
    "# ————————— Monta resultados para salvar —————————\n",
    "tweets_data = []\n",
    "if response.data:\n",
    "    users_index = {u['id']: u for u in response.includes.get('users', [])}\n",
    "    for tweet in response.data:\n",
    "        info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source,\n",
    "            'author': {\n",
    "                'id': tweet.author_id,\n",
    "                'username': users_index[tweet.author_id].username,\n",
    "                'name': users_index[tweet.author_id].name,\n",
    "            }\n",
    "        }\n",
    "        tweets_data.append(info)\n",
    "\n",
    "# ————————— Salva em JSON —————————\n",
    "output_path = os.path.join(DATA_DIR, f'tweets_{twitter_handle}.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tweets_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Salvos {len(tweets_data)} tweets em {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd5d8a7e76898c",
   "metadata": {},
   "source": [
    "### Youtube (Análise Geral de Fãs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea82564d0b8b86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:08:55.632710Z",
     "start_time": "2025-04-30T14:08:43.298626Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Testa se a chave da API está sendo carregada\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "\n",
    "# ID do vídeo do qual você quer obter os comentários\n",
    "video_id = '8aIcU-_5W34'\n",
    "\n",
    "\n",
    "def get_video_channel_name(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Obtém as informações do vídeo\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição e pega o nome do canal\n",
    "    response = request.execute()\n",
    "    if response['items']:\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "        return channel_name\n",
    "    return None\n",
    "\n",
    "# Função para obter comentários do vídeo\n",
    "def get_comments(video_id, api_key):\n",
    "    # Conectando à API do YouTube\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Lista para armazenar os comentários\n",
    "    comments = []\n",
    "    \n",
    "    channel_name = get_video_channel_name(video_id, api_key)\n",
    "    \n",
    "    # Inicializa a requisição para obter os comentários\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=100  # Max resultados por requisição (pode ajustar conforme necessário)\n",
    "    )\n",
    "    \n",
    "    # Realiza a requisição\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Itera sobre os comentários e armazena os dados\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data = {\n",
    "                    'video_id': video_id,                    # ← aqui você adiciona o ID do vídeo\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'text': comment['textDisplay'],\n",
    "                    'published_at': comment['publishedAt'],\n",
    "                    'likes': comment['likeCount'],\n",
    "                    'channel_name': channel_name\n",
    "                }\n",
    "            \n",
    "            comments.append(comment_data)\n",
    "        \n",
    "        # Verifica se existe uma próxima página de resultados\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Obter os comentários\n",
    "# Função para salvar os comentários sem sobrescrever o arquivo existente\n",
    "def save_comments(comments, filename='comentarios_video.json'):\n",
    "    # Verifica se o arquivo já existe\n",
    "    if os.path.exists(filename):\n",
    "        # Carrega o conteúdo existente\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            existing_comments = json.load(f)\n",
    "        \n",
    "        # Adiciona os novos comentários ao conteúdo existente\n",
    "        existing_comments.extend(comments)\n",
    "        \n",
    "        # Salva o conteúdo atualizado\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_comments, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        # Caso o arquivo não exista, cria um novo com os comentários\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comments, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Obter os comentários\n",
    "comments = get_comments(video_id, api_key)\n",
    "\n",
    "# Salvar os comentários sem sobrescrever o arquivo\n",
    "save_comments(comments)\n",
    "\n",
    "print(\"Comentários salvos em 'comentarios_video.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5021376815ef2",
   "metadata": {},
   "source": [
    "### Análise Geral (Twitter/X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5376ad21a39994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ————————— Configurações e autenticação —————————\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Parâmetros de pesquisa com a hashtag #DIADEFURIA incluída\n",
    "query = '(Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR Trigo OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson OR #FURIACS OR #FURIAR6 OR #FURIAFC OR #DIADEFURIA) -is:retweet lang:pt'\n",
    "max_results = 10\n",
    "\n",
    "# Fazendo a busca com os campos desejados\n",
    "tweets = client.search_recent_tweets(query=query, max_results=max_results,\n",
    "                                     tweet_fields=[\"author_id\", \"conversation_id\", \"created_at\", \"geo\", \"id\", \"lang\", \"source\", \"text\"],\n",
    "                                     user_fields=[\"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"url\", \"username\"],\n",
    "                                     expansions=[\"author_id\"])\n",
    "\n",
    "# Convertendo os tweets para um formato de dicionário\n",
    "tweets_data = []\n",
    "if tweets.data:\n",
    "    for tweet in tweets.data:\n",
    "        tweet_info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'author_id': tweet.author_id,\n",
    "            'conversation_id': tweet.conversation_id,\n",
    "            'geo': tweet.geo,\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source\n",
    "        }\n",
    "\n",
    "        # Obtendo informações do usuário (quem postou o tweet)\n",
    "        if tweets.includes and 'users' in tweets.includes:\n",
    "            for user in tweets.includes['users']:\n",
    "                if user.id == tweet.author_id:\n",
    "                    tweet_info['user'] = {\n",
    "                        'created_at': str(user.created_at),\n",
    "                        'description': user.description,\n",
    "                        'entities': user.entities,\n",
    "                        'location': user.location,\n",
    "                        'name': user.name,\n",
    "                        'url': user.url,\n",
    "                        'username': user.username\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "        tweets_data.append(tweet_info)\n",
    "\n",
    "# Nome do arquivo\n",
    "arquivo_json = 'tweetsGerais_furia.json'\n",
    "\n",
    "# Carrega o conteúdo existente, se houver\n",
    "if os.path.exists(arquivo_json):\n",
    "    with open(arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_existentes = json.load(f)\n",
    "else:\n",
    "    dados_existentes = []\n",
    "\n",
    "# Adiciona os novos tweets\n",
    "dados_existentes.extend(tweets_data)\n",
    "\n",
    "# Salva de volta no JSON\n",
    "with open(arquivo_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dados_existentes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tweets adicionados com sucesso a 'tweets_furia.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9cecf67adf235",
   "metadata": {},
   "source": [
    "### Análise Geral (Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb4487c423268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "query = \"FURIA OR Fallen OR KSCERATO OR yuurih OR molodoy OR skullz OR chelo OR fNb OR Goot OR Envy OR RedBert OR Fntzy OR R4re OR Handyy OR KDS OR yanxnz OR Lostt OR nzr OR Khalil OR havoc OR xand OR mwzera OR Xeratricky OR Pandxrz OR HisWattson\"\n",
    "\n",
    "subreddits = [\"GlobalOffensive\", \"csgo\", \"VALORANT\", \"cs2\", \"cblol\", \"LolEsports\", \"ValorantCompetitive\", \"VCT\"]\n",
    "limit = 50\n",
    "\n",
    "resultados = []\n",
    "\n",
    "# Loop pelos subreddits\n",
    "for subreddit in subreddits:\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr=on&limit={limit}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"data\" in data and \"children\" in data[\"data\"]:\n",
    "        for post in data[\"data\"][\"children\"]:\n",
    "            p = post[\"data\"]\n",
    "\n",
    "            # Verifica se a query aparece no título ou no texto do post\n",
    "            titulo = p.get(\"title\", \"\")\n",
    "            texto = p.get(\"selftext\", \"\")\n",
    "\n",
    "            # Verificação literal, sem usar lower()\n",
    "            if any(jogador in titulo or jogador in texto for jogador in query.split(\" OR \")):\n",
    "                resultados.append({\n",
    "                    \"titulo\": p.get(\"title\"),\n",
    "                    \"autor\": p.get(\"author\"),\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"score\": p.get(\"score\", 0),\n",
    "                    \"url\": \"https://reddit.com\" + p.get(\"permalink\"),\n",
    "                    \"data_criacao\": p.get(\"created_utc\"),\n",
    "                    \"comentario_exemplo\": p.get(\"selftext\", \"\")\n",
    "                })\n",
    "\n",
    "# Salva em JSON\n",
    "with open(\"posts_furia_reddit.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultados, f, indent=4, ensure_ascii=False)\n",
    "    print(\"Dados salvos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5023f63bcefe89",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Enriquecimento de Perfil com Dados Sociais e Multimídia</h3> </div>\n",
    "\n",
    "- **Análise de Comentários:** Para integrar comentários prévios do usuário no YouTube, Reddit e Twitter, incluir blocos que consumam APIs ou dados locais de análise anterior (supondo que existam). Usar `google-api-python-client` para extrair comentários de vídeos de e-sports do YouTube, `PRAW` para posts/comentários no Reddit, e `tweepy` ou dados simulados para tweets.  \n",
    "- **Processamento de Linguagem Natural:** Aplicar NLP para entender o perfil do usuário: usar bibliotecas como `transformers` ou `spaCy` para classificar sentimento, identificar tópicos ou palavras-chave frequentes nesses comentários. Por exemplo, gerar um gráfico de palavras-chave mais mencionadas em e-sports, ou uma análise de sentimento geral sobre jogos específicos.  \n",
    "- **Integração de Informações:** Combinar esses insights com os interesses declarados pelo usuário. Exibir visualmente (via `matplotlib` ou `seaborn`) uma nuvem de palavras ou gráfico que mostre as categorias de e-sports mais relevantes para o perfil (baseado em interesses + análise de comentários).  \n",
    "- **Perfis em Sites de e-Sports:** Permitir que o usuário insira links para seus perfis em plataformas de e-sports (como GameBattles, HLTV, Liquipedia). Usar `requests` e `BeautifulSoup` para raspar detalhes do perfil (jogos, histórico de partidas). Em seguida, aplicar um modelo de IA (ex: `transformers` BERT) para classificar se o conteúdo textual do perfil é relevante às preferências do usuário (por exemplo, buscando termos de jogos citados pelo usuário). Mostrar se há “match” entre interesses do usuário e informações do perfil scraped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc151832ad30fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e65090fbbfcb5a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Estruturação do Notebook</h3> </div>\n",
    "\n",
    "- Organizar o notebook em seções claras conforme as etapas acima: **Coleta de Dados**, **Validação de Identidade**, **Integração de Redes Sociais**, **Enriquecimento com Dados Sociais**, **Conclusão**.  \n",
    "- Incluir explicações breves em cada seção usando células Markdown, resumindo o objetivo daquela etapa. Combinar descrições em texto com células de código demonstrativas.  \n",
    "- Sugerir bibliotecas específicas no contexto de cada etapa: por exemplo, mencionar `ipywidgets` ou `streamlit` na coleta de dados, `pytesseract`/`OpenCV` na validação de documentos, `tweepy`/`PRAW`/`BeautifulSoup` na integração social, e `transformers`/`spaCy` na análise de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b8c4d4857ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfadd7aa567531f2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Dicas de Apresentação do Protótipo</h3> </div>\n",
    "\n",
    "- **Formatação Atraente:** Usar cabeçalhos (`#`, `##`), listas e imagens (logotipos de e-sports, ícones de redes sociais) para tornar o notebook visualmente agradável. Células Markdown bem elaboradas ajudam na legibilidade.  \n",
    "- **Interatividade:** Incluir elementos interativos (sliders, botões de upload, caixas de seleção) via `ipywidgets` para simular um fluxo real de uso. Isso torna a demonstração dinâmica mesmo no ambiente de notebook.  \n",
    "- **Visualização de Dados:** Aproveitar gráficos (matplotlib, seaborn ou plotly) para mostrar perfis de interesse ou resultados das análises de comentários. Um gráfico de barras ou nuvem de palavras torna o conteúdo mais didático.  \n",
    "- **Narração do Código:** Inserir comentários explicativos e outputs de exemplo que guiem o avaliador pelo processo passo a passo. Ao final, apresentar um breve resumo dos resultados obtidos para evidenciar que todos os requisitos foram atendidos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37fafd0bfde477",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Conclusão</h3> </div>\n",
    "\n",
    "Este plano garante uma implementação completa dos requisitos do desafio, integrando coleta de informações pessoais e de interesse em e-sports, validação de identidade baseada em IA, simulação de integração social e enriquecimento de perfil com dados externos. A organização em seções claras, o uso de bibliotecas especializadas (ex: **Streamlit/ipywidgets** para interfaces, **OpenCV/face_recognition** para validação, **transformers/spaCy** para IA, **pandas** para dados) e as sugestões de apresentação asseguram uma entrega alinhada e de fácil acompanhamento, mesmo no formato de notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
