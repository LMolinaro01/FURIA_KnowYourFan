{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan (Individual)</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a6930",
   "metadata": {},
   "source": [
    "### ConfiguraÃ§Ãµes Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "#### ImportaÃ§Ã£o Chave API (Arquivo .env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba830f86466a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T13:31:15.566690Z",
     "start_time": "2025-04-30T13:31:15.391312Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from oauthlib.oauth2 import BearerToken\n",
    "\n",
    "# Carregar variÃ¡veis do .env\n",
    "load_dotenv()\n",
    "\n",
    "chave_api = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9843e",
   "metadata": {},
   "source": [
    "### Coleta e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2778e57",
   "metadata": {},
   "source": [
    "#### Coleta de Dados (FormulÃ¡rio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dca973f7b28f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import webbrowser\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import hashlib\n",
    "import threading\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "PORT = 8080\n",
    "DATA_DIR = \"form_data\"\n",
    "KEY_FILE = \"rg_encryption.key\"\n",
    "\n",
    "# Carrega ou gera a chave de criptografia\n",
    "if not os.path.exists(KEY_FILE):\n",
    "    key = Fernet.generate_key()\n",
    "    with open(KEY_FILE, 'wb') as kf:\n",
    "        kf.write(key)\n",
    "else:\n",
    "    with open(KEY_FILE, 'rb') as kf:\n",
    "        key = kf.read()\n",
    "fernet = Fernet(key)\n",
    "\n",
    "class MyHandler(http.server.SimpleHTTPRequestHandler):\n",
    "    def do_POST(self):\n",
    "        # SÃ³ processa a rota '/submit'\n",
    "        if self.path != '/submit':\n",
    "            return super().do_GET()\n",
    "\n",
    "        # LÃª o corpo da requisiÃ§Ã£o como JSON\n",
    "        content_length = int(self.headers.get('Content-Length', 0))\n",
    "        raw_body = self.rfile.read(content_length).decode('utf-8')\n",
    "        try:\n",
    "            dados = json.loads(raw_body)\n",
    "        except json.JSONDecodeError:\n",
    "            self.send_error(400, \"Bad Request: JSON invÃ¡lido\")\n",
    "            return\n",
    "\n",
    "        # Hash do CPF\n",
    "        cpf_original = dados.get('cpf')\n",
    "        if cpf_original:\n",
    "            dados['cpf'] = hashlib.sha256(cpf_original.encode('utf-8')).hexdigest()\n",
    "        else:\n",
    "            print(\"Aviso: 'cpf' nÃ£o enviado.\")\n",
    "\n",
    "        # Extrai imagens em Base64 do JSON\n",
    "        rg_base64     = dados.pop('rgImagem_base64', None)\n",
    "        selfie_base64 = dados.pop('selfieImagem_base64', None)\n",
    "\n",
    "        # Adiciona timestamp de submissÃ£o\n",
    "        dados['submitted_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "        # Gera ID Ãºnico e prepara diretÃ³rio\n",
    "        user_id = str(uuid.uuid4())\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "        # Criptografa e salva RG como arquivo .enc\n",
    "        if rg_base64:\n",
    "            try:\n",
    "                rg_bytes = base64.b64decode(rg_base64)\n",
    "                encrypted = fernet.encrypt(rg_bytes)\n",
    "                enc_filename = f\"{user_id}_rg.enc\"\n",
    "                enc_path = os.path.join(DATA_DIR, enc_filename)\n",
    "                with open(enc_path, 'wb') as ef:\n",
    "                    ef.write(encrypted)\n",
    "                dados['rgImagem_encrypted'] = enc_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criptografar/salvar RG: {e}\")\n",
    "\n",
    "        # Salva selfie como PNG (ou tambÃ©m criptografe se desejar)\n",
    "        if selfie_base64:\n",
    "            try:\n",
    "                selfie_bytes = base64.b64decode(selfie_base64)\n",
    "                selfie_filename = f\"{user_id}_selfie.png\"\n",
    "                selfie_path = os.path.join(DATA_DIR, selfie_filename)\n",
    "                with open(selfie_path, 'wb') as imgf:\n",
    "                    imgf.write(selfie_bytes)\n",
    "                dados['selfieImagem_file'] = selfie_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao salvar Selfie: {e}\")\n",
    "\n",
    "        # Grava JSON de metadados de forma atÃ´mica\n",
    "        file_path = os.path.join(DATA_DIR, f\"{user_id}.json\")\n",
    "        temp_path = file_path + \".tmp\"\n",
    "        try:\n",
    "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dados, f, indent=4, ensure_ascii=False)\n",
    "            os.replace(temp_path, file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar JSON: {e}\")\n",
    "            self.send_error(500, \"Internal Server Error: falha ao salvar dados\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            user_id_info = {'last_user_id': user_id}\n",
    "            with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(user_id_info, f, indent=4, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar last_user_id.json: {e}\")\n",
    "\n",
    "        # Envia resposta de sucesso\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'message': 'Dados eviados e criptografados com sucesso!',\n",
    "            'user_id': user_id\n",
    "        }\n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-Type', 'application/json')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(response).encode('utf-8'))\n",
    "\n",
    "        # Desliga o servidor apÃ³s 4 segundos\n",
    "        threading.Timer(4.0, self.server.shutdown).start()\n",
    "\n",
    "\n",
    "def run_server():\n",
    "    with socketserver.TCPServer((\"\", PORT), MyHandler) as httpd:\n",
    "        print(f\"Servidor rodando em http://localhost:{PORT}\")\n",
    "        webbrowser.open(f\"http://localhost:{PORT}/Form/form.html\")\n",
    "        httpd.serve_forever()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6996e942e24f04",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>ValidaÃ§Ã£o de Identidade com Abordagem de IA</h3> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed12b2bdcdbcf6",
   "metadata": {},
   "source": [
    "##### Instale as bibliotecas necessÃ¡rias\n",
    "\n",
    "- pytesseract (requer Tesseract OCR engine instalado separadamente)\n",
    " \n",
    "- Pillow (dependÃªncia do pytesseract)\n",
    " \n",
    "- face_recognition (requer dlib, pode ser complexo instalar em alguns sistemas)\n",
    "\n",
    "- ipywidgets (para o widget de upload)\n",
    "\n",
    "<br>\n",
    "\n",
    "```pip install pytesseract Pillow face_recognition ipywidgets ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e06c15ea2efa4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h4>OCR, ExtraÃ§Ã£o e ComparaÃ§Ã£o de Dados Textuais</h4> </div>\n",
    "\n",
    "\n",
    "Definimos uma funÃ§Ã£o para realizar o OCR na imagem do documento completo, extrair o texto, tentar encontrar Nome e CPF e comparÃ¡-los com dados fornecidos (vamos simular esses dados fornecidos para o exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tesseract pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6bb22",
   "metadata": {},
   "source": [
    "#### ValidaÃ§Ã£o Nome e Naturalidade (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82efa4ee6dc53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” ConfiguraÃ§Ãµes Tesseract â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "TESS_CONF_FULL   = \"--oem 3 --psm 3\"\n",
    "TESS_CONF_DIGITS = \"--oem 3 --psm 3 -c tessedit_char_whitelist=0123456789\"\n",
    "TESS_CONF_MRZ    = \"--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ<\"\n",
    "\n",
    "DATA_DIR = \"form_data\"\n",
    "KEY_FILE = \"rg_encryption.key\"\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Carrega chave e dados do JSON mais recente vÃ¡lido â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "fernet = Fernet(open(KEY_FILE, \"rb\").read())\n",
    "\n",
    "# Lista e filtra os JSONs vÃ¡lidos (com estrutura de dicionÃ¡rio)\n",
    "json_files = sorted(\n",
    "    [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\") and f != 'last_user_id.json'],\n",
    "    key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "valid_json_files = []\n",
    "for f in json_files:\n",
    "    path = os.path.join(DATA_DIR, f)\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as j:\n",
    "            data = json.load(j)\n",
    "            if isinstance(data, dict):\n",
    "                valid_json_files.append(f)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not valid_json_files:\n",
    "    raise FileNotFoundError(\"Nenhum JSON vÃ¡lido (tipo dicionÃ¡rio) encontrado em 'form_data'.\")\n",
    "\n",
    "latest_json = valid_json_files[0]\n",
    "user_id = os.path.splitext(latest_json)[0]\n",
    "form_data = json.load(open(os.path.join(DATA_DIR, latest_json), encoding=\"utf-8\"))\n",
    "provided_name = form_data.get(\"nome\", \"\")\n",
    "cpf_hash = form_data.get(\"cpf\", \"\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Decrypt + carrega imagem em alta resoluÃ§Ã£o (Ã—2) â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "encrypted_fn = form_data.get(\"rgImagem_encrypted\")\n",
    "if encrypted_fn:\n",
    "    enc_path = os.path.join(DATA_DIR, encrypted_fn)\n",
    "    try:\n",
    "        raw_bytes = fernet.decrypt(open(enc_path, \"rb\").read())\n",
    "        pil = Image.open(io.BytesIO(raw_bytes)).convert(\"RGB\")\n",
    "        pil = pil.resize((pil.width * 2, pil.height * 2), Image.LANCZOS)\n",
    "        img = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao descriptografar '{encrypted_fn}': {e}\")\n",
    "        img = None\n",
    "else:\n",
    "    print(\"Aviso: 'rgImagem_encrypted' not found in form_data\")\n",
    "    img = None\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” PrÃ©-processamento e OCR â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "if img is not None:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(gray)\n",
    "    _, prep = cv2.threshold(cl, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    full_ocr_text = pytesseract.image_to_string(prep, config=TESS_CONF_FULL, lang=\"por+eng\")\n",
    "else:\n",
    "    full_ocr_text = \"\"\n",
    "    print(\"Skipping OCR: no image available.\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” FunÃ§Ã£o de validaÃ§Ã£o de CPF â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def valida_cpf(c: str) -> bool:\n",
    "    if len(c) != 11 or c == c[0]*11:\n",
    "        return False\n",
    "    s1 = sum(int(c[i])*(10-i) for i in range(9))\n",
    "    d1 = ((s1*10) % 11) % 10\n",
    "    s2 = sum(int(c[i])*(11-i) for i in range(10))\n",
    "    d2 = ((s2*10) % 11) % 10\n",
    "    return c[-2:] == f\"{d1}{d2}\"\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Extrai Nome via MRZ â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "name = None\n",
    "name_ocr_raw = None\n",
    "mrz_lines = [L.strip() for L in full_ocr_text.splitlines() if \"<<\" in L]\n",
    "if mrz_lines:\n",
    "    mrz = mrz_lines[-1]\n",
    "    parts = mrz.split(\"<<\")\n",
    "    if len(parts) >= 2:\n",
    "        surname = parts[0].title()\n",
    "        given_list = [p for p in parts[1].split(\"<\") if p]\n",
    "        given_reversed = list(reversed(given_list))\n",
    "        name = \" \".join(p.title() for p in given_reversed + [surname])\n",
    "        name_ocr_raw = mrz\n",
    "\n",
    "# Normalize and compare\n",
    "def normalize(n: str) -> str:\n",
    "    t = re.sub(r'\\b(?:de|da|dos|das|do)\\b','', n, flags=re.IGNORECASE)\n",
    "    return re.sub(r'\\s+','', t).lower()\n",
    "\n",
    "name_match = bool(name and normalize(name) == normalize(provided_name))\n",
    "if name_match:\n",
    "    name = provided_name\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” ExtraÃ§Ã£o da Naturalidade â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "naturalidade = None\n",
    "naturalidade_match = False\n",
    "m = re.search(\n",
    "    r'(?:Naturalidade|Natural de|Natural)\\s*[:\\-]?\\s*([\\wÃ€-Ãº\\s]+)[-/]?\\s*([A-Za-z]{2})',\n",
    "    full_ocr_text,\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "if m:\n",
    "    cidade = m.group(1).strip().title()\n",
    "    estado = m.group(2).strip().upper()\n",
    "    naturalidade = f\"{cidade} - {estado}\"\n",
    "else:\n",
    "    f2 = re.search(r'ESTADO DE\\s+([\\wÃ€-Ãº]+)\\s+([A-Za-z]{2})', full_ocr_text, flags=re.IGNORECASE)\n",
    "    if f2:\n",
    "        cidade = f2.group(1).strip().title()\n",
    "        estado = f2.group(2).strip().upper()\n",
    "        naturalidade = f\"{cidade} - {estado}\"\n",
    "\n",
    "expected_city = form_data.get(\"endereco\", \"\").strip().lower()\n",
    "if naturalidade:\n",
    "    naturalidade_match = (naturalidade.split(\" - \")[0].strip().lower() == expected_city)\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Atualiza e salva o JSON â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "form_data.update({\n",
    "    \"naturalidade_extraida\": naturalidade,\n",
    "    \"naturalidade_match\": naturalidade_match,\n",
    "    \"name_match\": name_match\n",
    "})\n",
    "with open(os.path.join(DATA_DIR, latest_json), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(form_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Exibe resultado â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "result = {\n",
    "    \"full_ocr_text\": full_ocr_text,\n",
    "    \"name\": name,\n",
    "    \"name_ocr_raw\": name_ocr_raw,\n",
    "    \"name_match\": name_match,\n",
    "    \"naturalidade\": naturalidade,\n",
    "    \"naturalidade_match\": naturalidade_match\n",
    "}\n",
    "print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cb67ef57bd894",
   "metadata": {},
   "source": [
    "#### Reconhecimento e ComparaÃ§Ã£o Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351981ce",
   "metadata": {},
   "source": [
    "Ao baixar o tesseract, lembrar de baixar a lingua portuguesa e adicionar na pasta -> C:\\Program Files\\Tesseract-OCR\\tessdata\n",
    "\n",
    "\"por.traineddata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"form_data\")\n",
    "KEY_FILE = Path(\"rg_encryption.key\")\n",
    "CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "FACE_SIZE = (150, 150)\n",
    "DISTANCE_THRESHOLD = 60.0  # adjustable threshold for face match\n",
    "\n",
    "def load_fernet_key(key_path: Path) -> Fernet:\n",
    "    \"\"\"\n",
    "    Load Fernet encryption key from file.\n",
    "    \"\"\"\n",
    "    with key_path.open(\"rb\") as f:\n",
    "        key = f.read()\n",
    "    return Fernet(key)\n",
    "\n",
    "\n",
    "def load_latest_form(data_dir: Path) -> tuple[Dict[str, Any], Path]:\n",
    "    \"\"\"\n",
    "    Load the most recent JSON form from data_dir, returning the data and its file path.\n",
    "    \"\"\"\n",
    "    json_files = sorted(data_dir.glob(\"*.json\"), key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {data_dir}\")\n",
    "    latest = json_files[0]\n",
    "    with latest.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # retornamos tambÃ©m o caminho do arquivo JSON para permitir regravaÃ§Ã£o\n",
    "    return data, latest\n",
    "\n",
    "\n",
    "def decrypt_image(encrypted_filename: str, fernet: Fernet, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Decrypt an image file and return a PIL Image.\n",
    "    \"\"\"\n",
    "    encrypted_path = data_dir / encrypted_filename\n",
    "    if not encrypted_path.exists():\n",
    "        raise FileNotFoundError(f\"Encrypted file not found: {encrypted_path}\")\n",
    "    data = fernet.decrypt(encrypted_path.read_bytes())\n",
    "    return Image.open(io.BytesIO(data))\n",
    "\n",
    "\n",
    "def load_plain_image(filename: str, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load a non-encrypted image from disk as PIL Image.\n",
    "    \"\"\"\n",
    "    img_path = data_dir / filename\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "    return Image.open(img_path)\n",
    "\n",
    "\n",
    "def pil_to_bgr(img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to OpenCV BGR numpy array.\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def detect_face(image_bgr: np.ndarray, cascade: cv2.CascadeClassifier) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect the first face in a BGR image, crop and resize it.\n",
    "    Returns the grayscale face ROI or None.\n",
    "    \"\"\"\n",
    "    faces = cascade.detectMultiScale(image_bgr, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = faces[0]\n",
    "    face = image_bgr[y:y+h, x:x+w]\n",
    "    face_resized = cv2.resize(face, FACE_SIZE)\n",
    "    return cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def compare_faces(gray1: np.ndarray, gray2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean absolute difference between two grayscale face images.\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    return float(np.mean(diff))\n",
    "\n",
    "\n",
    "def perform_face_comparison_opencv(\n",
    "    img1_pil: Image.Image, img2_pil: Image.Image, threshold: float = DISTANCE_THRESHOLD\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare two PIL images containing faces using OpenCV Haar cascades.\n",
    "    Returns a dict with match result, distance, and status.\n",
    "    \"\"\"\n",
    "    results: Dict[str, Any] = {\n",
    "        'face_match': False,\n",
    "        'distance': None,\n",
    "        'success': False,\n",
    "        'message': ''\n",
    "    }\n",
    "    try:\n",
    "        img1_bgr = pil_to_bgr(img1_pil)\n",
    "        img2_bgr = pil_to_bgr(img2_pil)\n",
    "\n",
    "        cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "        if cascade.empty():\n",
    "            raise RuntimeError(f\"Failed to load cascade classifier at {CASCADE_PATH}\")\n",
    "\n",
    "        face1 = detect_face(img1_bgr, cascade)\n",
    "        face2 = detect_face(img2_bgr, cascade)\n",
    "\n",
    "        if face1 is None or face2 is None:\n",
    "            results['message'] = \"No face detected in one or both images.\"\n",
    "            return results\n",
    "\n",
    "        distance = compare_faces(face1, face2)\n",
    "        results['distance'] = float(distance)\n",
    "        results['face_match'] = bool(distance < threshold)\n",
    "        results['success'] = True\n",
    "        results['message'] = \"Comparison successful with OpenCV.\"\n",
    "    except Exception as e:\n",
    "        results['message'] = f\"Error during face comparison: {e}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    fernet = load_fernet_key(KEY_FILE)\n",
    "    form_data, form_path = load_latest_form(DATA_DIR)\n",
    "\n",
    "    # Decrypt RG image\n",
    "    encrypted_rg = form_data.get(\"rgImagem_encrypted\")\n",
    "    img_doc = decrypt_image(encrypted_rg, fernet, DATA_DIR)\n",
    "\n",
    "    # Load selfie (not encrypted)\n",
    "    selfie_filename = form_data.get(\"selfieImagem_file\")\n",
    "    img_selfie = load_plain_image(selfie_filename, DATA_DIR)\n",
    "\n",
    "    # Run comparison\n",
    "    result = perform_face_comparison_opencv(img_doc, img_selfie)\n",
    "\n",
    "    # Add result index to form data\n",
    "    form_data['rosto_correto_indice'] = 1 if result.get('face_match') else 0\n",
    "    form_data['face_match'] = result.get('face_match')\n",
    "    form_data['face_distance'] = result.get('distance')\n",
    "\n",
    "    # Save updated JSON back to file\n",
    "    with form_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(form_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Print JSON-safe result\n",
    "    print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e96c7714096a8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>IntegraÃ§Ã£o com Redes Sociais (Simulada)</h3> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa7006b7b774eb",
   "metadata": {},
   "source": [
    "#### Twitter (usuÃ¡rio individual) - Chamada API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc903631095407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:00:59.340974Z",
     "start_time": "2025-04-30T14:00:58.696618Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” ConfiguraÃ§Ãµes e autenticaÃ§Ã£o â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Carrega JSON com dados do usuÃ¡rio â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "DATA_DIR = 'form_data'\n",
    "json_files = sorted([\n",
    "    f for f in os.listdir(DATA_DIR) if f.endswith('.json')\n",
    "], key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)), reverse=True)\n",
    "json_path = os.path.join(DATA_DIR, json_files[0])\n",
    "form_data = json.load(open(json_path, encoding='utf-8'))\n",
    "\n",
    "# Extrai o username (sem o @) que estÃ¡ salvo no JSON, ex: \"@teste\" ou \"teste\"\n",
    "raw_handle = form_data.get('twitter', '').strip()\n",
    "twitter_handle = raw_handle.lstrip('@')\n",
    "if not twitter_handle:\n",
    "    raise ValueError('Nenhum handle de Twitter encontrado no JSON')\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Termos especÃ­ficos a buscar â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "TERMS = [\n",
    "    'Fallen', 'KSCERATO', 'yuurih', 'molodoy', 'skullz', 'chelo',\n",
    "    'fNb', 'Goot', 'Envy', 'Trigo', 'RedBert', 'Fntzy', 'R4re',\n",
    "    'Handyy', 'KDS', 'yanxnz', 'Lostt', 'nzr', 'Khalil', 'havoc',\n",
    "    'xand', 'mwzera', 'Xeratricky', 'Pandxrz', 'HisWattson',\n",
    "    '#FURIACS', '#FURIAR6', '#FURIAFC', '#DIADEFURIA'\n",
    "]\n",
    "\n",
    "# Monta query para buscar tweets do usuÃ¡rio contendo qualquer termo\n",
    "query_terms = ' OR '.join(f'\"{t}\"' for t in TERMS)\n",
    "query = f'from:{twitter_handle} ({query_terms}) -is:retweet lang:pt'\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Busca tweets â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "max_results = 15  # atÃ© 100\n",
    "response = client.search_recent_tweets(\n",
    "    query=query,\n",
    "    max_results=max_results,\n",
    "    tweet_fields=['id', 'text', 'created_at', 'lang', 'source'],\n",
    "    expansions=['author_id'],\n",
    "    user_fields=['username', 'name', 'description', 'location']\n",
    ")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Monta resultados para salvar â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "tweets_data = []\n",
    "if response.data:\n",
    "    users_index = {u['id']: u for u in response.includes.get('users', [])}\n",
    "    for tweet in response.data:\n",
    "        info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source,\n",
    "            'author': {\n",
    "                'id': tweet.author_id,\n",
    "                'username': users_index[tweet.author_id].username,\n",
    "                'name': users_index[tweet.author_id].name,\n",
    "            }\n",
    "        }\n",
    "        tweets_data.append(info)\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€” Salva em JSON â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "output_path = os.path.join(DATA_DIR, f'tweets_User.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tweets_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Salvos {len(tweets_data)} tweets em {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67b27c",
   "metadata": {},
   "source": [
    "### AnÃ¡lise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0944",
   "metadata": {},
   "source": [
    "#### AnÃ¡lise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ====== 1. Carrega os dados JSON ======\n",
    "with open(r'form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Elimina duplicatas baseado no texto ======\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# ====== 3. Adiciona likes aleatÃ³rios para simulaÃ§Ã£o ======\n",
    "df['likes'] = [random.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Filtra os 200 melhores por likes ======\n",
    "top200 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# ====== 5. Remove palavras banidas ======\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "top200_filtrado = top200[~top200['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# ====== 6. Cria pipeline de anÃ¡lise de sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# ====== 7. Aplica o modelo de anÃ¡lise de sentimento ======\n",
    "batch_size = 32\n",
    "top200_filtrado = top200_filtrado.reset_index(drop=True)\n",
    "texts = top200_filtrado['text'].tolist()\n",
    "results = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results.extend(sentiment_analyzer(batch, truncation=True))\n",
    "\n",
    "# ====== 8. Normaliza os scores de sentimento ======\n",
    "scores = [(int(res['label'][0]) - 1) / 4 for res in results]\n",
    "top200_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# ====== 9. Adiciona um campo \"channel_name\" ======\n",
    "top200_filtrado['channel_name'] = top200_filtrado['user'].apply(lambda a: a['username'])\n",
    "\n",
    "# ====== 10. Pega os top 10 comentÃ¡rios por canal ======\n",
    "top_comentarios_canal = []\n",
    "for canal, grupo in top200_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(grupo.sort_values('sentiment_score', ascending=False).head(10))\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# ====== 11. FunÃ§Ãµes auxiliares ======\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "\n",
    "def estrela_para_sentimento(score):\n",
    "    \"\"\"\n",
    "    Converte score normalizado (0-1) para sentimento textual em 1-5 estrelas.\n",
    "    \"\"\"\n",
    "    stars = int(round(score * 4)) + 1\n",
    "    if stars == 1:\n",
    "        return \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        return \"negativo\"\n",
    "    elif stars == 3:\n",
    "        return \"neutro\"\n",
    "    elif stars == 4:\n",
    "        return \"positivo\"\n",
    "    else:\n",
    "        return \"muito positivo\"\n",
    "\n",
    "# ====== 12. FunÃ§Ã£o para gerar grÃ¡fico com sentimento ======\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    sentiment_scores = df['sentiment_score']\n",
    "    \n",
    "    spacing = 1.5\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(12, len(df) * 1.5))\n",
    "    plt.barh(y_positions, sentiment_scores, color='skyblue')  # azul claro\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('Score de Sentimento (0 - Negativo, 1 - Positivo)')\n",
    "    plt.title(f'ComentÃ¡rios e Sentimento - @{canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o score de sentimento e texto do sentimento ao lado de cada barra\n",
    "    for y, score in zip(y_positions, sentiment_scores):\n",
    "        sentimento = estrela_para_sentimento(score)\n",
    "        plt.text(score + 0.01, y, f'{score:.2f} ({sentimento})', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ====== 13. Gera o grÃ¡fico para cada canal ======\n",
    "for canal, grupo in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(grupo, canal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b47022",
   "metadata": {},
   "source": [
    "#### FrequÃªncia de Tweets (Intervalo de 3h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "\n",
    "# ====== 1. Carrega todos os tweets do arquivo ======\n",
    "with open('form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Transforma em DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Converte 'created_at' para datetime UTC e extrai hora ======\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "df['hora_utc'] = df['created_at'].dt.hour\n",
    "\n",
    "# ====== 3. Agrupa por blocos de 3 horas ======\n",
    "df['bloco_3h'] = df['hora_utc'].apply(lambda h: f\"{(h//3)*3:02d}hâ€“{(h//3)*3+2:02d}h\")\n",
    "frequencia = df['bloco_3h'].value_counts().sort_index()\n",
    "vals = frequencia.values\n",
    "\n",
    "# ====== 4. Identifica o bloco com maior nÃºmero de tweets ======\n",
    "bloco_mais_frequente = frequencia.idxmax()\n",
    "quantidade_maxima = frequencia.max()\n",
    "\n",
    "# ====== 5. EstatÃ­sticas ======\n",
    "mu = vals.mean() if len(vals)>0 else 0\n",
    "sigma = vals.std(ddof=0) if len(vals)>0 else 0\n",
    "median = np.median(vals) if len(vals)>0 else 0\n",
    "skewness = pd.Series(vals).skew() if len(vals)>0 else 0\n",
    "\n",
    "# ====== 6. Plot com mÃ©dia e banda Â±1Ïƒ ======\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(frequencia.index, vals, edgecolor='black', alpha=0.7, label='Tweets')\n",
    "plt.axhline(mu, linestyle='--', label=f'MÃ©dia = {mu:.2f}')\n",
    "if sigma > 0:\n",
    "    x = np.arange(len(vals))\n",
    "    plt.fill_between(x, mu-sigma, mu+sigma, alpha=0.2, label=f'Â±1Ïƒ = {sigma:.2f}')\n",
    "\n",
    "plt.title('FrequÃªncia de Tweets por Intervalo de 3 Horas (UTC) com EstatÃ­sticas')\n",
    "plt.xlabel('Intervalo de HorÃ¡rio (UTC)')\n",
    "plt.ylabel('Quantidade de Tweets')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== 7. Exibe estatÃ­sticas e bloco mais frequente no console ======\n",
    "print(f\"FrequÃªncia por bloco:\\n{frequencia.to_string()}\\n\")\n",
    "print(f\"Bloco mais frequente: {bloco_mais_frequente} com {quantidade_maxima} tweets\")\n",
    "print(f\"MÃ©dia = {mu:.2f}, Ïƒ = {sigma:.2f}, Mediana = {median:.2f}, Assimetria = {skewness:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56a672",
   "metadata": {},
   "source": [
    "### Dashbord individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd711c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "\n",
    "# ====== 1. ConfiguraÃ§Ãµes iniciais ======\n",
    "DATA_DIR = 'form_data'\n",
    "with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'r', encoding='utf-8') as f:\n",
    "    USER_ID = json.load(f)['last_user_id']\n",
    "USER_JSON = os.path.join(DATA_DIR, f'{USER_ID}.json')\n",
    "USER_IMAGE_PATH = os.path.join(DATA_DIR, f'{USER_ID}_selfie.png')\n",
    "TWEETS_JSON = os.path.join(DATA_DIR, 'tweets_User.json')\n",
    "\n",
    "# ====== 2. Carrega dados de usuÃ¡rio ======\n",
    "with open(USER_JSON, 'r', encoding='utf-8') as f:\n",
    "    user = json.load(f)\n",
    "\n",
    "# Define collabs e colecoes\n",
    "collabs = user.get('collabs', [])\n",
    "colecoes = user.get('colecoes', [])\n",
    "\n",
    "# ====== 3. Carrega e processa tweets ======\n",
    "df = pd.read_json(TWEETS_JSON)\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "# simula likes\n",
    "gen = random.Random(1000)\n",
    "df['likes'] = [gen.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Extrai blocos de 3h e identifica horÃ¡rio mais frequente ======\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "df['hora_utc'] = df['created_at'].dt.hour\n",
    "df['bloco_3h'] = df['hora_utc'].apply(lambda h: f\"{(h//3)*3:02d}hâ€“{(h//3)*3+2:02d}h\")\n",
    "frequencia = df['bloco_3h'].value_counts().sort_index()\n",
    "bloco_mais_frequente = frequencia.idxmax()\n",
    "quantidade_maxima = frequencia.max()\n",
    "\n",
    "# ====== 5. AnÃ¡lise de Sentimento ======\n",
    "top = df.sort_values('likes', ascending=False).head(300)\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    ")\n",
    "results = sentiment_analyzer(top['text'].tolist(), truncation=True)\n",
    "top['sentiment_score'] = [(int(r['label'][0]) - 1) / 4 for r in results]\n",
    "\n",
    "# converte em estrelas\n",
    "def score_to_star(score): return int(round(score * 4)) + 1\n",
    "top['stars'] = top['sentiment_score'].apply(score_to_star)\n",
    "\n",
    "# estatÃ­sticas de texto\n",
    "STOPWORDS = set(['a','o','as','os','e','Ã©','de','do','da','dos','das','em','no','na','nos','nas',\n",
    "                  'um','uma','uns','umas','para','por','com','sem','que','qui','on','the','and','is','in','to','of','it','you','for','this'])\n",
    "word_counts = Counter()\n",
    "for text in top['text']:\n",
    "    for w in text.lower().split():\n",
    "        w_clean = ''.join(ch for ch in w if ch.isalpha())\n",
    "        if w_clean and w_clean not in STOPWORDS:\n",
    "            word_counts[w_clean] += 1\n",
    "top_words = word_counts.most_common(10)\n",
    "\n",
    "# grÃ¡ficos\n",
    "fig_sentiment = px.bar(\n",
    "    x=top['stars'].value_counts().sort_index().index.astype(str) + 'â˜…',\n",
    "    y=top['stars'].value_counts().sort_index().values,\n",
    "    title='DistribuiÃ§Ã£o de Sentimento por Estrelas', labels={'x':'Estrelas','y':'Contagem'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "fig_words = px.bar(\n",
    "    x=[w for w,_ in top_words], y=[cnt for _,cnt in top_words],\n",
    "    title='Top 10 Palavras Mais Usadas', labels={'x':'Palavra','y':'FrequÃªncia'}, template='plotly_dark'\n",
    ")\n",
    "\n",
    "# frase mais positiva e pct fÃ£\n",
    "best_idx = top['sentiment_score'].idxmax()\n",
    "best_phrase = top.loc[best_idx,'text']\n",
    "fan_pct = (top['sentiment_score'] * top['likes']).sum() / top['likes'].sum() * 100\n",
    "fan_pct += 30  # aplica um bÃ´nus de afinidade com a FURIA\n",
    "fan_pct = min(fan_pct, 100)\n",
    "fan_emoji = 'ðŸ–¤' if fan_pct>75 else 'ðŸ˜Š' if fan_pct>60 else 'ðŸ˜' if fan_pct>45 else 'â˜¹ï¸' if fan_pct>25 else 'ðŸ˜­'\n",
    "\n",
    "# encode imagem\n",
    "encrypted = base64.b64encode(open(USER_IMAGE_PATH,'rb').read()).decode()\n",
    "IMAGE_SRC = f'data:image/png;base64,{encrypted}'\n",
    "\n",
    "# ====== 6. Monta App Dash ======\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div(style={'backgroundColor':'#121212','color':'#e0e0e0','fontFamily':'Inter, sans-serif','padding':'20px'}, children=[\n",
    "    html.H1('Dashboard Geral: FÃ£ da FURIA', style={'textAlign':'center','color':'#fff','fontFamily':'Georgia, serif'}),\n",
    "    html.Div(style={'display':'flex','gap':'20px'}, children=[\n",
    "        html.Div(style={'flex':'2','backgroundColor':'#1e1e1e','padding':'20px','borderRadius':'10px','boxShadow':'0 4px 15px rgba(0,0,0,0.2)'}, children=[\n",
    "            html.Img(src=IMAGE_SRC, style={'width':'120px','borderRadius':'8px','marginBottom':'15px','display':'block','margin':'0 auto'}),\n",
    "            html.Blockquote(best_phrase, style={'borderLeft':'4px solid #4caf50','padding':'10px 15px','backgroundColor':'#2a2a2a','fontStyle':'italic','fontFamily':'Inter, sans-serif'}),\n",
    "            html.P(['Nome: ', html.Span(user['nome'], style={'color':'#4caf50' if user.get('name_match') else '#888'})]),\n",
    "            html.P(['Porcentagem de fÃ£: ', f\"{fan_pct:.1f}% {fan_emoji}\"]),\n",
    "            # novo: bloco mais frequente\n",
    "            html.P([ 'HorÃ¡rio de maior atividade: ', html.Span(bloco_mais_frequente, style={'color':'#4caf50'}), f' ({quantidade_maxima} tweets)' ])\n",
    "        ]), \n",
    "        html.Div(style={'flex':'1','backgroundColor':'#1e1e1e','padding':'20px','borderRadius':'10px','boxShadow':'0 4px 15px rgba(0,0,0,0.2)'}, children=[\n",
    "            html.H3('Interesses e Atividades', style={'fontFamily':'Georgia, serif'}),\n",
    "            html.P('Jogos Favoritos: '+', '.join(user.get('jogos_furia',[]))),\n",
    "            html.P('Produtos Comprados: '+', '.join(user.get('produtos_furia',[]))),\n",
    "            html.P('Collabs: '+(', '.join(collabs) if collabs else 'Nenhum')),\n",
    "            html.P('ColeÃ§Ãµes: '+(', '.join(colecoes) if colecoes else 'Nenhuma'))\n",
    "        ])\n",
    "    ]),\n",
    "    html.Div(style={'display':'flex','gap':'20px','marginTop':'30px'}, children=[\n",
    "        html.Div(dcc.Graph(figure=fig_words), style={'flex':'1','backgroundColor':'#1e1e1e','padding':'10px','borderRadius':'8px'}),\n",
    "        html.Div(dcc.Graph(figure=fig_sentiment), style={'flex':'1','backgroundColor':'#1e1e1e','padding':'10px','borderRadius':'8px'})\n",
    "    ])\n",
    "])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
