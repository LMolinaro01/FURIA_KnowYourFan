{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbc009a6d1ea09",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h1> FURIA Know Your Fan (Individual)</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a6930",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ac0276a6352a1",
   "metadata": {},
   "source": [
    "#### Importação Chave API (Arquivo .env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba830f86466a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T13:31:15.566690Z",
     "start_time": "2025-04-30T13:31:15.391312Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from oauthlib.oauth2 import BearerToken\n",
    "\n",
    "# Carregar variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "chave_api = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9843e",
   "metadata": {},
   "source": [
    "### Coleta e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2778e57",
   "metadata": {},
   "source": [
    "#### Coleta de Dados (Formulário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dca973f7b28f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import webbrowser\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import hashlib\n",
    "import threading\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "PORT = 8080\n",
    "DATA_DIR = \"form_data\"\n",
    "KEY_FILE = \"rg_encryption.key\"\n",
    "\n",
    "# Carrega ou gera a chave de criptografia\n",
    "if not os.path.exists(KEY_FILE):\n",
    "    key = Fernet.generate_key()\n",
    "    with open(KEY_FILE, 'wb') as kf:\n",
    "        kf.write(key)\n",
    "else:\n",
    "    with open(KEY_FILE, 'rb') as kf:\n",
    "        key = kf.read()\n",
    "fernet = Fernet(key)\n",
    "\n",
    "class MyHandler(http.server.SimpleHTTPRequestHandler):\n",
    "    def do_POST(self):\n",
    "        # Só processa a rota '/submit'\n",
    "        if self.path != '/submit':\n",
    "            return super().do_GET()\n",
    "\n",
    "        # Lê o corpo da requisição como JSON\n",
    "        content_length = int(self.headers.get('Content-Length', 0))\n",
    "        raw_body = self.rfile.read(content_length).decode('utf-8')\n",
    "        try:\n",
    "            dados = json.loads(raw_body)\n",
    "        except json.JSONDecodeError:\n",
    "            self.send_error(400, \"Bad Request: JSON inválido\")\n",
    "            return\n",
    "\n",
    "        # Hash do CPF\n",
    "        cpf_original = dados.get('cpf')\n",
    "        if cpf_original:\n",
    "            dados['cpf'] = hashlib.sha256(cpf_original.encode('utf-8')).hexdigest()\n",
    "        else:\n",
    "            print(\"Aviso: 'cpf' não enviado.\")\n",
    "\n",
    "        # Extrai imagens em Base64 do JSON\n",
    "        rg_base64     = dados.pop('rgImagem_base64', None)\n",
    "        selfie_base64 = dados.pop('selfieImagem_base64', None)\n",
    "\n",
    "        # Adiciona timestamp de submissão\n",
    "        dados['submitted_at'] = datetime.utcnow().isoformat() + 'Z'\n",
    "\n",
    "        # Gera ID único e prepara diretório\n",
    "        user_id = str(uuid.uuid4())\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "        # Criptografa e salva RG como arquivo .enc\n",
    "        if rg_base64:\n",
    "            try:\n",
    "                rg_bytes = base64.b64decode(rg_base64)\n",
    "                encrypted = fernet.encrypt(rg_bytes)\n",
    "                enc_filename = f\"{user_id}_rg.enc\"\n",
    "                enc_path = os.path.join(DATA_DIR, enc_filename)\n",
    "                with open(enc_path, 'wb') as ef:\n",
    "                    ef.write(encrypted)\n",
    "                dados['rgImagem_encrypted'] = enc_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao criptografar/salvar RG: {e}\")\n",
    "\n",
    "        # Salva selfie como PNG (ou também criptografe se desejar)\n",
    "        if selfie_base64:\n",
    "            try:\n",
    "                selfie_bytes = base64.b64decode(selfie_base64)\n",
    "                selfie_filename = f\"{user_id}_selfie.png\"\n",
    "                selfie_path = os.path.join(DATA_DIR, selfie_filename)\n",
    "                with open(selfie_path, 'wb') as imgf:\n",
    "                    imgf.write(selfie_bytes)\n",
    "                dados['selfieImagem_file'] = selfie_filename\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao salvar Selfie: {e}\")\n",
    "\n",
    "        # Grava JSON de metadados de forma atômica\n",
    "        file_path = os.path.join(DATA_DIR, f\"{user_id}.json\")\n",
    "        temp_path = file_path + \".tmp\"\n",
    "        try:\n",
    "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dados, f, indent=4, ensure_ascii=False)\n",
    "            os.replace(temp_path, file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar JSON: {e}\")\n",
    "            self.send_error(500, \"Internal Server Error: falha ao salvar dados\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            user_id_info = {'last_user_id': user_id}\n",
    "            with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'w', encoding='utf-8') as f:\n",
    "                json.dump(user_id_info, f, indent=4, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar last_user_id.json: {e}\")\n",
    "\n",
    "        # Envia resposta de sucesso\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'message': 'Dados eviados e criptografados com sucesso!',\n",
    "            'user_id': user_id\n",
    "        }\n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-Type', 'application/json')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(response).encode('utf-8'))\n",
    "\n",
    "        # Desliga o servidor após 4 segundos\n",
    "        threading.Timer(4.0, self.server.shutdown).start()\n",
    "\n",
    "\n",
    "def run_server():\n",
    "    with socketserver.TCPServer((\"\", PORT), MyHandler) as httpd:\n",
    "        print(f\"Servidor rodando em http://localhost:{PORT}\")\n",
    "        webbrowser.open(f\"http://localhost:{PORT}/Form/form.html\")\n",
    "        httpd.serve_forever()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6996e942e24f04",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Validação de Identidade com Abordagem de IA</h3> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed12b2bdcdbcf6",
   "metadata": {},
   "source": [
    "##### Instale as bibliotecas necessárias\n",
    "\n",
    "- pytesseract (requer Tesseract OCR engine instalado separadamente)\n",
    " \n",
    "- Pillow (dependência do pytesseract)\n",
    " \n",
    "- face_recognition (requer dlib, pode ser complexo instalar em alguns sistemas)\n",
    "\n",
    "- ipywidgets (para o widget de upload)\n",
    "\n",
    "<br>\n",
    "\n",
    "```pip install pytesseract Pillow face_recognition ipywidgets ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e06c15ea2efa4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h4>OCR, Extração e Comparação de Dados Textuais</h4> </div>\n",
    "\n",
    "\n",
    "Definimos uma função para realizar o OCR na imagem do documento completo, extrair o texto, tentar encontrar Nome e CPF e compará-los com dados fornecidos (vamos simular esses dados fornecidos para o exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tesseract pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6bb22",
   "metadata": {},
   "source": [
    "#### Validação Nome e Naturalidade (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82efa4ee6dc53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# ————————— Configurações Tesseract —————————\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "TESS_CONF_FULL   = \"--oem 3 --psm 3\"\n",
    "TESS_CONF_DIGITS = \"--oem 3 --psm 3 -c tessedit_char_whitelist=0123456789\"\n",
    "TESS_CONF_MRZ    = \"--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ<\"\n",
    "\n",
    "DATA_DIR = \"form_data\"\n",
    "KEY_FILE = \"rg_encryption.key\"\n",
    "\n",
    "# ————————— Carrega chave e dados do JSON mais recente válido —————————\n",
    "fernet = Fernet(open(KEY_FILE, \"rb\").read())\n",
    "\n",
    "# Lista e filtra os JSONs válidos (com estrutura de dicionário)\n",
    "json_files = sorted(\n",
    "    [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\") and f != 'last_user_id.json'],\n",
    "    key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "valid_json_files = []\n",
    "for f in json_files:\n",
    "    path = os.path.join(DATA_DIR, f)\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as j:\n",
    "            data = json.load(j)\n",
    "            if isinstance(data, dict):\n",
    "                valid_json_files.append(f)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not valid_json_files:\n",
    "    raise FileNotFoundError(\"Nenhum JSON válido (tipo dicionário) encontrado em 'form_data'.\")\n",
    "\n",
    "latest_json = valid_json_files[0]\n",
    "user_id = os.path.splitext(latest_json)[0]\n",
    "form_data = json.load(open(os.path.join(DATA_DIR, latest_json), encoding=\"utf-8\"))\n",
    "provided_name = form_data.get(\"nome\", \"\")\n",
    "cpf_hash = form_data.get(\"cpf\", \"\")\n",
    "\n",
    "# ————————— Decrypt + carrega imagem em alta resolução (×2) —————————\n",
    "encrypted_fn = form_data.get(\"rgImagem_encrypted\")\n",
    "if encrypted_fn:\n",
    "    enc_path = os.path.join(DATA_DIR, encrypted_fn)\n",
    "    try:\n",
    "        raw_bytes = fernet.decrypt(open(enc_path, \"rb\").read())\n",
    "        pil = Image.open(io.BytesIO(raw_bytes)).convert(\"RGB\")\n",
    "        pil = pil.resize((pil.width * 2, pil.height * 2), Image.LANCZOS)\n",
    "        img = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao descriptografar '{encrypted_fn}': {e}\")\n",
    "        img = None\n",
    "else:\n",
    "    print(\"Aviso: 'rgImagem_encrypted' not found in form_data\")\n",
    "    img = None\n",
    "\n",
    "# ————————— Pré-processamento e OCR —————————\n",
    "if img is not None:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(gray)\n",
    "    _, prep = cv2.threshold(cl, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    full_ocr_text = pytesseract.image_to_string(prep, config=TESS_CONF_FULL, lang=\"por+eng\")\n",
    "else:\n",
    "    full_ocr_text = \"\"\n",
    "    print(\"Skipping OCR: no image available.\")\n",
    "\n",
    "# ————————— Função de validação de CPF —————————\n",
    "def valida_cpf(c: str) -> bool:\n",
    "    if len(c) != 11 or c == c[0]*11:\n",
    "        return False\n",
    "    s1 = sum(int(c[i])*(10-i) for i in range(9))\n",
    "    d1 = ((s1*10) % 11) % 10\n",
    "    s2 = sum(int(c[i])*(11-i) for i in range(10))\n",
    "    d2 = ((s2*10) % 11) % 10\n",
    "    return c[-2:] == f\"{d1}{d2}\"\n",
    "\n",
    "# ————————— Extrai Nome via MRZ —————————\n",
    "name = None\n",
    "name_ocr_raw = None\n",
    "mrz_lines = [L.strip() for L in full_ocr_text.splitlines() if \"<<\" in L]\n",
    "if mrz_lines:\n",
    "    mrz = mrz_lines[-1]\n",
    "    parts = mrz.split(\"<<\")\n",
    "    if len(parts) >= 2:\n",
    "        surname = parts[0].title()\n",
    "        given_list = [p for p in parts[1].split(\"<\") if p]\n",
    "        given_reversed = list(reversed(given_list))\n",
    "        name = \" \".join(p.title() for p in given_reversed + [surname])\n",
    "        name_ocr_raw = mrz\n",
    "\n",
    "# Normalize and compare\n",
    "def normalize(n: str) -> str:\n",
    "    t = re.sub(r'\\b(?:de|da|dos|das|do)\\b','', n, flags=re.IGNORECASE)\n",
    "    return re.sub(r'\\s+','', t).lower()\n",
    "\n",
    "name_match = bool(name and normalize(name) == normalize(provided_name))\n",
    "if name_match:\n",
    "    name = provided_name\n",
    "\n",
    "# ————————— Extração da Naturalidade —————————\n",
    "naturalidade = None\n",
    "naturalidade_match = False\n",
    "m = re.search(\n",
    "    r'(?:Naturalidade|Natural de|Natural)\\s*[:\\-]?\\s*([\\wÀ-ú\\s]+)[-/]?\\s*([A-Za-z]{2})',\n",
    "    full_ocr_text,\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "if m:\n",
    "    cidade = m.group(1).strip().title()\n",
    "    estado = m.group(2).strip().upper()\n",
    "    naturalidade = f\"{cidade} - {estado}\"\n",
    "else:\n",
    "    f2 = re.search(r'ESTADO DE\\s+([\\wÀ-ú]+)\\s+([A-Za-z]{2})', full_ocr_text, flags=re.IGNORECASE)\n",
    "    if f2:\n",
    "        cidade = f2.group(1).strip().title()\n",
    "        estado = f2.group(2).strip().upper()\n",
    "        naturalidade = f\"{cidade} - {estado}\"\n",
    "\n",
    "expected_city = form_data.get(\"endereco\", \"\").strip().lower()\n",
    "if naturalidade:\n",
    "    naturalidade_match = (naturalidade.split(\" - \")[0].strip().lower() == expected_city)\n",
    "\n",
    "# ————————— Atualiza e salva o JSON —————————\n",
    "form_data.update({\n",
    "    \"naturalidade_extraida\": naturalidade,\n",
    "    \"naturalidade_match\": naturalidade_match,\n",
    "    \"name_match\": name_match\n",
    "})\n",
    "with open(os.path.join(DATA_DIR, latest_json), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(form_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ————————— Exibe resultado —————————\n",
    "result = {\n",
    "    \"full_ocr_text\": full_ocr_text,\n",
    "    \"name\": name,\n",
    "    \"name_ocr_raw\": name_ocr_raw,\n",
    "    \"name_match\": name_match,\n",
    "    \"naturalidade\": naturalidade,\n",
    "    \"naturalidade_match\": naturalidade_match\n",
    "}\n",
    "print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cb67ef57bd894",
   "metadata": {},
   "source": [
    "#### Reconhecimento e Comparação Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351981ce",
   "metadata": {},
   "source": [
    "Ao baixar o tesseract, lembrar de baixar a lingua portuguesa e adicionar na pasta -> C:\\Program Files\\Tesseract-OCR\\tessdata\n",
    "\n",
    "\"por.traineddata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = Path(\"form_data\")\n",
    "KEY_FILE = Path(\"rg_encryption.key\")\n",
    "CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "FACE_SIZE = (150, 150)\n",
    "DISTANCE_THRESHOLD = 60.0  # adjustable threshold for face match\n",
    "\n",
    "def load_fernet_key(key_path: Path) -> Fernet:\n",
    "    \"\"\"\n",
    "    Load Fernet encryption key from file.\n",
    "    \"\"\"\n",
    "    with key_path.open(\"rb\") as f:\n",
    "        key = f.read()\n",
    "    return Fernet(key)\n",
    "\n",
    "\n",
    "def load_latest_form(data_dir: Path) -> tuple[Dict[str, Any], Path]:\n",
    "    \"\"\"\n",
    "    Load the most recent JSON form from data_dir, returning the data and its file path.\n",
    "    \"\"\"\n",
    "    json_files = sorted(data_dir.glob(\"*.json\"), key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {data_dir}\")\n",
    "    latest = json_files[0]\n",
    "    with latest.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # retornamos também o caminho do arquivo JSON para permitir regravação\n",
    "    return data, latest\n",
    "\n",
    "\n",
    "def decrypt_image(encrypted_filename: str, fernet: Fernet, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Decrypt an image file and return a PIL Image.\n",
    "    \"\"\"\n",
    "    encrypted_path = data_dir / encrypted_filename\n",
    "    if not encrypted_path.exists():\n",
    "        raise FileNotFoundError(f\"Encrypted file not found: {encrypted_path}\")\n",
    "    data = fernet.decrypt(encrypted_path.read_bytes())\n",
    "    return Image.open(io.BytesIO(data))\n",
    "\n",
    "\n",
    "def load_plain_image(filename: str, data_dir: Path) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load a non-encrypted image from disk as PIL Image.\n",
    "    \"\"\"\n",
    "    img_path = data_dir / filename\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "    return Image.open(img_path)\n",
    "\n",
    "\n",
    "def pil_to_bgr(img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to OpenCV BGR numpy array.\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def detect_face(image_bgr: np.ndarray, cascade: cv2.CascadeClassifier) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect the first face in a BGR image, crop and resize it.\n",
    "    Returns the grayscale face ROI or None.\n",
    "    \"\"\"\n",
    "    faces = cascade.detectMultiScale(image_bgr, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = faces[0]\n",
    "    face = image_bgr[y:y+h, x:x+w]\n",
    "    face_resized = cv2.resize(face, FACE_SIZE)\n",
    "    return cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def compare_faces(gray1: np.ndarray, gray2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean absolute difference between two grayscale face images.\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    return float(np.mean(diff))\n",
    "\n",
    "\n",
    "def perform_face_comparison_opencv(\n",
    "    img1_pil: Image.Image, img2_pil: Image.Image, threshold: float = DISTANCE_THRESHOLD\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare two PIL images containing faces using OpenCV Haar cascades.\n",
    "    Returns a dict with match result, distance, and status.\n",
    "    \"\"\"\n",
    "    results: Dict[str, Any] = {\n",
    "        'face_match': False,\n",
    "        'distance': None,\n",
    "        'success': False,\n",
    "        'message': ''\n",
    "    }\n",
    "    try:\n",
    "        img1_bgr = pil_to_bgr(img1_pil)\n",
    "        img2_bgr = pil_to_bgr(img2_pil)\n",
    "\n",
    "        cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "        if cascade.empty():\n",
    "            raise RuntimeError(f\"Failed to load cascade classifier at {CASCADE_PATH}\")\n",
    "\n",
    "        face1 = detect_face(img1_bgr, cascade)\n",
    "        face2 = detect_face(img2_bgr, cascade)\n",
    "\n",
    "        if face1 is None or face2 is None:\n",
    "            results['message'] = \"No face detected in one or both images.\"\n",
    "            return results\n",
    "\n",
    "        distance = compare_faces(face1, face2)\n",
    "        results['distance'] = float(distance)\n",
    "        results['face_match'] = bool(distance < threshold)\n",
    "        results['success'] = True\n",
    "        results['message'] = \"Comparison successful with OpenCV.\"\n",
    "    except Exception as e:\n",
    "        results['message'] = f\"Error during face comparison: {e}\"\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    fernet = load_fernet_key(KEY_FILE)\n",
    "    form_data, form_path = load_latest_form(DATA_DIR)\n",
    "\n",
    "    # Decrypt RG image\n",
    "    encrypted_rg = form_data.get(\"rgImagem_encrypted\")\n",
    "    img_doc = decrypt_image(encrypted_rg, fernet, DATA_DIR)\n",
    "\n",
    "    # Load selfie (not encrypted)\n",
    "    selfie_filename = form_data.get(\"selfieImagem_file\")\n",
    "    img_selfie = load_plain_image(selfie_filename, DATA_DIR)\n",
    "\n",
    "    # Run comparison\n",
    "    result = perform_face_comparison_opencv(img_doc, img_selfie)\n",
    "\n",
    "    # Add result index to form data\n",
    "    form_data['rosto_correto_indice'] = 1 if result.get('face_match') else 0\n",
    "    form_data['face_match'] = result.get('face_match')\n",
    "    form_data['face_distance'] = result.get('distance')\n",
    "\n",
    "    # Save updated JSON back to file\n",
    "    with form_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(form_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Print JSON-safe result\n",
    "    print(json.dumps(result, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e96c7714096a8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <h3>Integração com Redes Sociais (Simulada)</h3> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa7006b7b774eb",
   "metadata": {},
   "source": [
    "#### Twitter (usuário individual) - Chamada API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc903631095407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T14:00:59.340974Z",
     "start_time": "2025-04-30T14:00:58.696618Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.idea/.env')  # ex: Path('config/.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "chave_api = os.getenv(\"twitter_api\")\n",
    "\n",
    "# ————————— Configurações e autenticação —————————\n",
    "BEARER_TOKEN = chave_api  # Substitua pelo seu Bearer Token\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "# ————————— Carrega JSON com dados do usuário —————————\n",
    "DATA_DIR = 'form_data'\n",
    "json_files = sorted([\n",
    "    f for f in os.listdir(DATA_DIR) if f.endswith('.json')\n",
    "], key=lambda fn: os.path.getmtime(os.path.join(DATA_DIR, fn)), reverse=True)\n",
    "json_path = os.path.join(DATA_DIR, json_files[0])\n",
    "form_data = json.load(open(json_path, encoding='utf-8'))\n",
    "\n",
    "# Extrai o username (sem o @) que está salvo no JSON, ex: \"@teste\" ou \"teste\"\n",
    "raw_handle = form_data.get('twitter', '').strip()\n",
    "twitter_handle = raw_handle.lstrip('@')\n",
    "if not twitter_handle:\n",
    "    raise ValueError('Nenhum handle de Twitter encontrado no JSON')\n",
    "\n",
    "# ————————— Termos específicos a buscar —————————\n",
    "TERMS = [\n",
    "    'Fallen', 'KSCERATO', 'yuurih', 'molodoy', 'skullz', 'chelo',\n",
    "    'fNb', 'Goot', 'Envy', 'Trigo', 'RedBert', 'Fntzy', 'R4re',\n",
    "    'Handyy', 'KDS', 'yanxnz', 'Lostt', 'nzr', 'Khalil', 'havoc',\n",
    "    'xand', 'mwzera', 'Xeratricky', 'Pandxrz', 'HisWattson',\n",
    "    '#FURIACS', '#FURIAR6', '#FURIAFC', '#DIADEFURIA'\n",
    "]\n",
    "\n",
    "# Monta query para buscar tweets do usuário contendo qualquer termo\n",
    "query_terms = ' OR '.join(f'\"{t}\"' for t in TERMS)\n",
    "query = f'from:{twitter_handle} ({query_terms}) -is:retweet lang:pt'\n",
    "\n",
    "# ————————— Busca tweets —————————\n",
    "max_results = 15  # até 100\n",
    "response = client.search_recent_tweets(\n",
    "    query=query,\n",
    "    max_results=max_results,\n",
    "    tweet_fields=['id', 'text', 'created_at', 'lang', 'source'],\n",
    "    expansions=['author_id'],\n",
    "    user_fields=['username', 'name', 'description', 'location']\n",
    ")\n",
    "\n",
    "# ————————— Monta resultados para salvar —————————\n",
    "tweets_data = []\n",
    "if response.data:\n",
    "    users_index = {u['id']: u for u in response.includes.get('users', [])}\n",
    "    for tweet in response.data:\n",
    "        info = {\n",
    "            'tweet_id': tweet.id,\n",
    "            'text': tweet.text,\n",
    "            'created_at': str(tweet.created_at),\n",
    "            'lang': tweet.lang,\n",
    "            'source': tweet.source,\n",
    "            'author': {\n",
    "                'id': tweet.author_id,\n",
    "                'username': users_index[tweet.author_id].username,\n",
    "                'name': users_index[tweet.author_id].name,\n",
    "            }\n",
    "        }\n",
    "        tweets_data.append(info)\n",
    "\n",
    "# ————————— Salva em JSON —————————\n",
    "output_path = os.path.join(DATA_DIR, f'tweets_User.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tweets_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Salvos {len(tweets_data)} tweets em {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67b27c",
   "metadata": {},
   "source": [
    "### Análise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0944",
   "metadata": {},
   "source": [
    "#### Análise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ====== 1. Carrega os dados JSON ======\n",
    "with open(r'form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Elimina duplicatas baseado no texto ======\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# ====== 3. Adiciona likes aleatórios para simulação ======\n",
    "df['likes'] = [random.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Filtra os 200 melhores por likes ======\n",
    "top200 = df.sort_values('likes', ascending=False).head(200).copy()\n",
    "\n",
    "# ====== 5. Remove palavras banidas ======\n",
    "palavras_banidas = ['CAPIM', 'Desempedidos', 'G3X', 'g3x', 'DENDELE', 'LOUD', 'FUNKBOL', 'FLUXO REAL ELITE']\n",
    "top200_filtrado = top200[~top200['text'].str.upper().str.contains('|'.join(palavras_banidas))]\n",
    "\n",
    "# ====== 6. Cria pipeline de análise de sentimento ======\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# ====== 7. Aplica o modelo de análise de sentimento ======\n",
    "batch_size = 32\n",
    "top200_filtrado = top200_filtrado.reset_index(drop=True)\n",
    "texts = top200_filtrado['text'].tolist()\n",
    "results = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    results.extend(sentiment_analyzer(batch, truncation=True))\n",
    "\n",
    "# ====== 8. Normaliza os scores de sentimento ======\n",
    "scores = [(int(res['label'][0]) - 1) / 4 for res in results]\n",
    "top200_filtrado['sentiment_score'] = scores\n",
    "\n",
    "# ====== 9. Adiciona um campo \"channel_name\" ======\n",
    "top200_filtrado['channel_name'] = top200_filtrado['user'].apply(lambda a: a['username'])\n",
    "\n",
    "# ====== 10. Pega os top 10 comentários por canal ======\n",
    "top_comentarios_canal = []\n",
    "for canal, grupo in top200_filtrado.groupby('channel_name'):\n",
    "    top_comentarios_canal.append(grupo.sort_values('sentiment_score', ascending=False).head(10))\n",
    "top_comentarios_canal = pd.concat(top_comentarios_canal).reset_index(drop=True)\n",
    "\n",
    "# ====== 11. Funções auxiliares ======\n",
    "def simplificar_comentario(texto, limite=250):\n",
    "    if len(texto) <= limite:\n",
    "        return texto\n",
    "    palavras = texto.split()\n",
    "    return f\"{texto[:limite].rstrip()}... {' '.join(palavras[-2:])}\"\n",
    "\n",
    "\n",
    "def estrela_para_sentimento(score):\n",
    "    \"\"\"\n",
    "    Converte score normalizado (0-1) para sentimento textual em 1-5 estrelas.\n",
    "    \"\"\"\n",
    "    stars = int(round(score * 4)) + 1\n",
    "    if stars == 1:\n",
    "        return \"muito negativo\"\n",
    "    elif stars == 2:\n",
    "        return \"negativo\"\n",
    "    elif stars == 3:\n",
    "        return \"neutro\"\n",
    "    elif stars == 4:\n",
    "        return \"positivo\"\n",
    "    else:\n",
    "        return \"muito positivo\"\n",
    "\n",
    "# ====== 12. Função para gerar gráfico com sentimento ======\n",
    "def plot_comentarios_canal(df, canal):\n",
    "    comentarios = [textwrap.fill(simplificar_comentario(txt), width=50) for txt in df['text']]\n",
    "    sentiment_scores = df['sentiment_score']\n",
    "    \n",
    "    spacing = 1.5\n",
    "    y_positions = [i * spacing for i in range(len(comentarios))]\n",
    "\n",
    "    plt.figure(figsize=(12, len(df) * 1.5))\n",
    "    plt.barh(y_positions, sentiment_scores, color='skyblue')  # azul claro\n",
    "    plt.yticks(y_positions, comentarios)\n",
    "    plt.xlabel('Score de Sentimento (0 - Negativo, 1 - Positivo)')\n",
    "    plt.title(f'Comentários e Sentimento - @{canal}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Exibe o score de sentimento e texto do sentimento ao lado de cada barra\n",
    "    for y, score in zip(y_positions, sentiment_scores):\n",
    "        sentimento = estrela_para_sentimento(score)\n",
    "        plt.text(score + 0.01, y, f'{score:.2f} ({sentimento})', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ====== 13. Gera o gráfico para cada canal ======\n",
    "for canal, grupo in top_comentarios_canal.groupby('channel_name'):\n",
    "    plot_comentarios_canal(grupo, canal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b47022",
   "metadata": {},
   "source": [
    "#### Frequência de Tweets (Intervalo de 3h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "\n",
    "# ====== 1. Carrega todos os tweets do arquivo ======\n",
    "with open('form_data/tweets_User.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Transforma em DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ====== 2. Converte 'created_at' para datetime UTC e extrai hora ======\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "df['hora_utc'] = df['created_at'].dt.hour\n",
    "\n",
    "# ====== 3. Agrupa por blocos de 3 horas ======\n",
    "df['bloco_3h'] = df['hora_utc'].apply(lambda h: f\"{(h//3)*3:02d}h–{(h//3)*3+2:02d}h\")\n",
    "frequencia = df['bloco_3h'].value_counts().sort_index()\n",
    "vals = frequencia.values\n",
    "\n",
    "# ====== 4. Identifica o bloco com maior número de tweets ======\n",
    "bloco_mais_frequente = frequencia.idxmax()\n",
    "quantidade_maxima = frequencia.max()\n",
    "\n",
    "# ====== 5. Estatísticas ======\n",
    "mu = vals.mean() if len(vals)>0 else 0\n",
    "sigma = vals.std(ddof=0) if len(vals)>0 else 0\n",
    "median = np.median(vals) if len(vals)>0 else 0\n",
    "skewness = pd.Series(vals).skew() if len(vals)>0 else 0\n",
    "\n",
    "# ====== 6. Plot com média e banda ±1σ ======\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(frequencia.index, vals, edgecolor='black', alpha=0.7, label='Tweets')\n",
    "plt.axhline(mu, linestyle='--', label=f'Média = {mu:.2f}')\n",
    "if sigma > 0:\n",
    "    x = np.arange(len(vals))\n",
    "    plt.fill_between(x, mu-sigma, mu+sigma, alpha=0.2, label=f'±1σ = {sigma:.2f}')\n",
    "\n",
    "plt.title('Frequência de Tweets por Intervalo de 3 Horas (UTC) com Estatísticas')\n",
    "plt.xlabel('Intervalo de Horário (UTC)')\n",
    "plt.ylabel('Quantidade de Tweets')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== 7. Exibe estatísticas e bloco mais frequente no console ======\n",
    "print(f\"Frequência por bloco:\\n{frequencia.to_string()}\\n\")\n",
    "print(f\"Bloco mais frequente: {bloco_mais_frequente} com {quantidade_maxima} tweets\")\n",
    "print(f\"Média = {mu:.2f}, σ = {sigma:.2f}, Mediana = {median:.2f}, Assimetria = {skewness:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56a672",
   "metadata": {},
   "source": [
    "### Dashbord individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd711c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "\n",
    "# ====== 1. Configurações iniciais ======\n",
    "DATA_DIR = 'form_data'\n",
    "with open(os.path.join(DATA_DIR, 'last_user_id.json'), 'r', encoding='utf-8') as f:\n",
    "    USER_ID = json.load(f)['last_user_id']\n",
    "USER_JSON = os.path.join(DATA_DIR, f'{USER_ID}.json')\n",
    "USER_IMAGE_PATH = os.path.join(DATA_DIR, f'{USER_ID}_selfie.png')\n",
    "TWEETS_JSON = os.path.join(DATA_DIR, 'tweets_User.json')\n",
    "\n",
    "# ====== 2. Carrega dados de usuário ======\n",
    "with open(USER_JSON, 'r', encoding='utf-8') as f:\n",
    "    user = json.load(f)\n",
    "\n",
    "# Define collabs e colecoes\n",
    "collabs = user.get('collabs', [])\n",
    "colecoes = user.get('colecoes', [])\n",
    "\n",
    "# ====== 3. Carrega e processa tweets ======\n",
    "df = pd.read_json(TWEETS_JSON)\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "# simula likes\n",
    "gen = random.Random(1000)\n",
    "df['likes'] = [gen.randint(1, 5000) for _ in range(len(df))]\n",
    "\n",
    "# ====== 4. Extrai blocos de 3h e identifica horário mais frequente ======\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "df['hora_utc'] = df['created_at'].dt.hour\n",
    "df['bloco_3h'] = df['hora_utc'].apply(lambda h: f\"{(h//3)*3:02d}h–{(h//3)*3+2:02d}h\")\n",
    "frequencia = df['bloco_3h'].value_counts().sort_index()\n",
    "bloco_mais_frequente = frequencia.idxmax()\n",
    "quantidade_maxima = frequencia.max()\n",
    "\n",
    "# ====== 5. Análise de Sentimento ======\n",
    "top = df.sort_values('likes', ascending=False).head(300)\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    ")\n",
    "results = sentiment_analyzer(top['text'].tolist(), truncation=True)\n",
    "top['sentiment_score'] = [(int(r['label'][0]) - 1) / 4 for r in results]\n",
    "\n",
    "# converte em estrelas\n",
    "def score_to_star(score): return int(round(score * 4)) + 1\n",
    "top['stars'] = top['sentiment_score'].apply(score_to_star)\n",
    "\n",
    "# estatísticas de texto\n",
    "STOPWORDS = set(['a','o','as','os','e','é','de','do','da','dos','das','em','no','na','nos','nas',\n",
    "                  'um','uma','uns','umas','para','por','com','sem','que','qui','on','the','and','is','in','to','of','it','you','for','this'])\n",
    "word_counts = Counter()\n",
    "for text in top['text']:\n",
    "    for w in text.lower().split():\n",
    "        w_clean = ''.join(ch for ch in w if ch.isalpha())\n",
    "        if w_clean and w_clean not in STOPWORDS:\n",
    "            word_counts[w_clean] += 1\n",
    "top_words = word_counts.most_common(10)\n",
    "\n",
    "# gráficos\n",
    "fig_sentiment = px.bar(\n",
    "    x=top['stars'].value_counts().sort_index().index.astype(str) + '★',\n",
    "    y=top['stars'].value_counts().sort_index().values,\n",
    "    title='Distribuição de Sentimento por Estrelas', labels={'x':'Estrelas','y':'Contagem'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "fig_words = px.bar(\n",
    "    x=[w for w,_ in top_words], y=[cnt for _,cnt in top_words],\n",
    "    title='Top 10 Palavras Mais Usadas', labels={'x':'Palavra','y':'Frequência'}, template='plotly_dark'\n",
    ")\n",
    "\n",
    "# frase mais positiva e pct fã\n",
    "best_idx = top['sentiment_score'].idxmax()\n",
    "best_phrase = top.loc[best_idx,'text']\n",
    "fan_pct = (top['sentiment_score'] * top['likes']).sum() / top['likes'].sum() * 100\n",
    "fan_pct += 30  # aplica um bônus de afinidade com a FURIA\n",
    "fan_pct = min(fan_pct, 100)\n",
    "fan_emoji = '🖤' if fan_pct>75 else '😊' if fan_pct>60 else '😐' if fan_pct>45 else '☹️' if fan_pct>25 else '😭'\n",
    "\n",
    "# encode imagem\n",
    "encrypted = base64.b64encode(open(USER_IMAGE_PATH,'rb').read()).decode()\n",
    "IMAGE_SRC = f'data:image/png;base64,{encrypted}'\n",
    "\n",
    "# ====== 6. Monta App Dash ======\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div(style={'backgroundColor':'#121212','color':'#e0e0e0','fontFamily':'Inter, sans-serif','padding':'20px'}, children=[\n",
    "    html.H1('Dashboard Geral: Fã da FURIA', style={'textAlign':'center','color':'#fff','fontFamily':'Georgia, serif'}),\n",
    "    html.Div(style={'display':'flex','gap':'20px'}, children=[\n",
    "        html.Div(style={'flex':'2','backgroundColor':'#1e1e1e','padding':'20px','borderRadius':'10px','boxShadow':'0 4px 15px rgba(0,0,0,0.2)'}, children=[\n",
    "            html.Img(src=IMAGE_SRC, style={'width':'120px','borderRadius':'8px','marginBottom':'15px','display':'block','margin':'0 auto'}),\n",
    "            html.Blockquote(best_phrase, style={'borderLeft':'4px solid #4caf50','padding':'10px 15px','backgroundColor':'#2a2a2a','fontStyle':'italic','fontFamily':'Inter, sans-serif'}),\n",
    "            html.P(['Nome: ', html.Span(user['nome'], style={'color':'#4caf50' if user.get('name_match') else '#888'})]),\n",
    "            html.P(['Porcentagem de fã: ', f\"{fan_pct:.1f}% {fan_emoji}\"]),\n",
    "            # novo: bloco mais frequente\n",
    "            html.P([ 'Horário de maior atividade: ', html.Span(bloco_mais_frequente, style={'color':'#4caf50'}), f' ({quantidade_maxima} tweets)' ])\n",
    "        ]), \n",
    "        html.Div(style={'flex':'1','backgroundColor':'#1e1e1e','padding':'20px','borderRadius':'10px','boxShadow':'0 4px 15px rgba(0,0,0,0.2)'}, children=[\n",
    "            html.H3('Interesses e Atividades', style={'fontFamily':'Georgia, serif'}),\n",
    "            html.P('Jogos Favoritos: '+', '.join(user.get('jogos_furia',[]))),\n",
    "            html.P('Produtos Comprados: '+', '.join(user.get('produtos_furia',[]))),\n",
    "            html.P('Collabs: '+(', '.join(collabs) if collabs else 'Nenhum')),\n",
    "            html.P('Coleções: '+(', '.join(colecoes) if colecoes else 'Nenhuma'))\n",
    "        ])\n",
    "    ]),\n",
    "    html.Div(style={'display':'flex','gap':'20px','marginTop':'30px'}, children=[\n",
    "        html.Div(dcc.Graph(figure=fig_words), style={'flex':'1','backgroundColor':'#1e1e1e','padding':'10px','borderRadius':'8px'}),\n",
    "        html.Div(dcc.Graph(figure=fig_sentiment), style={'flex':'1','backgroundColor':'#1e1e1e','padding':'10px','borderRadius':'8px'})\n",
    "    ])\n",
    "])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
